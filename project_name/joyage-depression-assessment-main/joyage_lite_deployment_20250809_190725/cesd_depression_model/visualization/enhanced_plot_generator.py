#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå™¨
åŒ…å«æ›´å¤šæœ‰è¯´æœåŠ›çš„å›¾è¡¨ç”¨äºCESDæŠ‘éƒé¢„æµ‹ç ”ç©¶
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import (
    roc_curve, precision_recall_curve, confusion_matrix,
    classification_report, roc_auc_score, average_precision_score
)
from sklearn.calibration import calibration_curve
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class EnhancedPlotGenerator:
    """å¢å¼ºç‰ˆå›¾è¡¨ç”Ÿæˆå™¨"""
    
    def __init__(self, figsize=(12, 8), dpi=300):
        self.figsize = figsize
        self.dpi = dpi
        self.colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#8B5A3C', '#6B8E23']
        
    def plot_comprehensive_model_comparison(self, results_dict, save_path='plots/comprehensive_model_comparison.png'):
        """ç»¼åˆæ¨¡å‹æ¯”è¾ƒå›¾ï¼ˆå¤šå­å›¾ï¼‰"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('CESDæŠ‘éƒé¢„æµ‹æ¨¡å‹ç»¼åˆæ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        # æå–æ•°æ®
        models = list(results_dict.keys())
        auroc_scores = [results_dict[model]['auroc'] for model in models]
        auprc_scores = [results_dict[model]['auprc'] for model in models]
        accuracy_scores = [results_dict[model]['accuracy'] for model in models]
        f1_scores = [results_dict[model]['f1'] for model in models]
        
        # 1. AUROCæ¯”è¾ƒ
        ax1 = axes[0, 0]
        bars1 = ax1.bar(models, auroc_scores, color=self.colors[:len(models)])
        ax1.set_title('AUROCæ€§èƒ½æ¯”è¾ƒ', fontweight='bold')
        ax1.set_ylabel('AUROC')
        ax1.set_ylim(0.5, 1.0)
        self._add_value_labels(ax1, bars1)
        
        # 2. AUPRCæ¯”è¾ƒ
        ax2 = axes[0, 1]
        bars2 = ax2.bar(models, auprc_scores, color=self.colors[:len(models)])
        ax2.set_title('AUPRCæ€§èƒ½æ¯”è¾ƒ', fontweight='bold')
        ax2.set_ylabel('AUPRC')
        ax2.set_ylim(0, 1.0)
        self._add_value_labels(ax2, bars2)
        
        # 3. å‡†ç¡®ç‡æ¯”è¾ƒ
        ax3 = axes[0, 2]
        bars3 = ax3.bar(models, accuracy_scores, color=self.colors[:len(models)])
        ax3.set_title('å‡†ç¡®ç‡æ¯”è¾ƒ', fontweight='bold')
        ax3.set_ylabel('å‡†ç¡®ç‡')
        ax3.set_ylim(0, 1.0)
        self._add_value_labels(ax3, bars3)
        
        # 4. F1åˆ†æ•°æ¯”è¾ƒ
        ax4 = axes[1, 0]
        bars4 = ax4.bar(models, f1_scores, color=self.colors[:len(models)])
        ax4.set_title('F1åˆ†æ•°æ¯”è¾ƒ', fontweight='bold')
        ax4.set_ylabel('F1åˆ†æ•°')
        ax4.set_ylim(0, 1.0)
        self._add_value_labels(ax4, bars4)
        
        # 5. é›·è¾¾å›¾
        ax5 = axes[1, 1]
        self._plot_radar_chart(ax5, models, [auroc_scores, auprc_scores, accuracy_scores, f1_scores])
        
        # 6. æ€§èƒ½çƒ­åŠ›å›¾
        ax6 = axes[1, 2]
        self._plot_performance_heatmap(ax6, results_dict)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… ç»¼åˆæ¨¡å‹æ¯”è¾ƒå›¾å·²ä¿å­˜: {save_path}")
        
    def plot_confidence_intervals(self, results_dict, save_path='plots/confidence_intervals.png'):
        """ç½®ä¿¡åŒºé—´å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        models = list(results_dict.keys())
        
        # AUROCç½®ä¿¡åŒºé—´
        auroc_means = []
        auroc_lower = []
        auroc_upper = []
        
        for model in models:
            if 'auroc_ci' in results_dict[model]:
                ci = results_dict[model]['auroc_ci']
                auroc_means.append(results_dict[model]['auroc'])
                auroc_lower.append(ci[0])
                auroc_upper.append(ci[1])
            else:
                auroc_means.append(results_dict[model]['auroc'])
                auroc_lower.append(results_dict[model]['auroc'] * 0.95)
                auroc_upper.append(results_dict[model]['auroc'] * 1.05)
        
        # ç»˜åˆ¶AUROCç½®ä¿¡åŒºé—´
        x_pos = np.arange(len(models))
        ax1.errorbar(x_pos, auroc_means, yerr=[np.array(auroc_means) - np.array(auroc_lower), 
                                              np.array(auroc_upper) - np.array(auroc_means)], 
                    fmt='o', capsize=5, capthick=2, markersize=8)
        ax1.set_title('AUROC 95%ç½®ä¿¡åŒºé—´', fontweight='bold')
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('AUROC')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(models, rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # AUPRCç½®ä¿¡åŒºé—´
        auprc_means = []
        auprc_lower = []
        auprc_upper = []
        
        for model in models:
            if 'auprc_ci' in results_dict[model]:
                ci = results_dict[model]['auprc_ci']
                auprc_means.append(results_dict[model]['auprc'])
                auprc_lower.append(ci[0])
                auprc_upper.append(ci[1])
            else:
                auprc_means.append(results_dict[model]['auprc'])
                auprc_lower.append(results_dict[model]['auprc'] * 0.95)
                auprc_upper.append(results_dict[model]['auprc'] * 1.05)
        
        # ç»˜åˆ¶AUPRCç½®ä¿¡åŒºé—´
        ax2.errorbar(x_pos, auprc_means, yerr=[np.array(auprc_means) - np.array(auprc_lower), 
                                              np.array(auprc_upper) - np.array(auprc_means)], 
                    fmt='s', capsize=5, capthick=2, markersize=8, color='orange')
        ax2.set_title('AUPRC 95%ç½®ä¿¡åŒºé—´', fontweight='bold')
        ax2.set_xlabel('æ¨¡å‹')
        ax2.set_ylabel('AUPRC')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(models, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… ç½®ä¿¡åŒºé—´å›¾å·²ä¿å­˜: {save_path}")
        
    def plot_calibration_analysis(self, y_true, y_pred_proba_dict, save_path='plots/calibration_analysis.png'):
        """æ ¡å‡†åˆ†æå›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # æ ¡å‡†æ›²çº¿
        for i, (model_name, y_pred) in enumerate(y_pred_proba_dict.items()):
            fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred, n_bins=10)
            ax1.plot(mean_predicted_value, fraction_of_positives, 
                    marker='o', label=model_name, color=self.colors[i % len(self.colors)])
        
        # å®Œç¾æ ¡å‡†çº¿
        ax1.plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡å‡†')
        ax1.set_xlabel('å¹³å‡é¢„æµ‹æ¦‚ç‡')
        ax1.set_ylabel('å®é™…æ­£ä¾‹æ¯”ä¾‹')
        ax1.set_title('æ ¡å‡†æ›²çº¿', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å¯é æ€§å›¾
        for i, (model_name, y_pred) in enumerate(y_pred_proba_dict.items()):
            fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred, n_bins=10)
            ax2.bar(mean_predicted_value, fraction_of_positives - mean_predicted_value, 
                   alpha=0.7, label=model_name, color=self.colors[i % len(self.colors)])
        
        ax2.axhline(y=0, color='k', linestyle='-', alpha=0.5)
        ax2.set_xlabel('å¹³å‡é¢„æµ‹æ¦‚ç‡')
        ax2.set_ylabel('æ ¡å‡†è¯¯å·® (å®é™… - é¢„æµ‹)')
        ax2.set_title('å¯é æ€§å›¾', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ ¡å‡†åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        
    def plot_feature_importance_comparison(self, feature_importance_dict, save_path='plots/feature_importance_comparison.png'):
        """ç‰¹å¾é‡è¦æ€§æ¯”è¾ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ç‰¹å¾é‡è¦æ€§æ¯”è¾ƒåˆ†æ', fontsize=16, fontweight='bold')
        
        # è·å–æ‰€æœ‰ç‰¹å¾
        all_features = set()
        for model_importance in feature_importance_dict.values():
            all_features.update(model_importance.keys())
        all_features = sorted(list(all_features))
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§çŸ©é˜µ
        importance_matrix = []
        for model_name, importance_dict in feature_importance_dict.items():
            row = [importance_dict.get(feature, 0) for feature in all_features]
            importance_matrix.append(row)
        
        importance_df = pd.DataFrame(importance_matrix, 
                                   index=feature_importance_dict.keys(), 
                                   columns=all_features)
        
        # 1. çƒ­åŠ›å›¾
        sns.heatmap(importance_df.T, annot=True, fmt='.3f', cmap='YlOrRd', 
                   ax=axes[0, 0], cbar_kws={'label': 'é‡è¦æ€§åˆ†æ•°'})
        axes[0, 0].set_title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾', fontweight='bold')
        axes[0, 0].set_xlabel('æ¨¡å‹')
        axes[0, 0].set_ylabel('ç‰¹å¾')
        
        # 2. å‰10ç‰¹å¾æ¯”è¾ƒ
        top_features = importance_df.mean().nlargest(10).index
        top_importance = importance_df[top_features]
        
        top_importance.T.plot(kind='bar', ax=axes[0, 1], color=self.colors)
        axes[0, 1].set_title('å‰10ç‰¹å¾é‡è¦æ€§æ¯”è¾ƒ', fontweight='bold')
        axes[0, 1].set_xlabel('ç‰¹å¾')
        axes[0, 1].set_ylabel('é‡è¦æ€§åˆ†æ•°')
        axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ
        importance_df.T.boxplot(ax=axes[1, 0])
        axes[1, 0].set_title('ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ', fontweight='bold')
        axes[1, 0].set_xlabel('æ¨¡å‹')
        axes[1, 0].set_ylabel('é‡è¦æ€§åˆ†æ•°')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æ¨¡å‹é—´ç‰¹å¾é‡è¦æ€§ç›¸å…³æ€§
        correlation_matrix = importance_df.T.corr()
        sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', 
                   ax=axes[1, 1], center=0)
        axes[1, 1].set_title('æ¨¡å‹é—´ç‰¹å¾é‡è¦æ€§ç›¸å…³æ€§', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§æ¯”è¾ƒå›¾å·²ä¿å­˜: {save_path}")
        
    def plot_external_validation_comparison(self, internal_results, external_results, save_path='plots/external_validation_comparison.png'):
        """å†…éƒ¨éªŒè¯vså¤–éƒ¨éªŒè¯æ¯”è¾ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('å†…éƒ¨éªŒè¯ vs å¤–éƒ¨éªŒè¯æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        models = list(internal_results.keys())
        metrics = ['auroc', 'auprc', 'accuracy', 'f1']
        
        for i, metric in enumerate(metrics):
            ax = axes[i // 2, i % 2]
            
            internal_scores = [internal_results[model].get(metric, 0) for model in models]
            external_scores = [external_results[model].get(metric, 0) for model in models]
            
            x = np.arange(len(models))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, internal_scores, width, label='å†…éƒ¨éªŒè¯', alpha=0.8)
            bars2 = ax.bar(x + width/2, external_scores, width, label='å¤–éƒ¨éªŒè¯', alpha=0.8)
            
            ax.set_title(f'{metric.upper()} æ¯”è¾ƒ', fontweight='bold')
            ax.set_xlabel('æ¨¡å‹')
            ax.set_ylabel(metric.upper())
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            self._add_value_labels(ax, bars1)
            self._add_value_labels(ax, bars2)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… å¤–éƒ¨éªŒè¯æ¯”è¾ƒå›¾å·²ä¿å­˜: {save_path}")
        
    def plot_model_stability_analysis(self, cv_results_dict, save_path='plots/model_stability_analysis.png'):
        """æ¨¡å‹ç¨³å®šæ€§åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('æ¨¡å‹ç¨³å®šæ€§åˆ†æ', fontsize=16, fontweight='bold')
        
        models = list(cv_results_dict.keys())
        
        # 1. CVåˆ†æ•°åˆ†å¸ƒ
        for i, model in enumerate(models):
            cv_scores = cv_results_dict[model]['cv_scores']
            axes[0, 0].hist(cv_scores, alpha=0.7, label=model, color=self.colors[i % len(self.colors)])
        
        axes[0, 0].set_title('äº¤å‰éªŒè¯åˆ†æ•°åˆ†å¸ƒ', fontweight='bold')
        axes[0, 0].set_xlabel('AUROCåˆ†æ•°')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. åˆ†æ•°ç®±çº¿å›¾
        cv_data = []
        cv_labels = []
        for model in models:
            cv_scores = cv_results_dict[model]['cv_scores']
            cv_data.extend(cv_scores)
            cv_labels.extend([model] * len(cv_scores))
        
        cv_df = pd.DataFrame({'Model': cv_labels, 'Score': cv_data})
        sns.boxplot(data=cv_df, x='Model', y='Score', ax=axes[0, 1])
        axes[0, 1].set_title('äº¤å‰éªŒè¯åˆ†æ•°ç®±çº¿å›¾', fontweight='bold')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. æ ‡å‡†å·®æ¯”è¾ƒ
        std_scores = [np.std(cv_results_dict[model]['cv_scores']) for model in models]
        bars = axes[1, 0].bar(models, std_scores, color=self.colors[:len(models)])
        axes[1, 0].set_title('æ¨¡å‹ç¨³å®šæ€§ (æ ‡å‡†å·®)', fontweight='bold')
        axes[1, 0].set_xlabel('æ¨¡å‹')
        axes[1, 0].set_ylabel('æ ‡å‡†å·®')
        axes[1, 0].tick_params(axis='x', rotation=45)
        self._add_value_labels(axes[1, 0], bars)
        
        # 4. ç¨³å®šæ€§vsæ€§èƒ½æ•£ç‚¹å›¾
        mean_scores = [np.mean(cv_results_dict[model]['cv_scores']) for model in models]
        axes[1, 1].scatter(std_scores, mean_scores, s=100, alpha=0.7)
        for i, model in enumerate(models):
            axes[1, 1].annotate(model, (std_scores[i], mean_scores[i]), 
                              xytext=(5, 5), textcoords='offset points')
        
        axes[1, 1].set_title('ç¨³å®šæ€§ vs æ€§èƒ½', fontweight='bold')
        axes[1, 1].set_xlabel('æ ‡å‡†å·® (ç¨³å®šæ€§)')
        axes[1, 1].set_ylabel('å¹³å‡åˆ†æ•° (æ€§èƒ½)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ¨¡å‹ç¨³å®šæ€§åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        
    def _add_value_labels(self, ax, bars):
        """åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾"""
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom')
    
    def _plot_radar_chart(self, ax, models, metrics_list):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        # æ ‡å‡†åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´
        metrics_names = ['AUROC', 'AUPRC', 'Accuracy', 'F1']
        normalized_metrics = []
        
        for metrics in metrics_list:
            normalized = [(score - min(metrics)) / (max(metrics) - min(metrics)) for score in metrics]
            normalized_metrics.append(normalized)
        
        # é›·è¾¾å›¾è§’åº¦
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        for i, model in enumerate(models):
            values = normalized_metrics[i] + normalized_metrics[i][:1]  # é—­åˆå›¾å½¢
            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=self.colors[i % len(self.colors)])
            ax.fill(angles, values, alpha=0.25, color=self.colors[i % len(self.colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_names)
        ax.set_ylim(0, 1)
        ax.set_title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', fontweight='bold')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
    
    def _plot_performance_heatmap(self, ax, results_dict):
        """ç»˜åˆ¶æ€§èƒ½çƒ­åŠ›å›¾"""
        metrics = ['auroc', 'auprc', 'accuracy', 'f1', 'precision', 'recall']
        models = list(results_dict.keys())
        
        heatmap_data = []
        for model in models:
            row = [results_dict[model].get(metric, 0) for metric in metrics]
            heatmap_data.append(row)
        
        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd',
                   xticklabels=[m.upper() for m in metrics], 
                   yticklabels=models, ax=ax)
        ax.set_title('æ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾', fontweight='bold')

def main():
    """æµ‹è¯•å¢å¼ºç‰ˆå›¾è¡¨ç”Ÿæˆå™¨"""
    print("ğŸš€ å¢å¼ºç‰ˆå›¾è¡¨ç”Ÿæˆå™¨æµ‹è¯•")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    results_dict = {
        'LightGBM': {'auroc': 0.759, 'auprc': 0.432, 'accuracy': 0.712, 'f1': 0.567},
        'XGBoost': {'auroc': 0.758, 'auprc': 0.428, 'accuracy': 0.708, 'f1': 0.562},
        'RandomForest': {'auroc': 0.755, 'auprc': 0.425, 'accuracy': 0.705, 'f1': 0.558},
        'CatBoost': {'auroc': 0.761, 'auprc': 0.435, 'accuracy': 0.715, 'f1': 0.570}
    }
    
    # åˆ›å»ºå›¾è¡¨ç”Ÿæˆå™¨
    plotter = EnhancedPlotGenerator()
    
    # ç”Ÿæˆç»¼åˆæ¯”è¾ƒå›¾
    plotter.plot_comprehensive_model_comparison(results_dict)
    
    print("âœ… å¢å¼ºç‰ˆå›¾è¡¨ç”Ÿæˆå™¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main() 