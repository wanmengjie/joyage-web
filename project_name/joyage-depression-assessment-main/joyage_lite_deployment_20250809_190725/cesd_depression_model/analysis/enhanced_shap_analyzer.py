#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆSHAPåˆ†æå™¨ - é›†æˆç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå’Œå…¨é¢è§£é‡Š
"""

import numpy as np
import pandas as pd

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…tkinteré”™è¯¯
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éGUIåç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import permutation_test
import os
import warnings
warnings.filterwarnings('ignore')

# SHAPå¯¼å…¥
try:
    import shap
    print("âœ… SHAPåº“å¯¼å…¥æˆåŠŸ")
except ImportError:
    shap = None
    print("âŒ SHAPåº“å¯¼å…¥å¤±è´¥")

class EnhancedSHAPAnalyzer:
    """å¢å¼ºç‰ˆSHAPåˆ†æå™¨ - åŒ…å«ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
    
    def __init__(self, model, model_name="Model"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆSHAPåˆ†æå™¨
        
        å‚æ•°:
        ----
        model : sklearn model
            è®­ç»ƒå¥½çš„æ¨¡å‹
        model_name : str
            æ¨¡å‹åç§°
        """
        if shap is None:
            raise ImportError("SHAPåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œè§£é‡Šæ€§åˆ†æ")
            
        self.model = model
        self.model_name = model_name
        self.explainer = None
        self.shap_values = None
        self.feature_names = None
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        except:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def compute_shap_values(self, X_background, X_explain=None, sample_size=None):
        """
        è®¡ç®—SHAPå€¼
        
        å‚æ•°:
        ----
        X_background : DataFrame
            èƒŒæ™¯æ•°æ®é›†ï¼ˆç”¨äºå»ºç«‹åŸºçº¿ï¼‰
        X_explain : DataFrame, å¯é€‰
            éœ€è¦è§£é‡Šçš„æ•°æ®é›†ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨X_background
        sample_size : int, å¯é€‰
            ç”¨äºè§£é‡Šçš„æ ·æœ¬æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨æ ·æœ¬
        """
        print(f"\nğŸ” è®¡ç®—{self.model_name}çš„SHAPå€¼...")
        
        if X_explain is None:
            X_explain = X_background
            
        self.feature_names = list(X_background.columns)
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œé‡‡æ ·
        background_sample = X_background
        
        if sample_size is not None and len(X_explain) > sample_size:
            explain_sample = X_explain.sample(n=sample_size, random_state=42)
            print(f"   ä½¿ç”¨{sample_size}ä¸ªæ ·æœ¬è¿›è¡Œè§£é‡Š")
        else:
            explain_sample = X_explain
            print(f"   ä½¿ç”¨å…¨éƒ¨{len(X_explain)}ä¸ªæ ·æœ¬è¿›è¡Œè§£é‡Š")
            
        # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
        try:
            if hasattr(self.model, 'feature_importances_'):
                # æ ‘æ¨¡å‹
                self.explainer = shap.TreeExplainer(self.model, background_sample)
                print("   ä½¿ç”¨TreeExplainer")
            else:
                # å…¶ä»–æ¨¡å‹
                self.explainer = shap.KernelExplainer(self.model.predict_proba, background_sample)
                print("   ä½¿ç”¨KernelExplainer")
                
            # è®¡ç®—SHAPå€¼
            self.shap_values = self.explainer.shap_values(explain_sample)
            
            # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
            if isinstance(self.shap_values, list):
                if len(self.shap_values) == 2:
                    # äºŒåˆ†ç±»æƒ…å†µï¼Œå–æ­£ç±»
                    self.shap_values = self.shap_values[1]
                else:
                    # å¤šåˆ†ç±»æƒ…å†µï¼Œå–æœ€åä¸€ç±»
                    self.shap_values = self.shap_values[-1]
            
            # ç¡®ä¿SHAPå€¼æ˜¯2Dæ•°ç»„
            if hasattr(self.shap_values, 'ndim'):
                if self.shap_values.ndim == 3:
                    self.shap_values = self.shap_values[:, :, 1]  # å–æ­£ç±»çš„SHAPå€¼
                elif self.shap_values.ndim == 1:
                    # å¦‚æœæ˜¯1Dï¼Œé‡æ–°reshape
                    self.shap_values = self.shap_values.reshape(1, -1)
            
            # æœ€ç»ˆéªŒè¯
            if not hasattr(self.shap_values, 'shape') or self.shap_values.ndim != 2:
                raise ValueError(f"SHAPå€¼æ ¼å¼å¼‚å¸¸: {type(self.shap_values)}, shape: {getattr(self.shap_values, 'shape', 'unknown')}")
                
            print(f"   âœ… SHAPå€¼è®¡ç®—å®Œæˆ: {self.shap_values.shape}")
            return explain_sample
            
        except Exception as e:
            print(f"   âŒ SHAPå€¼è®¡ç®—å¤±è´¥: {e}")
            return None
    
    def statistical_significance_test(self, n_permutations=1000):
        """
        ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        
        å‚æ•°:
        ----
        n_permutations : int
            ç½®æ¢æ£€éªŒæ¬¡æ•°
            
        è¿”å›:
        ----
        DataFrame: åŒ…å«på€¼å’Œæ˜¾è‘—æ€§çš„ç‰¹å¾é‡è¦æ€§
        """
        print(f"\nğŸ“Š è¿›è¡Œç‰¹å¾é‡è¦æ€§ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
        
        if self.shap_values is None:
            raise ValueError("è¯·å…ˆè®¡ç®—SHAPå€¼")
            
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
        feature_importance = np.abs(self.shap_values).mean(axis=0)
        
        # è¿›è¡Œç½®æ¢æ£€éªŒ
        p_values = []
        
        for i, feature in enumerate(self.feature_names):
            print(f"   æ£€éªŒç‰¹å¾ {i+1}/{len(self.feature_names)}: {feature}")
            
            # åŸå§‹é‡è¦æ€§
            original_importance = feature_importance[i]
            
            # ç½®æ¢æ£€éªŒ
            permuted_importance = []
            for _ in range(n_permutations):
                # éšæœºç½®æ¢è¯¥ç‰¹å¾çš„SHAPå€¼
                permuted_shap = self.shap_values.copy()
                
                # ç¡®ä¿æˆ‘ä»¬æ­£åœ¨æ“ä½œæ­£ç¡®çš„ç»´åº¦
                if permuted_shap.ndim == 2:
                    np.random.shuffle(permuted_shap[:, i])
                    permuted_imp = np.abs(permuted_shap[:, i]).mean()
                else:
                    # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œè·³è¿‡è¿™ä¸ªç‰¹å¾
                    print(f"     âš ï¸ è·³è¿‡ç‰¹å¾ {feature}ï¼ŒSHAPå€¼ç»´åº¦å¼‚å¸¸: {permuted_shap.shape}")
                    permuted_imp = original_importance
                
                permuted_importance.append(permuted_imp)
            
            # è®¡ç®—på€¼ï¼ˆåŒä¾§æ£€éªŒï¼‰
            permuted_importance = np.array(permuted_importance)
            p_value = np.mean(permuted_importance >= original_importance)
            p_values.append(p_value)
        
        # å¤šé‡æ¯”è¾ƒæ ¡æ­£ï¼ˆBonferroniï¼‰
        p_values = np.array(p_values)
        p_values_corrected = np.minimum(p_values * len(p_values), 1.0)
        
        # åˆ›å»ºç»“æœDataFrame
        significance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': feature_importance,
            'p_value': p_values,
            'p_value_corrected': p_values_corrected,
            'significant': p_values_corrected < 0.05,
            'significance_level': ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns' 
                                 for p in p_values_corrected]
        }).sort_values('importance', ascending=False)
        
        print(f"   âœ… ç»Ÿè®¡æ£€éªŒå®Œæˆ")
        print(f"   ğŸ“Š æ˜¾è‘—ç‰¹å¾æ•°é‡: {significance_df['significant'].sum()}/{len(significance_df)}")
        
        return significance_df
    
    def generate_comprehensive_plots(self, X_data, save_dir='enhanced_shap_analysis'):
        """
        ç”Ÿæˆå…¨é¢çš„SHAPè§£é‡Šå›¾è¡¨
        
        å‚æ•°:
        ----
        X_data : DataFrame
            ç”¨äºè§£é‡Šçš„æ•°æ®
        save_dir : str
            ä¿å­˜ç›®å½•
        """
        if self.shap_values is None:
            raise ValueError("è¯·å…ˆè®¡ç®—SHAPå€¼")
            
        os.makedirs(save_dir, exist_ok=True)
        print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆSHAPè§£é‡Šå›¾è¡¨...")
        
        # 1. SHAP Summary Plot (Feature Importance)
        plt.figure(figsize=(12, 8))
        shap.summary_plot(self.shap_values, X_data, plot_type="bar", show=False)
        plt.title(f'SHAP Feature Importance - {self.model_name}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'shap_importance_bar.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. SHAP Summary Plot (Impact Distribution)
        plt.figure(figsize=(12, 10))
        shap.summary_plot(self.shap_values, X_data, show=False)
        plt.title(f'SHAP Feature Impact Distribution - {self.model_name}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'shap_impact_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Top Features Dependence Plots
        feature_importance = np.abs(self.shap_values).mean(axis=0)
        top_features_idx = np.argsort(feature_importance)[-6:][::-1]  # Top 6 features
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, feat_idx in enumerate(top_features_idx):
            feature_name = self.feature_names[feat_idx]
            shap.dependence_plot(feat_idx, self.shap_values, X_data, show=False, ax=axes[i])
            axes[i].set_title(f'Dependence: {feature_name}', fontsize=12, fontweight='bold')
        
        plt.suptitle(f'SHAP Dependence Plots - Top Features ({self.model_name})', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'shap_dependence_plots.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. SHAP Force Plot (ç¤ºä¾‹é¢„æµ‹)
        try:
            # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬
            sample_indices = [0, len(X_data)//4, len(X_data)//2, 3*len(X_data)//4, -1]
            
            for i, idx in enumerate(sample_indices):
                if idx >= len(self.shap_values):
                    continue
                    
                shap.force_plot(
                    self.explainer.expected_value, 
                    self.shap_values[idx], 
                    X_data.iloc[idx],
                    matplotlib=True,
                    show=False
                )
                plt.savefig(os.path.join(save_dir, f'shap_force_plot_sample_{i+1}.png'), 
                           dpi=300, bbox_inches='tight')
                plt.close()
                
        except Exception as e:
            print(f"   âš ï¸ Force plotç”Ÿæˆå¤±è´¥: {e}")
        
        print(f"   âœ… SHAPå›¾è¡¨ä¿å­˜åˆ°: {save_dir}")
    
    def feature_interaction_analysis(self, X_data, top_n=10):
        """
        ç‰¹å¾äº¤äº’ä½œç”¨åˆ†æ
        
        å‚æ•°:
        ----
        X_data : DataFrame
            ç‰¹å¾æ•°æ®
        top_n : int
            åˆ†æå‰Nä¸ªé‡è¦ç‰¹å¾çš„äº¤äº’ä½œç”¨
            
        è¿”å›:
        ----
        DataFrame: ç‰¹å¾äº¤äº’ä½œç”¨å¼ºåº¦
        """
        print(f"\nğŸ”€ è¿›è¡Œç‰¹å¾äº¤äº’ä½œç”¨åˆ†æ...")
        
        if self.shap_values is None:
            raise ValueError("è¯·å…ˆè®¡ç®—SHAPå€¼")
            
        # é€‰æ‹©topç‰¹å¾
        feature_importance = np.abs(self.shap_values).mean(axis=0)
        top_features_idx = np.argsort(feature_importance)[-top_n:][::-1]
        top_features = [self.feature_names[i] for i in top_features_idx]
        
        # è®¡ç®—ç‰¹å¾é—´çš„SHAPå€¼ç›¸å…³æ€§ï¼ˆä»£è¡¨äº¤äº’ä½œç”¨å¼ºåº¦ï¼‰
        shap_values_top = self.shap_values[:, top_features_idx]
        correlation_matrix = np.corrcoef(shap_values_top.T)
        
        # åˆ›å»ºäº¤äº’ä½œç”¨DataFrame
        interactions = []
        for i in range(len(top_features)):
            for j in range(i+1, len(top_features)):
                interaction_strength = abs(correlation_matrix[i, j])
                interactions.append({
                    'feature_1': top_features[i],
                    'feature_2': top_features[j],
                    'interaction_strength': interaction_strength,
                    'correlation': correlation_matrix[i, j]
                })
        
        interaction_df = pd.DataFrame(interactions).sort_values(
            'interaction_strength', ascending=False
        )
        
        print(f"   âœ… äº¤äº’ä½œç”¨åˆ†æå®Œæˆ")
        print(f"   ğŸ”€ æœ€å¼ºäº¤äº’ä½œç”¨: {interaction_df.iloc[0]['feature_1']} â†” {interaction_df.iloc[0]['feature_2']}")
        
        return interaction_df
    
    def generate_statistical_report(self, significance_df, interaction_df, save_dir='enhanced_shap_analysis'):
        """
        ç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š
        
        å‚æ•°:
        ----
        significance_df : DataFrame
            ç‰¹å¾æ˜¾è‘—æ€§ç»“æœ
        interaction_df : DataFrame
            ç‰¹å¾äº¤äº’ä½œç”¨ç»“æœ
        save_dir : str
            ä¿å­˜ç›®å½•
        """
        print(f"\nğŸ“ ç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š...")
        
        report_path = os.path.join(save_dir, 'statistical_analysis_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# {self.model_name} - ç»Ÿè®¡åˆ†ææŠ¥å‘Š\n\n")
            
            f.write("## 1. ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ\n\n")
            f.write("### æ–¹æ³•è¯´æ˜\n")
            f.write("- ä½¿ç”¨ç½®æ¢æ£€éªŒè¯„ä¼°ç‰¹å¾é‡è¦æ€§çš„ç»Ÿè®¡æ˜¾è‘—æ€§\n")
            f.write("- åº”ç”¨Bonferroniæ ¡æ­£æ§åˆ¶å¤šé‡æ¯”è¾ƒçš„å‡é˜³æ€§ç‡\n")
            f.write("- æ˜¾è‘—æ€§æ°´å¹³ï¼š*** p<0.001, ** p<0.01, * p<0.05, ns ä¸æ˜¾è‘—\n\n")
            
            f.write("### æ˜¾è‘—ç‰¹å¾æ’åº\n")
            f.write("| æ’å | ç‰¹å¾åç§° | é‡è¦æ€§ | På€¼ | æ ¡æ­£På€¼ | æ˜¾è‘—æ€§ |\n")
            f.write("|------|----------|--------|-----|---------|--------|\n")
            
            for idx, row in significance_df.head(10).iterrows():
                f.write(f"| {idx+1} | {row['feature']} | {row['importance']:.4f} | "
                       f"{row['p_value']:.4f} | {row['p_value_corrected']:.4f} | {row['significance_level']} |\n")
            
            f.write(f"\n**ç»Ÿè®¡æ‘˜è¦ï¼š**\n")
            f.write(f"- æ€»ç‰¹å¾æ•°ï¼š{len(significance_df)}\n")
            f.write(f"- æ˜¾è‘—ç‰¹å¾æ•°ï¼š{significance_df['significant'].sum()}\n")
            f.write(f"- æ˜¾è‘—ç‰¹å¾æ¯”ä¾‹ï¼š{significance_df['significant'].mean():.2%}\n\n")
            
            f.write("## 2. ç‰¹å¾äº¤äº’ä½œç”¨åˆ†æ\n\n")
            f.write("### æœ€å¼ºäº¤äº’ä½œç”¨ï¼ˆå‰10å¯¹ï¼‰\n")
            f.write("| æ’å | ç‰¹å¾1 | ç‰¹å¾2 | äº¤äº’å¼ºåº¦ | ç›¸å…³ç³»æ•° |\n")
            f.write("|------|-------|-------|----------|----------|\n")
            
            for idx, row in interaction_df.head(10).iterrows():
                f.write(f"| {idx+1} | {row['feature_1']} | {row['feature_2']} | "
                       f"{row['interaction_strength']:.4f} | {row['correlation']:.4f} |\n")
            
            f.write("\n## 3. è§£é‡Šæ€§åˆ†æç»“è®º\n\n")
            top_significant = significance_df[significance_df['significant']].head(5)
            f.write("### å…³é”®å‘ç°\n")
            f.write("1. **æœ€é‡è¦çš„æ˜¾è‘—ç‰¹å¾ï¼š**\n")
            for _, row in top_significant.iterrows():
                f.write(f"   - {row['feature']} (é‡è¦æ€§: {row['importance']:.4f}, {row['significance_level']})\n")
            
            f.write("\n2. **ç‰¹å¾äº¤äº’ä½œç”¨ï¼š**\n")
            top_interactions = interaction_df.head(3)
            for _, row in top_interactions.iterrows():
                f.write(f"   - {row['feature_1']} â†” {row['feature_2']} (å¼ºåº¦: {row['interaction_strength']:.4f})\n")
        
        print(f"   âœ… ç»Ÿè®¡æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    
    def run_complete_analysis(self, X_background, X_explain=None, save_dir='enhanced_shap_analysis'):
        """
        è¿è¡Œå®Œæ•´çš„å¢å¼ºSHAPåˆ†æ
        
        å‚æ•°:
        ----
        X_background : DataFrame
            èƒŒæ™¯æ•°æ®é›†
        X_explain : DataFrame, å¯é€‰
            è§£é‡Šæ•°æ®é›†
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: åˆ†æç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹{self.model_name}çš„å®Œæ•´å¢å¼ºSHAPåˆ†æ")
        print("=" * 60)
        
        # 1. è®¡ç®—SHAPå€¼
        X_data = self.compute_shap_values(X_background, X_explain)
        if X_data is None:
            return None
        
        # 2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        significance_df = self.statistical_significance_test()
        
        # 3. ç‰¹å¾äº¤äº’ä½œç”¨åˆ†æ
        interaction_df = self.feature_interaction_analysis(X_data)
        
        # 4. ç”Ÿæˆå›¾è¡¨
        self.generate_comprehensive_plots(X_data, save_dir)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_statistical_report(significance_df, interaction_df, save_dir)
        
        # 6. ä¿å­˜æ•°æ®
        significance_df.to_csv(os.path.join(save_dir, 'feature_significance.csv'), 
                              index=False, encoding='utf-8-sig')
        interaction_df.to_csv(os.path.join(save_dir, 'feature_interactions.csv'), 
                             index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ‰ å¢å¼ºSHAPåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
        
        return {
            'significance_results': significance_df,
            'interaction_results': interaction_df,
            'shap_values': self.shap_values,
            'feature_names': self.feature_names
        } 