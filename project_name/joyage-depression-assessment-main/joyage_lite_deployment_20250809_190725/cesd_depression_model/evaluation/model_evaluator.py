"""
Model evaluation module for CESD Depression Prediction Model
"""

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    brier_score_loss, roc_curve, precision_recall_curve
)
from sklearn.model_selection import cross_val_score, StratifiedKFold
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨ - åŒ…å«å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå›ºå®š95%ç½®ä¿¡åŒºé—´"""
    
    def __init__(self, random_state=42):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨
        
        å‚æ•°:
        ----
        random_state : int, é»˜è®¤42
            éšæœºç§å­
        """
        self.random_state = random_state
        
    def evaluate_model(self, model, X_test, y_test, bootstrap_ci=True, n_bootstraps=1000):
        """
        å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
        ----
        model : sklearn model
            è®­ç»ƒå¥½çš„æ¨¡å‹
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•ç›®æ ‡
        bootstrap_ci : bool, é»˜è®¤ä¸º True
            æ˜¯å¦è®¡ç®—Bootstrapç½®ä¿¡åŒºé—´
        n_bootstraps : int, é»˜è®¤ä¸º 1000
            Bootstrapé‡‡æ ·æ¬¡æ•°
            
        è¿”å›:
        ----
        dict : åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        try:
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # è®¡ç®—æ··æ·†çŸ©é˜µ
            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
            
            # åŸºæœ¬æŒ‡æ ‡
            metrics = {
                # 1. å‡†ç¡®ç‡
                'accuracy': accuracy_score(y_test, y_pred),
                
                # 2. ç²¾ç¡®ç‡
                'precision': precision_score(y_test, y_pred, zero_division=0),
                
                # 3. å¬å›ç‡ï¼ˆæ•æ„Ÿæ€§ï¼‰
                'recall': recall_score(y_test, y_pred, zero_division=0),
                
                # 4. F1åˆ†æ•°
                'f1_score': f1_score(y_test, y_pred, zero_division=0),
                
                # 5. AUROC
                'roc_auc': roc_auc_score(y_test, y_pred_proba),
                
                # 6. AUPRC
                'pr_auc': average_precision_score(y_test, y_pred_proba),
                
                # 7. C_Index (å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œç­‰åŒäºAUROC)
                'c_index': roc_auc_score(y_test, y_pred_proba),
                
                # 8. ç‰¹å¼‚æ€§
                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
                
                # 9. è´Ÿé¢„æµ‹å€¼ (NPV)
                'npv': tn / (tn + fn) if (tn + fn) > 0 else 0,
                
                # 10. Brieråˆ†æ•°
                'brier_score': brier_score_loss(y_test, y_pred_proba),
                
                # é¢å¤–æœ‰ç”¨æŒ‡æ ‡
                'positive_predictive_value': precision_score(y_test, y_pred, zero_division=0),  # ä¸precisionç›¸åŒ
                'sensitivity': recall_score(y_test, y_pred, zero_division=0),  # ä¸recallç›¸åŒ
                'true_positive': int(tp),
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn),
                'sample_size': len(y_test)
            }
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            if bootstrap_ci:
                ci_metrics = self._calculate_bootstrap_confidence_intervals(
                    model, X_test, y_test, n_bootstraps
                )
                # åˆå¹¶ç½®ä¿¡åŒºé—´ç»“æœ
                for key, ci in ci_metrics.items():
                    metrics[f'{key}_ci_lower'] = ci[0]
                    metrics[f'{key}_ci_upper'] = ci[1]
            
            return metrics
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
            return {}
    
    def _calculate_bootstrap_confidence_intervals(self, model, X_test, y_test, n_bootstraps=1000):
        """
        è®¡ç®—Bootstrap 95%ç½®ä¿¡åŒºé—´
        
        å‚æ•°:
        ----
        model : sklearn model
            è®­ç»ƒå¥½çš„æ¨¡å‹
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•ç›®æ ‡
        n_bootstraps : int
            Bootstrapé‡‡æ ·æ¬¡æ•°
            
        è¿”å›:
        ----
        dict : å„æŒ‡æ ‡çš„ç½®ä¿¡åŒºé—´
        """
        np.random.seed(self.random_state)
        
        bootstrap_metrics = {
            'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [],
            'roc_auc': [], 'pr_auc': [], 'c_index': [], 'specificity': [], 
            'npv': [], 'brier_score': []
        }
        
        n_samples = len(y_test)
        
        for i in range(n_bootstraps):
            # Bootstrapé‡‡æ ·
            indices = np.random.choice(n_samples, size=n_samples, replace=True)
            X_boot = X_test.iloc[indices]
            y_boot = y_test.iloc[indices]
            
            try:
                # é¢„æµ‹
                y_pred_boot = model.predict(X_boot)
                y_pred_proba_boot = model.predict_proba(X_boot)[:, 1]
                
                # è®¡ç®—æ··æ·†çŸ©é˜µ
                tn, fp, fn, tp = confusion_matrix(y_boot, y_pred_boot).ravel()
                
                # è®¡ç®—æŒ‡æ ‡
                bootstrap_metrics['accuracy'].append(accuracy_score(y_boot, y_pred_boot))
                bootstrap_metrics['precision'].append(precision_score(y_boot, y_pred_boot, zero_division=0))
                bootstrap_metrics['recall'].append(recall_score(y_boot, y_pred_boot, zero_division=0))
                bootstrap_metrics['f1_score'].append(f1_score(y_boot, y_pred_boot, zero_division=0))
                bootstrap_metrics['roc_auc'].append(roc_auc_score(y_boot, y_pred_proba_boot))
                bootstrap_metrics['pr_auc'].append(average_precision_score(y_boot, y_pred_proba_boot))
                bootstrap_metrics['c_index'].append(roc_auc_score(y_boot, y_pred_proba_boot))
                bootstrap_metrics['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)
                bootstrap_metrics['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)
                bootstrap_metrics['brier_score'].append(brier_score_loss(y_boot, y_pred_proba_boot))
                
            except Exception:
                # å¦‚æœæŸæ¬¡é‡‡æ ·å‡ºç°é—®é¢˜ï¼Œè·³è¿‡
                continue
        
        # è®¡ç®—95%ç½®ä¿¡åŒºé—´
        confidence_intervals = {}
        for metric, values in bootstrap_metrics.items():
            if values:  # ç¡®ä¿æœ‰å€¼
                ci_lower = np.percentile(values, 2.5)
                ci_upper = np.percentile(values, 97.5)
                confidence_intervals[metric] = (ci_lower, ci_upper)
            else:
                confidence_intervals[metric] = (0, 0)
        
        return confidence_intervals
    
    def evaluate_all_models(self, models, X_test, y_test, bootstrap_ci=True):
        """
        è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        
        å‚æ•°:
        ----
        models : dict
            æ¨¡å‹å­—å…¸
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•ç›®æ ‡
        bootstrap_ci : bool
            æ˜¯å¦è®¡ç®—ç½®ä¿¡åŒºé—´
            
        è¿”å›:
        ----
        dict : æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        """
        all_results = {}
        
        for model_name, model in models.items():
            try:
                print(f"  ğŸ“Š è¯„ä¼° {model_name}...")
                results = self.evaluate_model(model, X_test, y_test, bootstrap_ci=bootstrap_ci)
                all_results[model_name] = results
                
                # æ‰“å°ä¸»è¦æŒ‡æ ‡
                print(f"    âœ“ AUROC: {results['roc_auc']:.4f}")
                print(f"    âœ“ AUPRC: {results['pr_auc']:.4f}")
                print(f"    âœ“ F1: {results['f1_score']:.4f}")
                print(f"    âœ“ å‡†ç¡®ç‡: {results['accuracy']:.4f}")
                
                if bootstrap_ci and 'roc_auc_ci_lower' in results:
                    print(f"    âœ“ AUROC 95%CI: [{results['roc_auc_ci_lower']:.4f}, {results['roc_auc_ci_upper']:.4f}]")
                
            except Exception as e:
                print(f"    âŒ {model_name} è¯„ä¼°å¤±è´¥: {str(e)}")
                all_results[model_name] = {}
        
        return all_results
    
    def find_best_model(self, results, metric='roc_auc'):
        """
        æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        
        å‚æ•°:
        ----
        results : dict
            è¯„ä¼°ç»“æœå­—å…¸
        metric : str
            ç”¨äºæ¯”è¾ƒçš„æŒ‡æ ‡
            
        è¿”å›:
        ----
        tuple : (æœ€ä½³æ¨¡å‹åç§°, æœ€ä½³åˆ†æ•°)
        """
        if not results:
            return None, None
            
        best_model = None
        best_score = -1
        
        for model_name, metrics in results.items():
            if metric in metrics and metrics[metric] > best_score:
                best_score = metrics[metric]
                best_model = model_name
        
        return best_model, best_score
    
    def generate_comparison_table(self, results, save_path=None):
        """
        ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒè¡¨
        
        å‚æ•°:
        ----
        results : dict
            è¯„ä¼°ç»“æœå­—å…¸
        save_path : str, å¯é€‰
            ä¿å­˜è·¯å¾„
            
        è¿”å›:
        ----
        DataFrame : æ¯”è¾ƒè¡¨
        """
        if not results:
            return pd.DataFrame()
        
        # æå–æ‰€æœ‰æŒ‡æ ‡
        comparison_data = []
        
        for model_name, metrics in results.items():
            if metrics:  # ç¡®ä¿æœ‰è¯„ä¼°ç»“æœ
                row = {'Model': model_name}
                
                # ä¸»è¦æŒ‡æ ‡
                main_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 
                              'roc_auc', 'pr_auc', 'c_index', 'specificity', 
                              'npv', 'brier_score']
                
                for metric in main_metrics:
                    if metric in metrics:
                        row[metric.upper()] = round(metrics[metric], 4)
                    
                    # æ·»åŠ ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    ci_lower_key = f'{metric}_ci_lower'
                    ci_upper_key = f'{metric}_ci_upper'
                    if ci_lower_key in metrics and ci_upper_key in metrics:
                        row[f'{metric.upper()}_95CI'] = f"[{metrics[ci_lower_key]:.4f}, {metrics[ci_upper_key]:.4f}]"
                
                # æ··æ·†çŸ©é˜µæŒ‡æ ‡
                if 'true_positive' in metrics:
                    row['TP'] = metrics['true_positive']
                    row['TN'] = metrics['true_negative']
                    row['FP'] = metrics['false_positive']
                    row['FN'] = metrics['false_negative']
                
                comparison_data.append(row)
        
        # åˆ›å»ºDataFrameå¹¶æŒ‰AUROCæ’åº
        df = pd.DataFrame(comparison_data)
        if not df.empty and 'ROC_AUC' in df.columns:
            df = df.sort_values('ROC_AUC', ascending=False)
        
        # ä¿å­˜ç»“æœ
        if save_path:
            df.to_csv(save_path, index=False)
            print(f"âœ… æ¨¡å‹æ¯”è¾ƒè¡¨å·²ä¿å­˜: {save_path}")
        
        return df
    
    def cross_validate_model(self, model, X, y, cv_folds=10, scoring_metrics=None):
        """
        äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ - ä½¿ç”¨Bootstrapè®¡ç®—95%ç½®ä¿¡åŒºé—´
        
        å‚æ•°:
        ----
        model : sklearn model
            è¦è¯„ä¼°çš„æ¨¡å‹
        X : DataFrame
            ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        cv_folds : int
            äº¤å‰éªŒè¯æŠ˜æ•°
        scoring_metrics : list, å¯é€‰
            è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
            
        è¿”å›:
        ----
        dict : äº¤å‰éªŒè¯ç»“æœï¼ŒåŒ…å«Bootstrap 95%ç½®ä¿¡åŒºé—´
        """
        if scoring_metrics is None:
            scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision']
        
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        cv_results = {}
        
        print(f"ğŸ”„ æ‰§è¡Œ {cv_folds} æŠ˜äº¤å‰éªŒè¯...")
        
        for metric in scoring_metrics:
            try:
                # æ‰§è¡Œäº¤å‰éªŒè¯
                scores = cross_val_score(model, X, y, cv=cv, scoring=metric, n_jobs=1)
                
                # åŸºæœ¬ç»Ÿè®¡
                cv_results[f'{metric}_scores'] = scores.tolist()
                cv_results[f'{metric}_mean'] = scores.mean()
                cv_results[f'{metric}_std'] = scores.std()
                
                # ä½¿ç”¨Bootstrapé‡é‡‡æ ·è®¡ç®—çœŸæ­£çš„95%ç½®ä¿¡åŒºé—´
                # å¯¹äº¤å‰éªŒè¯å¾—åˆ†è¿›è¡ŒBootstrapé‡é‡‡æ ·
                n_bootstrap = 1000
                np.random.seed(self.random_state)
                bootstrap_means = []
                
                for _ in range(n_bootstrap):
                    # å¯¹CVåˆ†æ•°è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·
                    bootstrap_sample = np.random.choice(scores, size=len(scores), replace=True)
                    bootstrap_means.append(bootstrap_sample.mean())
                
                # è®¡ç®—95%ç½®ä¿¡åŒºé—´
                ci_lower = np.percentile(bootstrap_means, 2.5)
                ci_upper = np.percentile(bootstrap_means, 97.5)
                
                cv_results[f'{metric}_ci_lower'] = ci_lower
                cv_results[f'{metric}_ci_upper'] = ci_upper
                
                print(f"  âœ“ {metric}: {scores.mean():.4f} Â± {scores.std():.4f} [95%CI: {ci_lower:.4f}, {ci_upper:.4f}]")
                
            except Exception as e:
                print(f"âš ï¸ äº¤å‰éªŒè¯æŒ‡æ ‡ {metric} è®¡ç®—å¤±è´¥: {str(e)}")
        
        # æ·»åŠ äº¤å‰éªŒè¯çš„å…ƒä¿¡æ¯
        cv_results['cv_folds'] = cv_folds
        cv_results['total_samples'] = len(y)
        cv_results['samples_per_fold'] = len(y) // cv_folds
        cv_results['confidence_interval_method'] = 'Bootstrap on CV scores'
        
        return cv_results
    
    def print_detailed_metrics(self, results, model_name):
        """
        æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
        
        å‚æ•°:
        ----
        results : dict
            è¯„ä¼°ç»“æœ
        model_name : str
            æ¨¡å‹åç§°
        """
        if not results:
            print(f"âŒ {model_name} æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return
        
        print(f"\nğŸ“Š {model_name} è¯¦ç»†è¯„ä¼°æŒ‡æ ‡:")
        print("=" * 60)
        
        # ä¸»è¦æ€§èƒ½æŒ‡æ ‡
        print("ğŸ¯ ä¸»è¦æ€§èƒ½æŒ‡æ ‡:")
        main_metrics = [
            ('accuracy', 'å‡†ç¡®ç‡'), ('precision', 'ç²¾ç¡®ç‡'), ('recall', 'å¬å›ç‡'),
            ('f1_score', 'F1åˆ†æ•°'), ('roc_auc', 'AUROC'), ('pr_auc', 'AUPRC'),
            ('c_index', 'C-Index'), ('specificity', 'ç‰¹å¼‚æ€§'), ('npv', 'è´Ÿé¢„æµ‹å€¼'),
            ('brier_score', 'Brieråˆ†æ•°')
        ]
        
        for metric_key, metric_name in main_metrics:
            if metric_key in results:
                value = results[metric_key]
                print(f"  {metric_name}: {value:.4f}")
                
                # æ‰“å°ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                ci_lower_key = f'{metric_key}_ci_lower'
                ci_upper_key = f'{metric_key}_ci_upper'
                if ci_lower_key in results and ci_upper_key in results:
                    print(f"    95%CI: [{results[ci_lower_key]:.4f}, {results[ci_upper_key]:.4f}]")
        
        # æ··æ·†çŸ©é˜µ
        if all(key in results for key in ['true_positive', 'true_negative', 'false_positive', 'false_negative']):
            print(f"\nğŸ”¢ æ··æ·†çŸ©é˜µ:")
            print(f"  çœŸé˜³æ€§ (TP): {results['true_positive']}")
            print(f"  çœŸé˜´æ€§ (TN): {results['true_negative']}")
            print(f"  å‡é˜³æ€§ (FP): {results['false_positive']}")
            print(f"  å‡é˜´æ€§ (FN): {results['false_negative']}")
            print(f"  æ ·æœ¬æ€»æ•°: {results['sample_size']}") 