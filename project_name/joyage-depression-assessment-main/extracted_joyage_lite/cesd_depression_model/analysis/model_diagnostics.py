#!/usr/bin/env python3
"""
æ¨¡å‹è¯Šæ–­åˆ†æå™¨ - æä¾›å…¨é¢çš„æ¨¡å‹è¯Šæ–­åŠŸèƒ½
"""

import numpy as np
import pandas as pd

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…tkinteré”™è¯¯
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éGUIåç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import (
    precision_recall_curve, roc_curve,
    confusion_matrix, classification_report
)

# å¤„ç†ä¸åŒsklearnç‰ˆæœ¬çš„brier_score_losså¯¼å…¥
try:
    from sklearn.metrics import brier_score_loss
except ImportError:
    # ä¸ºæ›´è€ç‰ˆæœ¬æä¾›brier_score_lossçš„ç®€å•å®ç°
    def brier_score_loss(y_true, y_prob):
        return np.mean((y_prob - y_true) ** 2)

# å¤„ç†ä¸åŒsklearnç‰ˆæœ¬çš„calibration_curveå¯¼å…¥
try:
    from sklearn.calibration import calibration_curve
except ImportError:
    try:
        from sklearn.metrics import calibration_curve
    except ImportError:
        # ä¸ºæ›´è€çš„sklearnç‰ˆæœ¬æä¾›ç®€å•çš„æ›¿ä»£å®ç°
        def calibration_curve(y_true, y_prob_pos, n_bins=5, strategy='uniform'):
            bin_boundaries = np.linspace(0., 1. + 1e-8, n_bins + 1)
            bin_lowers = bin_boundaries[:-1]
            bin_uppers = bin_boundaries[1:]
            
            bin_centers = []
            fraction_of_positives = []
            
            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
                in_bin = (y_prob_pos > bin_lower) & (y_prob_pos <= bin_upper)
                prop_in_bin = in_bin.mean()
                
                if prop_in_bin > 0:
                    fraction_of_positives_in_bin = y_true[in_bin].mean()
                    bin_centers.append((bin_lower + bin_upper) / 2)
                    fraction_of_positives.append(fraction_of_positives_in_bin)
            
            return np.array(fraction_of_positives), np.array(bin_centers)
import warnings
import os
warnings.filterwarnings('ignore')

class ModelDiagnosticsAnalyzer:
    """æ¨¡å‹è¯Šæ–­åˆ†æå™¨"""
    
    def __init__(self, model, model_name="Model"):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯Šæ–­åˆ†æå™¨
        
        å‚æ•°:
        ----
        model : sklearn model
            è®­ç»ƒå¥½çš„æ¨¡å‹
        model_name : str
            æ¨¡å‹åç§°
        """
        self.model = model
        self.model_name = model_name
        
        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        plt.rcParams['figure.figsize'] = (10, 6)
        plt.rcParams['font.size'] = 12
        
    def calibration_analysis(self, X_test, y_test, n_bins=10, save_dir='model_diagnostics'):
        """
        æ¨¡å‹æ ¡å‡†åˆ†æ
        
        å‚æ•°:
        ----
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•æ ‡ç­¾
        n_bins : int
            æ ¡å‡†æ›²çº¿çš„åˆ†ç®±æ•°
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: æ ¡å‡†åˆ†æç»“æœ
        """
        print(f"\nğŸ“Š {self.model_name} - æ¨¡å‹æ ¡å‡†åˆ†æ")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # è·å–é¢„æµ‹æ¦‚ç‡
        y_prob = self.model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æ ¡å‡†æ›²çº¿
        fraction_of_positives, mean_predicted_value = calibration_curve(
            y_test, y_prob, n_bins=n_bins, strategy='uniform'
        )
        
        # è®¡ç®—Brieråˆ†æ•°
        brier_score = brier_score_loss(y_test, y_prob)
        
        # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å­å›¾1ï¼šæ ¡å‡†æ›²çº¿
        ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
        ax1.plot(mean_predicted_value, fraction_of_positives, 'o-', 
                label=f'{self.model_name} (Brier Score: {brier_score:.4f})')
        ax1.set_xlabel('Mean Predicted Probability')
        ax1.set_ylabel('Fraction of Positives')
        ax1.set_title('Calibration Curve')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        ax2.hist(y_prob[y_test == 0], bins=20, alpha=0.7, label='Negative Class', density=True)
        ax2.hist(y_prob[y_test == 1], bins=20, alpha=0.7, label='Positive Class', density=True)
        ax2.set_xlabel('Predicted Probability')
        ax2.set_ylabel('Density')
        ax2.set_title('Prediction Probability Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'calibration_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # è®¡ç®—æ ¡å‡†æŒ‡æ ‡
        calibration_metrics = {
            'brier_score': brier_score,
            'mean_calibration_error': np.mean(np.abs(fraction_of_positives - mean_predicted_value)),
            'max_calibration_error': np.max(np.abs(fraction_of_positives - mean_predicted_value)),
            'calibration_bins': n_bins,
            'fraction_of_positives': fraction_of_positives,
            'mean_predicted_value': mean_predicted_value
        }
        
        print(f"   âœ… Brier Score: {brier_score:.4f}")
        print(f"   âœ… Mean Calibration Error: {calibration_metrics['mean_calibration_error']:.4f}")
        print(f"   âœ… Max Calibration Error: {calibration_metrics['max_calibration_error']:.4f}")
        
        return calibration_metrics
    
    def residual_analysis(self, X_test, y_test, save_dir='model_diagnostics'):
        """
        æ®‹å·®åˆ†æï¼ˆé€‚ç”¨äºæ¦‚ç‡é¢„æµ‹ï¼‰
        
        å‚æ•°:
        ----
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•æ ‡ç­¾
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: æ®‹å·®åˆ†æç»“æœ
        """
        print(f"\nğŸ” {self.model_name} - æ®‹å·®åˆ†æ")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # è·å–é¢„æµ‹
        y_prob = self.model.predict_proba(X_test)[:, 1]
        y_pred = (y_prob >= 0.5).astype(int)
        
        # è®¡ç®—æ®‹å·®ï¼ˆå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨Pearsonæ®‹å·®ï¼‰
        residuals = y_test - y_prob
        pearson_residuals = residuals / np.sqrt(y_prob * (1 - y_prob) + 1e-8)
        
        # åˆ›å»ºæ®‹å·®å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å­å›¾1ï¼šæ®‹å·® vs é¢„æµ‹å€¼
        axes[0, 0].scatter(y_prob, residuals, alpha=0.6)
        axes[0, 0].axhline(y=0, color='red', linestyle='--')
        axes[0, 0].set_xlabel('Predicted Probability')
        axes[0, 0].set_ylabel('Residuals')
        axes[0, 0].set_title('Residuals vs Predicted')
        axes[0, 0].grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šPearsonæ®‹å·® vs é¢„æµ‹å€¼
        axes[0, 1].scatter(y_prob, pearson_residuals, alpha=0.6)
        axes[0, 1].axhline(y=0, color='red', linestyle='--')
        axes[0, 1].set_xlabel('Predicted Probability')
        axes[0, 1].set_ylabel('Pearson Residuals')
        axes[0, 1].set_title('Pearson Residuals vs Predicted')
        axes[0, 1].grid(True, alpha=0.3)
        
        # å­å›¾3ï¼šæ®‹å·®Q-Qå›¾
        stats.probplot(residuals, dist="norm", plot=axes[1, 0])
        axes[1, 0].set_title('Q-Q Plot of Residuals')
        axes[1, 0].grid(True, alpha=0.3)
        
        # å­å›¾4ï¼šæ®‹å·®ç›´æ–¹å›¾
        axes[1, 1].hist(residuals, bins=30, density=True, alpha=0.7)
        axes[1, 1].set_xlabel('Residuals')
        axes[1, 1].set_ylabel('Density')
        axes[1, 1].set_title('Distribution of Residuals')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'residual_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # è®¡ç®—æ®‹å·®ç»Ÿè®¡é‡
        residual_stats = {
            'mean_residual': np.mean(residuals),
            'std_residual': np.std(residuals),
            'mean_abs_residual': np.mean(np.abs(residuals)),
            'mean_pearson_residual': np.mean(pearson_residuals),
            'std_pearson_residual': np.std(pearson_residuals),
            'shapiro_test_pvalue': stats.shapiro(residuals)[1],
            'ljung_box_test': self._ljung_box_test(residuals)
        }
        
        print(f"   âœ… Mean Residual: {residual_stats['mean_residual']:.4f}")
        print(f"   âœ… Mean Absolute Residual: {residual_stats['mean_abs_residual']:.4f}")
        print(f"   âœ… Shapiro-Wilk Test p-value: {residual_stats['shapiro_test_pvalue']:.4f}")
        
        return residual_stats
    
    def prediction_distribution_analysis(self, X_test, y_test, save_dir='model_diagnostics'):
        """
        é¢„æµ‹åˆ†å¸ƒåˆ†æ
        
        å‚æ•°:
        ----
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•æ ‡ç­¾
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: é¢„æµ‹åˆ†å¸ƒåˆ†æç»“æœ
        """
        print(f"\nğŸ“ˆ {self.model_name} - é¢„æµ‹åˆ†å¸ƒåˆ†æ")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # è·å–é¢„æµ‹
        y_prob = self.model.predict_proba(X_test)[:, 1]
        y_pred = (y_prob >= 0.5).astype(int)
        
        # åˆ›å»ºåˆ†å¸ƒåˆ†æå›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å­å›¾1ï¼šé¢„æµ‹æ¦‚ç‡å¯†åº¦å›¾
        axes[0, 0].hist(y_prob[y_test == 0], bins=30, alpha=0.7, label='True Negative', density=True)
        axes[0, 0].hist(y_prob[y_test == 1], bins=30, alpha=0.7, label='True Positive', density=True)
        axes[0, 0].set_xlabel('Predicted Probability')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].set_title('Prediction Probability Distribution by True Class')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šé¢„æµ‹æ¦‚ç‡ç®±çº¿å›¾
        prob_data = [y_prob[y_test == 0], y_prob[y_test == 1]]
        axes[0, 1].boxplot(prob_data, labels=['Negative', 'Positive'])
        axes[0, 1].set_ylabel('Predicted Probability')
        axes[0, 1].set_title('Prediction Probability Box Plot')
        axes[0, 1].grid(True, alpha=0.3)
        
        # å­å›¾3ï¼šæ··æ·†çŸ©é˜µçƒ­å›¾
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
        axes[1, 0].set_xlabel('Predicted')
        axes[1, 0].set_ylabel('Actual')
        axes[1, 0].set_title('Confusion Matrix')
        
        # å­å›¾4ï¼šé˜ˆå€¼åˆ†æ
        thresholds = np.linspace(0, 1, 101)
        precisions, recalls, f1_scores = [], [], []
        
        for threshold in thresholds:
            y_pred_thresh = (y_prob >= threshold).astype(int)
            if len(np.unique(y_pred_thresh)) > 1:
                precision = np.sum((y_pred_thresh == 1) & (y_test == 1)) / np.sum(y_pred_thresh == 1)
                recall = np.sum((y_pred_thresh == 1) & (y_test == 1)) / np.sum(y_test == 1)
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            else:
                precision, recall, f1 = 0, 0, 0
            
            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)
        
        axes[1, 1].plot(thresholds, precisions, label='Precision')
        axes[1, 1].plot(thresholds, recalls, label='Recall')
        axes[1, 1].plot(thresholds, f1_scores, label='F1-Score')
        axes[1, 1].axvline(x=0.5, color='red', linestyle='--', label='Default Threshold')
        axes[1, 1].set_xlabel('Threshold')
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].set_title('Performance vs Threshold')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'prediction_distribution_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # è®¡ç®—åˆ†å¸ƒç»Ÿè®¡é‡
        distribution_stats = {
            'prob_mean_negative': np.mean(y_prob[y_test == 0]),
            'prob_std_negative': np.std(y_prob[y_test == 0]),
            'prob_mean_positive': np.mean(y_prob[y_test == 1]),
            'prob_std_positive': np.std(y_prob[y_test == 1]),
            'separation_score': np.abs(np.mean(y_prob[y_test == 1]) - np.mean(y_prob[y_test == 0])),
            'optimal_threshold': thresholds[np.argmax(f1_scores)] if f1_scores else 0.5,
            'max_f1_score': np.max(f1_scores) if f1_scores else 0
        }
        
        print(f"   âœ… Class Separation Score: {distribution_stats['separation_score']:.4f}")
        print(f"   âœ… Optimal Threshold: {distribution_stats['optimal_threshold']:.3f}")
        print(f"   âœ… Max F1 Score: {distribution_stats['max_f1_score']:.4f}")
        
        return distribution_stats
    
    def feature_reliability_analysis(self, X_test, y_test, feature_names=None, save_dir='model_diagnostics'):
        """
        ç‰¹å¾å¯é æ€§åˆ†æ
        
        å‚æ•°:
        ----
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•æ ‡ç­¾
        feature_names : list, å¯é€‰
            ç‰¹å¾åç§°åˆ—è¡¨
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: ç‰¹å¾å¯é æ€§åˆ†æç»“æœ
        """
        print(f"\nğŸ”§ {self.model_name} - ç‰¹å¾å¯é æ€§åˆ†æ")
        
        os.makedirs(save_dir, exist_ok=True)
        
        if feature_names is None:
            feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(X_test.shape[1])]
        
        # ç‰¹å¾ç¨³å®šæ€§åˆ†æï¼ˆé€šè¿‡é¢„æµ‹ä¸€è‡´æ€§ï¼‰
        feature_stability = []
        
        for i, feature_name in enumerate(feature_names):
            # åˆ›å»ºè¯¥ç‰¹å¾çš„æ‰°åŠ¨ç‰ˆæœ¬
            X_perturbed = X_test.copy()
            feature_std = np.std(X_test.iloc[:, i])
            X_perturbed.iloc[:, i] += np.random.normal(0, feature_std * 0.1, len(X_test))
            
            # æ¯”è¾ƒåŸå§‹é¢„æµ‹å’Œæ‰°åŠ¨åé¢„æµ‹
            y_prob_original = self.model.predict_proba(X_test)[:, 1]
            y_prob_perturbed = self.model.predict_proba(X_perturbed)[:, 1]
            
            # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
            consistency = 1 - np.mean(np.abs(y_prob_original - y_prob_perturbed))
            feature_stability.append(consistency)
        
        # åˆ›å»ºç‰¹å¾å¯é æ€§å›¾
        plt.figure(figsize=(12, 8))
        feature_stability_df = pd.DataFrame({
            'feature': feature_names,
            'stability': feature_stability
        }).sort_values('stability', ascending=True)
        
        plt.barh(range(len(feature_stability_df)), feature_stability_df['stability'])
        plt.yticks(range(len(feature_stability_df)), feature_stability_df['feature'])
        plt.xlabel('Prediction Stability (1 - Mean Absolute Change)')
        plt.title(f'Feature Reliability Analysis - {self.model_name}')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'feature_reliability.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        reliability_stats = {
            'feature_stability': dict(zip(feature_names, feature_stability)),
            'mean_stability': np.mean(feature_stability),
            'min_stability': np.min(feature_stability),
            'most_stable_feature': feature_names[np.argmax(feature_stability)],
            'least_stable_feature': feature_names[np.argmin(feature_stability)]
        }
        
        print(f"   âœ… Mean Feature Stability: {reliability_stats['mean_stability']:.4f}")
        print(f"   âœ… Most Stable Feature: {reliability_stats['most_stable_feature']}")
        print(f"   âœ… Least Stable Feature: {reliability_stats['least_stable_feature']}")
        
        return reliability_stats
    
    def _ljung_box_test(self, residuals, lags=10):
        """
        Ljung-Boxæ£€éªŒï¼ˆæ£€éªŒæ®‹å·®çš„è‡ªç›¸å…³æ€§ï¼‰
        
        å‚æ•°:
        ----
        residuals : array
            æ®‹å·®
        lags : int
            æ»åé˜¶æ•°
            
        è¿”å›:
        ----
        dict: æ£€éªŒç»“æœ
        """
        try:
            from statsmodels.stats.diagnostic import acorr_ljungbox
            result = acorr_ljungbox(residuals, lags=lags, return_df=True)
            return {
                'statistic': float(result['lb_stat'].iloc[-1]),
                'p_value': float(result['lb_pvalue'].iloc[-1]),
                'significant_autocorr': float(result['lb_pvalue'].iloc[-1]) < 0.05
            }
        except ImportError:
            return {
                'statistic': None,
                'p_value': None,
                'significant_autocorr': None,
                'note': 'statsmodels not available'
            }
    
    def generate_diagnostic_report(self, calibration_results, residual_results, 
                                 distribution_results, reliability_results, 
                                 save_dir='model_diagnostics'):
        """
        ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        
        å‚æ•°:
        ----
        calibration_results : dict
            æ ¡å‡†åˆ†æç»“æœ
        residual_results : dict
            æ®‹å·®åˆ†æç»“æœ
        distribution_results : dict
            åˆ†å¸ƒåˆ†æç»“æœ
        reliability_results : dict
            å¯é æ€§åˆ†æç»“æœ
        save_dir : str
            ä¿å­˜ç›®å½•
        """
        print(f"\nğŸ“ ç”Ÿæˆ{self.model_name}è¯Šæ–­æŠ¥å‘Š...")
        
        report_path = os.path.join(save_dir, f'{self.model_name}_diagnostic_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# {self.model_name} - æ¨¡å‹è¯Šæ–­æŠ¥å‘Š\n\n")
            
            f.write("## 1. æ¨¡å‹æ ¡å‡†åˆ†æ\n\n")
            f.write("### æ ¡å‡†æŒ‡æ ‡\n")
            f.write(f"- **Brier Score**: {calibration_results['brier_score']:.4f}\n")
            f.write(f"- **å¹³å‡æ ¡å‡†è¯¯å·®**: {calibration_results['mean_calibration_error']:.4f}\n")
            f.write(f"- **æœ€å¤§æ ¡å‡†è¯¯å·®**: {calibration_results['max_calibration_error']:.4f}\n\n")
            
            f.write("### è§£é‡Š\n")
            if calibration_results['brier_score'] < 0.1:
                f.write("- âœ… Brieråˆ†æ•°è¾ƒä½ï¼Œæ¨¡å‹æ ¡å‡†è‰¯å¥½\n")
            elif calibration_results['brier_score'] < 0.2:
                f.write("- âš ï¸ Brieråˆ†æ•°ä¸­ç­‰ï¼Œæ¨¡å‹æ ¡å‡†ä¸€èˆ¬\n")
            else:
                f.write("- âŒ Brieråˆ†æ•°è¾ƒé«˜ï¼Œæ¨¡å‹æ ¡å‡†è¾ƒå·®\n")
            
            f.write("\n## 2. æ®‹å·®åˆ†æ\n\n")
            f.write("### æ®‹å·®ç»Ÿè®¡\n")
            f.write(f"- **å¹³å‡æ®‹å·®**: {residual_results['mean_residual']:.4f}\n")
            f.write(f"- **æ®‹å·®æ ‡å‡†å·®**: {residual_results['std_residual']:.4f}\n")
            f.write(f"- **å¹³å‡ç»å¯¹æ®‹å·®**: {residual_results['mean_abs_residual']:.4f}\n")
            f.write(f"- **Shapiro-Wilkæ­£æ€æ€§æ£€éªŒpå€¼**: {residual_results['shapiro_test_pvalue']:.4f}\n\n")
            
            f.write("### è§£é‡Š\n")
            if abs(residual_results['mean_residual']) < 0.01:
                f.write("- âœ… å¹³å‡æ®‹å·®æ¥è¿‘0ï¼Œæ— ç³»ç»Ÿæ€§åå·®\n")
            else:
                f.write("- âš ï¸ å¹³å‡æ®‹å·®åç¦»0ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§åå·®\n")
                
            if residual_results['shapiro_test_pvalue'] > 0.05:
                f.write("- âœ… æ®‹å·®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ\n")
            else:
                f.write("- âš ï¸ æ®‹å·®åç¦»æ­£æ€åˆ†å¸ƒ\n")
            
            f.write("\n## 3. é¢„æµ‹åˆ†å¸ƒåˆ†æ\n\n")
            f.write("### åˆ†å¸ƒç‰¹å¾\n")
            f.write(f"- **è´Ÿç±»é¢„æµ‹æ¦‚ç‡å‡å€¼**: {distribution_results['prob_mean_negative']:.4f}\n")
            f.write(f"- **æ­£ç±»é¢„æµ‹æ¦‚ç‡å‡å€¼**: {distribution_results['prob_mean_positive']:.4f}\n")
            f.write(f"- **ç±»åˆ«åˆ†ç¦»åº¦**: {distribution_results['separation_score']:.4f}\n")
            f.write(f"- **æœ€ä¼˜é˜ˆå€¼**: {distribution_results['optimal_threshold']:.3f}\n")
            f.write(f"- **æœ€å¤§F1åˆ†æ•°**: {distribution_results['max_f1_score']:.4f}\n\n")
            
            f.write("### è§£é‡Š\n")
            if distribution_results['separation_score'] > 0.3:
                f.write("- âœ… ç±»åˆ«åˆ†ç¦»åº¦è‰¯å¥½ï¼Œæ¨¡å‹åŒºåˆ†èƒ½åŠ›å¼º\n")
            elif distribution_results['separation_score'] > 0.1:
                f.write("- âš ï¸ ç±»åˆ«åˆ†ç¦»åº¦ä¸­ç­‰ï¼Œæ¨¡å‹åŒºåˆ†èƒ½åŠ›ä¸€èˆ¬\n")
            else:
                f.write("- âŒ ç±»åˆ«åˆ†ç¦»åº¦è¾ƒä½ï¼Œæ¨¡å‹åŒºåˆ†èƒ½åŠ›è¾ƒå¼±\n")
            
            f.write("\n## 4. ç‰¹å¾å¯é æ€§åˆ†æ\n\n")
            f.write("### å¯é æ€§æŒ‡æ ‡\n")
            f.write(f"- **å¹³å‡ç‰¹å¾ç¨³å®šæ€§**: {reliability_results['mean_stability']:.4f}\n")
            f.write(f"- **æœ€ç¨³å®šç‰¹å¾**: {reliability_results['most_stable_feature']}\n")
            f.write(f"- **æœ€ä¸ç¨³å®šç‰¹å¾**: {reliability_results['least_stable_feature']}\n\n")
            
            f.write("### Top 5 æœ€ç¨³å®šç‰¹å¾\n")
            stability_sorted = sorted(reliability_results['feature_stability'].items(), 
                                    key=lambda x: x[1], reverse=True)
            for i, (feature, stability) in enumerate(stability_sorted[:5]):
                f.write(f"{i+1}. {feature}: {stability:.4f}\n")
            
            f.write("\n## 5. æ€»ä½“è¯Šæ–­ç»“è®º\n\n")
            f.write("### æ¨¡å‹å¥åº·çŠ¶æ€\n")
            
            # ç»¼åˆè¯„åˆ†
            health_score = 0
            if calibration_results['brier_score'] < 0.15:
                health_score += 25
            if abs(residual_results['mean_residual']) < 0.01:
                health_score += 25
            if distribution_results['separation_score'] > 0.2:
                health_score += 25
            if reliability_results['mean_stability'] > 0.8:
                health_score += 25
                
            f.write(f"**ç»¼åˆå¥åº·è¯„åˆ†**: {health_score}/100\n\n")
            
            if health_score >= 80:
                f.write("- âœ… æ¨¡å‹æ•´ä½“å¥åº·çŠ¶æ€è‰¯å¥½\n")
            elif health_score >= 60:
                f.write("- âš ï¸ æ¨¡å‹å¥åº·çŠ¶æ€ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–\n")
            else:
                f.write("- âŒ æ¨¡å‹å¥åº·çŠ¶æ€è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡\n")
        
        print(f"   âœ… è¯Šæ–­æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    
    def run_complete_diagnostics(self, X_test, y_test, save_dir='model_diagnostics'):
        """
        è¿è¡Œå®Œæ•´çš„æ¨¡å‹è¯Šæ–­åˆ†æ
        
        å‚æ•°:
        ----
        X_test : DataFrame
            æµ‹è¯•ç‰¹å¾
        y_test : Series
            æµ‹è¯•æ ‡ç­¾
        save_dir : str
            ä¿å­˜ç›®å½•
            
        è¿”å›:
        ----
        dict: å®Œæ•´è¯Šæ–­ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹{self.model_name}çš„å®Œæ•´è¯Šæ–­åˆ†æ")
        print("=" * 60)
        
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. æ ¡å‡†åˆ†æ
        calibration_results = self.calibration_analysis(X_test, y_test, save_dir=save_dir)
        
        # 2. æ®‹å·®åˆ†æ
        residual_results = self.residual_analysis(X_test, y_test, save_dir=save_dir)
        
        # 3. é¢„æµ‹åˆ†å¸ƒåˆ†æ
        distribution_results = self.prediction_distribution_analysis(X_test, y_test, save_dir=save_dir)
        
        # 4. ç‰¹å¾å¯é æ€§åˆ†æ
        reliability_results = self.feature_reliability_analysis(X_test, y_test, save_dir=save_dir)
        
        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_diagnostic_report(
            calibration_results, residual_results, 
            distribution_results, reliability_results, 
            save_dir=save_dir
        )
        
        print(f"\nğŸ‰ {self.model_name}å®Œæ•´è¯Šæ–­åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
        
        return {
            'calibration': calibration_results,
            'residual': residual_results,
            'distribution': distribution_results,
            'reliability': reliability_results,
            'model_name': self.model_name
        } 