"""
Performance Comparison Analysis for Feature Selection
ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†ææ¨¡å—
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

class PerformanceComparator:
    """ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.comparison_results = {}
        self.timing_results = {}
        self.memory_results = {}
        
    def comprehensive_comparison(self, pipeline, train_path, test_path=None, 
                                feature_selection_configs=None):
        """
        å…¨é¢çš„ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†æ
        
        Parameters:
        -----------
        pipeline : CESDPredictionPipeline
            ä¸»æµæ°´çº¿å¯¹è±¡
        train_path : str
            è®­ç»ƒæ•°æ®è·¯å¾„
        test_path : str, optional
            æµ‹è¯•æ•°æ®è·¯å¾„
        feature_selection_configs : list
            ç‰¹å¾é€‰æ‹©é…ç½®åˆ—è¡¨
        """
        print(f"\n{'='*80}")
        print("ğŸ” ç‰¹å¾é€‰æ‹©å…¨é¢æ•ˆèƒ½å¯¹æ¯”åˆ†æ")
        print(f"{'='*80}")
        
        # é»˜è®¤ç‰¹å¾é€‰æ‹©é…ç½®
        if feature_selection_configs is None:
            feature_selection_configs = [
                {
                    'name': 'NoSelection',
                    'use_feature_selection': False,
                    'description': 'ä¸ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆå…¨ç‰¹å¾ï¼‰'
                },
                {
                    'name': 'Conservative_20',
                    'use_feature_selection': True,
                    'k_best': 20,
                    'methods': ['variance', 'univariate', 'rfe', 'model_based'],
                    'description': 'ä¿å®ˆç­–ç•¥ï¼ˆ20ä¸ªç‰¹å¾ï¼‰'
                },
                {
                    'name': 'Moderate_15',
                    'use_feature_selection': True,
                    'k_best': 15,
                    'methods': ['univariate', 'rfe', 'model_based'],
                    'description': 'ä¸­ç­‰ç­–ç•¥ï¼ˆ15ä¸ªç‰¹å¾ï¼‰'
                },
                {
                    'name': 'Aggressive_10',
                    'use_feature_selection': True,
                    'k_best': 10,
                    'methods': ['rfe', 'model_based'],
                    'description': 'æ¿€è¿›ç­–ç•¥ï¼ˆ10ä¸ªç‰¹å¾ï¼‰'
                },
                {
                    'name': 'Statistical_Only',
                    'use_feature_selection': True,
                    'k_best': 18,
                    'methods': ['variance', 'univariate'],
                    'description': 'ä»…ç»Ÿè®¡æ–¹æ³•ï¼ˆ18ä¸ªç‰¹å¾ï¼‰'
                }
            ]
        
        # å¯¹æ¯ç§é…ç½®è¿›è¡Œå®Œæ•´åˆ†æ
        for config in feature_selection_configs:
            print(f"\n{'='*60}")
            print(f"ğŸ”§ æµ‹è¯•é…ç½®: {config['name']}")
            print(f"ğŸ“ æè¿°: {config['description']}")
            print(f"{'='*60}")
            
            # é‡æ–°åˆå§‹åŒ–æµæ°´çº¿
            pipeline.__init__(self.random_state)
            
            # è¿è¡Œåˆ†æå¹¶è®°å½•æ€§èƒ½
            config_results = self._run_single_configuration(
                pipeline, train_path, test_path, config
            )
            
            self.comparison_results[config['name']] = config_results
        
        # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        self._generate_comprehensive_report()
        
        return self.comparison_results
    
    def _run_single_configuration(self, pipeline, train_path, test_path, config):
        """è¿è¡Œå•ä¸ªé…ç½®çš„å®Œæ•´åˆ†æ"""
        results = {
            'config': config,
            'timing': {},
            'memory': {},
            'performance': {},
            'feature_info': {}
        }
        
        try:
            # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
            start_time = time.time()
            pipeline.load_and_preprocess_data(train_path, test_path, use_smote=False)
            preprocessing_time = time.time() - start_time
            
            results['timing']['preprocessing'] = preprocessing_time
            results['feature_info']['original_features'] = pipeline.X_train.shape[1]
            results['feature_info']['training_samples'] = pipeline.X_train.shape[0]
            
            # 2. ç‰¹å¾é€‰æ‹©ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config['use_feature_selection']:
                start_time = time.time()
                pipeline.apply_feature_selection(
                    methods=config.get('methods', ['variance', 'univariate', 'rfe', 'model_based']),
                    k_best=config.get('k_best', 20),
                    variance_threshold=config.get('variance_threshold', 0.01)
                )
                feature_selection_time = time.time() - start_time
                
                results['timing']['feature_selection'] = feature_selection_time
                results['feature_info']['selected_features'] = pipeline.X_train_selected.shape[1]
                results['feature_info']['feature_reduction_ratio'] = (
                    1 - pipeline.X_train_selected.shape[1] / pipeline.X_train.shape[1]
                )
            else:
                results['timing']['feature_selection'] = 0
                results['feature_info']['selected_features'] = pipeline.X_train.shape[1]
                results['feature_info']['feature_reduction_ratio'] = 0
            
            # 3. æ¨¡å‹è®­ç»ƒ
            start_time = time.time()
            models = pipeline.train_models(use_feature_selection=config['use_feature_selection'])
            training_time = time.time() - start_time
            
            results['timing']['training'] = training_time
            results['feature_info']['models_trained'] = len(models)
            
            # 4. æ¨¡å‹è¯„ä¼°
            start_time = time.time()
            evaluation_results = pipeline.evaluate_models(use_feature_selection=config['use_feature_selection'])
            evaluation_time = time.time() - start_time
            
            results['timing']['evaluation'] = evaluation_time
            results['timing']['total'] = sum(results['timing'].values())
            
            # 5. æå–æ€§èƒ½æŒ‡æ ‡
            for model_name, metrics in evaluation_results.items():
                results['performance'][model_name] = {
                    'auroc': metrics['auroc']['value'],
                    'auprc': metrics['auprc']['value'],
                    'accuracy': metrics['accuracy']['value'],
                    'f1_score': metrics['f1_score']['value'],
                    'precision': metrics['precision']['value'],
                    'recall': metrics['recall']['value']
                }
            
            # 6. è®¡ç®—å¹³å‡æ€§èƒ½
            all_auroc = [m['auroc'] for m in results['performance'].values()]
            all_f1 = [m['f1_score'] for m in results['performance'].values()]
            
            results['performance']['average'] = {
                'auroc': np.mean(all_auroc),
                'auroc_std': np.std(all_auroc),
                'f1_score': np.mean(all_f1),
                'f1_std': np.std(all_f1),
                'best_auroc': max(all_auroc),
                'best_f1': max(all_f1)
            }
            
            print(f"âœ… {config['name']} é…ç½®å®Œæˆ")
            print(f"   ç‰¹å¾æ•°: {results['feature_info']['original_features']} â†’ {results['feature_info']['selected_features']}")
            print(f"   å¹³å‡AUROC: {results['performance']['average']['auroc']:.3f}")
            print(f"   æ€»ç”¨æ—¶: {results['timing']['total']:.1f}ç§’")
            
        except Exception as e:
            print(f"âŒ {config['name']} é…ç½®å¤±è´¥: {e}")
            results['error'] = str(e)
        
        return results
    
    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”ç»¼åˆæŠ¥å‘Š")
        print(f"{'='*80}")
        
        # 1. æ€§èƒ½å¯¹æ¯”è¡¨
        self._generate_performance_table()
        
        # 2. æ—¶é—´æ•ˆç‡å¯¹æ¯”
        self._generate_timing_analysis()
        
        # 3. ç‰¹å¾ç»´åº¦å¯¹æ¯”
        self._generate_feature_analysis()
        
        # 4. å¯è§†åŒ–å¯¹æ¯”
        self._generate_visualizations()
        
        # 5. ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_detailed_results()
    
    def _generate_performance_table(self):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨"""
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
        print("-" * 80)
        
        performance_data = []
        
        for config_name, results in self.comparison_results.items():
            if 'error' not in results:
                row = {
                    'Configuration': config_name,
                    'Features': results['feature_info']['selected_features'],
                    'Reduction': f"{results['feature_info']['feature_reduction_ratio']*100:.1f}%",
                    'Avg_AUROC': results['performance']['average']['auroc'],
                    'Best_AUROC': results['performance']['average']['best_auroc'],
                    'Avg_F1': results['performance']['average']['f1_score'],
                    'Best_F1': results['performance']['average']['best_f1'],
                    'Total_Time': results['timing']['total']
                }
                performance_data.append(row)
        
        performance_df = pd.DataFrame(performance_data)
        performance_df = performance_df.sort_values('Avg_AUROC', ascending=False)
        
        # æ˜¾ç¤ºè¡¨æ ¼
        print(performance_df.to_string(index=False, float_format='%.3f'))
        
        # ä¿å­˜è¡¨æ ¼
        performance_df.to_csv('feature_selection_performance_comparison.csv', 
                             index=False, encoding='utf-8-sig')
        
        # è®¡ç®—æœ€ä½³é…ç½®
        best_config = performance_df.iloc[0]
        baseline_config = performance_df[performance_df['Configuration'] == 'NoSelection']
        
        if not baseline_config.empty:
            baseline_auroc = baseline_config['Avg_AUROC'].iloc[0]
            best_auroc = best_config['Avg_AUROC']
            improvement = ((best_auroc - baseline_auroc) / baseline_auroc) * 100
            
            print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config['Configuration']}")
            print(f"   AUROCæ”¹è¿›: {improvement:+.2f}%")
            print(f"   ç‰¹å¾å‡å°‘: {best_config['Reduction']}")
            print(f"   æ—¶é—´èŠ‚çœ: {(1 - best_config['Total_Time'] / baseline_config['Total_Time'].iloc[0]) * 100:.1f}%")
    
    def _generate_timing_analysis(self):
        """ç”Ÿæˆæ—¶é—´æ•ˆç‡åˆ†æ"""
        print(f"\nâ±ï¸ æ—¶é—´æ•ˆç‡åˆ†æ")
        print("-" * 50)
        
        timing_data = []
        for config_name, results in self.comparison_results.items():
            if 'error' not in results:
                timing_data.append({
                    'Configuration': config_name,
                    'Features': results['feature_info']['selected_features'],
                    'Preprocessing': results['timing']['preprocessing'],
                    'Feature_Selection': results['timing']['feature_selection'],
                    'Training': results['timing']['training'],
                    'Evaluation': results['timing']['evaluation'],
                    'Total': results['timing']['total']
                })
        
        timing_df = pd.DataFrame(timing_data)
        print(timing_df.to_string(index=False, float_format='%.2f'))
        
        # è®¡ç®—æ—¶é—´èŠ‚çœ
        baseline_time = timing_df[timing_df['Configuration'] == 'NoSelection']['Total'].iloc[0]
        timing_df['Time_Savings'] = ((baseline_time - timing_df['Total']) / baseline_time * 100)
        
        print(f"\nğŸ’¡ æ—¶é—´æ•ˆç‡æ´å¯Ÿ:")
        for _, row in timing_df.iterrows():
            if row['Configuration'] != 'NoSelection':
                print(f"   {row['Configuration']:15s}: {row['Time_Savings']:+6.1f}% æ—¶é—´å˜åŒ–")
    
    def _generate_feature_analysis(self):
        """ç”Ÿæˆç‰¹å¾ç»´åº¦åˆ†æ"""
        print(f"\nğŸ” ç‰¹å¾ç»´åº¦åˆ†æ")
        print("-" * 50)
        
        feature_data = []
        for config_name, results in self.comparison_results.items():
            if 'error' not in results:
                feature_data.append({
                    'Configuration': config_name,
                    'Original': results['feature_info']['original_features'],
                    'Selected': results['feature_info']['selected_features'],
                    'Reduction_Ratio': results['feature_info']['feature_reduction_ratio'],
                    'Avg_AUROC': results['performance']['average']['auroc']
                })
        
        feature_df = pd.DataFrame(feature_data)
        print(feature_df.to_string(index=False, float_format='%.3f'))
        
        # åˆ†æç‰¹å¾æ•°é‡ä¸æ€§èƒ½çš„å…³ç³»
        correlation = feature_df['Selected'].corr(feature_df['Avg_AUROC'])
        print(f"\nğŸ“Š ç‰¹å¾æ•°é‡ä¸AUROCç›¸å…³æ€§: {correlation:.3f}")
        
        if correlation > 0.5:
            print("   ğŸ’­ æ›´å¤šç‰¹å¾é€šå¸¸å¸¦æ¥æ›´å¥½æ€§èƒ½")
        elif correlation < -0.5:
            print("   ğŸ’­ æ›´å°‘ç‰¹å¾å¯èƒ½å¸¦æ¥æ›´å¥½æ€§èƒ½ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰")
        else:
            print("   ğŸ’­ ç‰¹å¾æ•°é‡ä¸æ€§èƒ½å…³ç³»ä¸æ˜æ˜¾ï¼Œè´¨é‡æ¯”æ•°é‡æ›´é‡è¦")
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾...")
        
        # å‡†å¤‡æ•°æ®
        configs = []
        aurocs = []
        f1s = []
        features = []
        times = []
        
        for config_name, results in self.comparison_results.items():
            if 'error' not in results:
                configs.append(config_name)
                aurocs.append(results['performance']['average']['auroc'])
                f1s.append(results['performance']['average']['f1_score'])
                features.append(results['feature_info']['selected_features'])
                times.append(results['timing']['total'])
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»º2x2å­å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. AUROCå¯¹æ¯”
        bars1 = ax1.bar(configs, aurocs, color='skyblue', alpha=0.7)
        ax1.set_title('AUROC Performance Comparison', fontsize=14, fontweight='bold')
        ax1.set_ylabel('AUROC Score')
        ax1.set_ylim(min(aurocs) - 0.01, max(aurocs) + 0.01)
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, aurocs):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # 2. ç‰¹å¾æ•°é‡å¯¹æ¯”
        bars2 = ax2.bar(configs, features, color='lightgreen', alpha=0.7)
        ax2.set_title('Feature Count Comparison', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Number of Features')
        ax2.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars2, features):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value}', ha='center', va='bottom', fontsize=10)
        
        # 3. æ—¶é—´æ•ˆç‡å¯¹æ¯”
        bars3 = ax3.bar(configs, times, color='orange', alpha=0.7)
        ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Total Time (seconds)')
        ax3.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars3, times):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01,
                    f'{value:.1f}s', ha='center', va='bottom', fontsize=10)
        
        # 4. æ•ˆç‡vsæ€§èƒ½æ•£ç‚¹å›¾
        scatter = ax4.scatter(features, aurocs, s=[t*10 for t in times], 
                             c=range(len(configs)), cmap='viridis', alpha=0.7)
        ax4.set_title('Efficiency vs Performance', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Number of Features')
        ax4.set_ylabel('AUROC Score')
        
        # æ·»åŠ é…ç½®æ ‡ç­¾
        for i, config in enumerate(configs):
            ax4.annotate(config, (features[i], aurocs[i]), 
                        xytext=(5, 5), textcoords='offset points', 
                        fontsize=9, alpha=0.8)
        
        plt.tight_layout()
        plt.savefig('feature_selection_comprehensive_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: feature_selection_comprehensive_comparison.png")
    
    def _save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        # ä¿å­˜å®Œæ•´çš„å¯¹æ¯”ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONæ ¼å¼è¯¦ç»†ç»“æœ
        detailed_file = f'feature_selection_detailed_comparison_{timestamp}.json'
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(self.comparison_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary_file = f'feature_selection_summary_report_{timestamp}.md'
        self._generate_summary_report(summary_file)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜:")
        print(f"   - {detailed_file}")
        print(f"   - {summary_file}")
        print(f"   - feature_selection_performance_comparison.csv")
        print(f"   - feature_selection_comprehensive_comparison.png")
    
    def _generate_summary_report(self, filename):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("# ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“Š ä¸»è¦å‘ç°\n\n")
            
            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best_auroc = 0
            best_config = None
            baseline_auroc = 0
            
            for config_name, results in self.comparison_results.items():
                if 'error' not in results:
                    auroc = results['performance']['average']['auroc']
                    if config_name == 'NoSelection':
                        baseline_auroc = auroc
                    elif auroc > best_auroc:
                        best_auroc = auroc
                        best_config = config_name
            
            if best_config and baseline_auroc > 0:
                improvement = ((best_auroc - baseline_auroc) / baseline_auroc) * 100
                f.write(f"### ğŸ† æœ€ä½³ç‰¹å¾é€‰æ‹©ç­–ç•¥: {best_config}\n")
                f.write(f"- **æ€§èƒ½æ”¹è¿›**: {improvement:+.2f}%\n")
                f.write(f"- **æœ€ä½³AUROC**: {best_auroc:.3f}\n")
                f.write(f"- **åŸºçº¿AUROC**: {baseline_auroc:.3f}\n\n")
            
            f.write("## ğŸ“ˆ è¯¦ç»†ç»“æœ\n\n")
            f.write("| é…ç½® | ç‰¹å¾æ•° | å¹³å‡AUROC | æœ€ä½³AUROC | æ—¶é—´(ç§’) |\n")
            f.write("|------|--------|-----------|-----------|----------|\n")
            
            for config_name, results in self.comparison_results.items():
                if 'error' not in results:
                    f.write(f"| {config_name} | {results['feature_info']['selected_features']} | "
                           f"{results['performance']['average']['auroc']:.3f} | "
                           f"{results['performance']['average']['best_auroc']:.3f} | "
                           f"{results['timing']['total']:.1f} |\n")
            
            f.write("\n## ğŸ’¡ å»ºè®®\n\n")
            
            if best_config:
                best_results = self.comparison_results[best_config]
                f.write(f"1. **æ¨èä½¿ç”¨ {best_config} ç­–ç•¥**\n")
                f.write(f"   - ç‰¹å¾æ•°é‡: {best_results['feature_info']['selected_features']}\n")
                f.write(f"   - ç‰¹å¾é€‰æ‹©æ–¹æ³•: {', '.join(best_results['config'].get('methods', []))}\n")
                f.write(f"   - é¢„æœŸæ€§èƒ½æå‡: {improvement:+.2f}%\n\n")
            
            f.write("2. **æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©**\n")
            f.write("   - è¿½æ±‚æœ€é«˜æ€§èƒ½: ä½¿ç”¨æœ€ä½³é…ç½®\n")
            f.write("   - è¿½æ±‚è®¡ç®—æ•ˆç‡: é€‰æ‹©ç‰¹å¾æ•°è¾ƒå°‘çš„é…ç½®\n")
            f.write("   - è¿½æ±‚æ¨¡å‹è§£é‡Šæ€§: é€‰æ‹©10-15ä¸ªç‰¹å¾çš„é…ç½®\n\n")

def create_performance_comparison_example():
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”ç¤ºä¾‹å‡½æ•°"""
    print("ğŸš€ ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†æç¤ºä¾‹")
    print("=" * 50)
    
    try:
        from ..core.main_pipeline import CESDPredictionPipeline
        
        # åˆ›å»ºæµæ°´çº¿å’Œå¯¹æ¯”åˆ†æå™¨
        pipeline = CESDPredictionPipeline(random_state=42)
        comparator = PerformanceComparator(random_state=42)
        
        # è¿è¡Œå…¨é¢å¯¹æ¯”åˆ†æ
        results = comparator.comprehensive_comparison(
            pipeline=pipeline,
            train_path="charls2018 20250722.csv",
            test_path="klosa2018 20250722.csv"
        )
        
        print("\nâœ… ç‰¹å¾é€‰æ‹©æ•ˆèƒ½å¯¹æ¯”åˆ†æå®Œæˆï¼")
        return results
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    create_performance_comparison_example() 