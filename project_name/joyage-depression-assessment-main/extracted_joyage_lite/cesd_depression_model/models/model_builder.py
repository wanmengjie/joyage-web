"""
Model builder module for CESD Depression Prediction Model
"""

import numpy as np
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                            VotingClassifier, StackingClassifier, 
                            ExtraTreesClassifier, AdaBoostClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import xgboost as xgb
import lightgbm as lgb
try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸ CatBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡CatBoostæ¨¡å‹")
from ..config import MODEL_PARAMS, CV_SETTINGS, N_JOBS

class ModelBuilder:
    """æ¨¡å‹æ„å»ºç±»"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.best_params = {}
        self.data_processor = None
        
    def set_data_processor(self, data_processor):
        """è®¾ç½®æ•°æ®å¤„ç†å™¨"""
        self.data_processor = data_processor
        
    def build_base_models_with_preprocessing(self, train_data, use_pre_encoded_data=False):
        """
        æ„å»ºåŸºç¡€æ¨¡å‹ï¼ˆæ”¯æŒæ¨¡å‹ç‰¹å®šé¢„å¤„ç†ï¼‰
        
        å‚æ•°:
        ----
        train_data : DataFrame
            åŒ…å«ç›®æ ‡å˜é‡çš„è®­ç»ƒæ•°æ®
        use_pre_encoded_data : bool
            æ•°æ®æ˜¯å¦å·²ç»ç¼–ç 
        """
        print(f"\nğŸ”§ æ„å»ºåŸºç¡€æ¨¡å‹ï¼ˆæ”¯æŒæ¨¡å‹ç‰¹å®šé¢„å¤„ç†ï¼‰...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        target_col = 'depressed'
        if target_col not in train_data.columns:
            raise ValueError(f"ç›®æ ‡å˜é‡ '{target_col}' ä¸åœ¨æ•°æ®ä¸­")
        
        feature_cols = [col for col in train_data.columns if col != target_col]
        X = train_data[feature_cols].copy()
        y = train_data[target_col].copy()
        
        print(f"ğŸ“Š ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(X)}")
        
        models = {}
        self.model_preprocessed_data = {}  # å­˜å‚¨æ¨¡å‹ç‰¹å®šçš„é¢„å¤„ç†ä¿¡æ¯
        
        if not use_pre_encoded_data:
            # åŸæœ‰çš„é¢„å¤„ç†é€»è¾‘
            return self.build_base_models(), {}
        else:
            # ä½¿ç”¨å·²ç¼–ç çš„æ•°æ®ï¼Œä½†éœ€è¦å¤„ç†è‡ªé€‚åº”SVM
            print(f"âœ… ä½¿ç”¨å·²ç¼–ç æ•°æ®æ„å»ºæ¨¡å‹...")
            
            # 1. è®­ç»ƒæ ‘æ¨¡å‹ï¼ˆä¸éœ€è¦é¢å¤–é¢„å¤„ç†ï¼‰
            tree_models = self._get_tree_models()
            
            print(f"\nğŸŒ³ è®­ç»ƒæ ‘æ¨¡å‹...")
            for name, model in tree_models.items():
                try:
                    print(f"   è®­ç»ƒ {name}...")
                    model.fit(X, y)
                    models[name] = model
                    
                    # è®°å½•é¢„å¤„ç†ä¿¡æ¯
                    self.model_preprocessed_data[name] = {
                        'model_type': 'tree',
                        'preprocessing': 'none'
                    }
                    
                    print(f"   âœ“ {name} è®­ç»ƒå®Œæˆ")
                except Exception as e:
                    print(f"   âœ— {name} è®­ç»ƒå¤±è´¥: {e}")
            
            # 2. è®­ç»ƒçº¿æ€§æ¨¡å‹ï¼ˆéœ€è¦é¢å¤–é¢„å¤„ç†ï¼‰
            linear_models = self._get_linear_models()
            
            print(f"\nğŸ“ˆ è®­ç»ƒçº¿æ€§æ¨¡å‹ (éœ€è¦æ ‡å‡†åŒ–)...")
            try:
                # å¯¹äºçº¿æ€§æ¨¡å‹ï¼Œåªè¿›è¡Œæ ‡å‡†åŒ–ï¼Œä¸é‡æ–°ç¼–ç 
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                for name, model_config in linear_models.items():
                    try:
                        print(f"   è®­ç»ƒ {name}...")
                        
                        # å¤„ç†è‡ªé€‚åº”SVM
                        if name == 'svc' and model_config == 'adaptive_svc':
                            model = self._create_adaptive_svm(X_scaled, y)
                        else:
                            model = model_config
                        
                        model.fit(X_scaled, y)
                        models[name] = model
                        
                        # è®°å½•é¢„å¤„ç†ä¿¡æ¯
                        self.model_preprocessed_data[name] = {
                            'model_type': 'linear',
                            'preprocessing': 'standardization',
                            'scaler': scaler
                        }
                        
                        print(f"   âœ“ {name} è®­ç»ƒå®Œæˆ")
                    except Exception as e:
                        print(f"   âœ— {name} è®­ç»ƒå¤±è´¥: {e}")
            except Exception as e:
                print(f"   âš ï¸ çº¿æ€§æ¨¡å‹æ ‡å‡†åŒ–å¤±è´¥: {e}")
            
            # 3. è·³è¿‡é›†æˆæ¨¡å‹è®­ç»ƒï¼ˆç”¨æˆ·è¦æ±‚ç¦ç”¨ï¼‰
            print(f"\nğŸš« è·³è¿‡é›†æˆæ¨¡å‹è®­ç»ƒï¼ˆç”¨æˆ·è¦æ±‚ç¦ç”¨æ‰€æœ‰é›†æˆæ¨¡å‹ï¼‰")
            print(f"   âœ… åŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå…± {len(models)} ä¸ªæ¨¡å‹")
            print(f"   ğŸ“ˆ åŸºç¡€æ¨¡å‹å·²æä¾›ä¼˜ç§€æ€§èƒ½ï¼Œæ— éœ€é›†æˆæ¨¡å‹")
            
        # è¿”å›æ¨¡å‹å’Œé¢„å¤„ç†ä¿¡æ¯
        return models, self.model_preprocessed_data
    
    def _get_tree_models(self):
        """è·å–æ ‘æ¨¡å‹å­—å…¸"""
        from ..config import N_JOBS
        
        tree_models = {
            'rf': RandomForestClassifier(random_state=self.random_state, n_jobs=N_JOBS),
            'gb': GradientBoostingClassifier(random_state=self.random_state),
            'xgb': xgb.XGBClassifier(random_state=self.random_state, n_jobs=N_JOBS, eval_metric='logloss'),
            'lgb': lgb.LGBMClassifier(random_state=self.random_state, n_jobs=N_JOBS, verbose=-1),
            'extra_trees': ExtraTreesClassifier(random_state=self.random_state, n_jobs=N_JOBS),
            'adaboost': AdaBoostClassifier(random_state=self.random_state)
        }
        
        # æ£€æŸ¥CatBoostæ˜¯å¦å¯ç”¨
        try:
            from catboost import CatBoostClassifier
            tree_models['catboost'] = CatBoostClassifier(
                random_state=self.random_state,
                verbose=False,
                allow_writing_files=False
            )
        except ImportError:
            print("âš ï¸ CatBoostä¸å¯ç”¨ï¼Œè·³è¿‡")
        
        return tree_models
    
    def _get_linear_models(self):
        """è·å–çº¿æ€§æ¨¡å‹å­—å…¸"""
        from ..config import N_JOBS
        
        return {
            # ğŸ”§ ä¼˜åŒ–é€»è¾‘å›å½’ï¼šå¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œä¿®æ”¹æ±‚è§£å™¨ï¼Œè°ƒæ•´å®¹å¿åº¦
            'lr': LogisticRegression(
                random_state=self.random_state, 
                max_iter=5000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                solver='liblinear',  # æ›´æ¢æ±‚è§£å™¨ï¼Œé€šå¸¸æ”¶æ•›æ›´å¿«
                tol=1e-3,  # æ”¾å®½å®¹å¿åº¦
                n_jobs=N_JOBS
            ),
            # ğŸš« ç”¨æˆ·è¦æ±‚ï¼šå®Œå…¨ç¦ç”¨SVCæ¨¡å‹ï¼ˆé¿å…é•¿æ—¶é—´è®­ç»ƒï¼‰
            # 'svc': CalibratedClassifierCV(
            #     LinearSVC(random_state=self.random_state, max_iter=5000, dual=False),
            #     method='sigmoid',  # sigmoidæ¯”isotonicæ›´å¿«
            #     cv=3,              # 3æŠ˜è¶³å¤Ÿï¼Œæ¯”5æŠ˜å¿«
            #     n_jobs=1           # é¿å…åµŒå¥—å¹¶è¡Œ
            # )
        }
    
    def _get_optimal_svm(self):
        """
        æ™ºèƒ½é€‰æ‹©æœ€ä¼˜çš„SVMå®ç°
        
        æ ¹æ®æ•°æ®è§„æ¨¡å’Œè®¡ç®—èµ„æºé€‰æ‹©ï¼š
        - å°æ•°æ®é›† (< 3000æ ·æœ¬): ä½¿ç”¨SVC with RBFæ ¸
        - ä¸­ç­‰æ•°æ®é›† (3000-8000æ ·æœ¬): ä½¿ç”¨SVC with çº¿æ€§æ ¸
        - å¤§æ•°æ®é›† (> 8000æ ·æœ¬): ä½¿ç”¨SVC with çº¿æ€§æ ¸ï¼ˆä½†å¢åŠ Cå‚æ•°é™åˆ¶ï¼‰
        """
        # è¿™é‡Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨å®é™…ä½¿ç”¨æ—¶æ ¹æ®æ•°æ®è§„æ¨¡å†³å®š
        return 'adaptive_svc'  # æ ‡è®°ä¸ºè‡ªé€‚åº”SVC
    
    def _create_adaptive_svm(self, X_train, y_train):
        """
        æ ¹æ®æ•°æ®è§„æ¨¡åˆ›å»ºåˆé€‚çš„SVCæ¨¡å‹
        """
        sample_count = len(X_train)
        
        if sample_count < 3000:
            # å°æ•°æ®é›†ï¼šä½¿ç”¨RBFæ ¸SVCï¼Œæ”¯æŒæ¦‚ç‡é¢„æµ‹
            print(f"   æ•°æ®é›†è¾ƒå°({sample_count}æ ·æœ¬)ï¼Œä½¿ç”¨RBFæ ¸SVC")
            return SVC(
                kernel='rbf', 
                probability=True, 
                random_state=self.random_state,
                gamma='scale',
                C=1.0
            )
        elif sample_count < 8000:
            # ä¸­ç­‰æ•°æ®é›†ï¼šä½¿ç”¨çº¿æ€§æ ¸SVCï¼Œæ”¯æŒæ¦‚ç‡é¢„æµ‹
            print(f"   æ•°æ®é›†ä¸­ç­‰({sample_count}æ ·æœ¬)ï¼Œä½¿ç”¨çº¿æ€§æ ¸SVC")
            return SVC(
                kernel='linear', 
                probability=True, 
                random_state=self.random_state,
                C=1.0
            )
        else:
            # å¤§æ•°æ®é›†ï¼šä½¿ç”¨çº¿æ€§æ ¸SVCï¼Œé™åˆ¶Cå‚æ•°ä»¥æé«˜é€Ÿåº¦
            print(f"   æ•°æ®é›†è¾ƒå¤§({sample_count}æ ·æœ¬)ï¼Œä½¿ç”¨çº¿æ€§æ ¸SVCï¼ˆä¼˜åŒ–å‚æ•°ï¼‰")
            return SVC(
                kernel='linear', 
                probability=True, 
                random_state=self.random_state,
                C=0.1  # è¾ƒå°çš„Cå€¼ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
            )
        
    def build_base_models(self):
        """æ„å»ºåŸºç¡€æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        base_models = {
            'rf': RandomForestClassifier(random_state=self.random_state, n_jobs=N_JOBS),
            'gb': GradientBoostingClassifier(random_state=self.random_state),
            'xgb': xgb.XGBClassifier(random_state=self.random_state, eval_metric='logloss', n_jobs=N_JOBS),
            'lgb': lgb.LGBMClassifier(random_state=self.random_state, verbosity=-1, n_jobs=N_JOBS),
            'lr': LogisticRegression(
                random_state=self.random_state, 
                max_iter=5000, 
                solver='liblinear', 
                tol=1e-3, 
                n_jobs=N_JOBS
            ),
            # ğŸš« ç”¨æˆ·è¦æ±‚ï¼šå®Œå…¨ç¦ç”¨SVCæ¨¡å‹ï¼ˆé¿å…é•¿æ—¶é—´è®­ç»ƒï¼‰
            # 'svc': SVC(kernel='linear', probability=True, random_state=self.random_state),
            # ğŸ†• æ–°å¢æ¨¡å‹
            'extra_trees': ExtraTreesClassifier(random_state=self.random_state, n_jobs=N_JOBS),
            'adaboost': AdaBoostClassifier(random_state=self.random_state),
        }
        
        # ğŸ†• æ·»åŠ CatBoost (å¦‚æœå¯ç”¨)
        if CATBOOST_AVAILABLE:
            base_models['catboost'] = CatBoostClassifier(
                random_state=self.random_state, 
                verbose=False,
                iterations=100
            )
        
        return base_models
        
    def build_ensemble_models(self, base_models, X=None, y=None):
        """
        æ„å»ºé›†æˆæ¨¡å‹ï¼ˆä»…ä¿ç•™è½¯æŠ•ç¥¨å’ŒåŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨ï¼‰
        
        å‚æ•°:
        ----
        base_models : dict
            åŸºç¡€æ¨¡å‹å­—å…¸ï¼Œé”®ä¸ºæ¨¡å‹åç§°ï¼Œå€¼ä¸ºæ¨¡å‹å¯¹è±¡
        X : array-like, å¯é€‰
            è®­ç»ƒæ•°æ®ï¼Œç”¨äºæŸäº›éœ€è¦è®­ç»ƒçš„é›†æˆæ¨¡å‹
        y : array-like, å¯é€‰
            ç›®æ ‡å˜é‡ï¼Œç”¨äºæŸäº›éœ€è¦è®­ç»ƒçš„é›†æˆæ¨¡å‹
            
        è¿”å›:
        ----
        dict : é›†æˆæ¨¡å‹å­—å…¸
        """
        print("ğŸ”§ æ„å»ºç²¾é€‰é›†æˆæ¨¡å‹ï¼šä»…ä¿ç•™åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨")
        print("   è·³è¿‡è½¯æŠ•ç¥¨ã€ç¡¬æŠ•ç¥¨å’Œå †å åˆ†ç±»å™¨")
        
        from sklearn.ensemble import VotingClassifier
        ensemble_models = {}
        
        # è·³è¿‡è½¯æŠ•ç¥¨åˆ†ç±»å™¨ - ç”¨æˆ·è¦æ±‚ç¦ç”¨
        print("\nğŸš« è·³è¿‡è½¯æŠ•ç¥¨åˆ†ç±»å™¨ï¼ˆç”¨æˆ·è¦æ±‚ç¦ç”¨ï¼‰")
        
        try:
            # 1. åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨ (Weighted Voting) - å”¯ä¸€ä¿ç•™çš„é›†æˆæ¨¡å‹
            print("\nâš–ï¸ åˆ›å»ºåŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨...")
            
            # ç­›é€‰æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹
            proba_models = {}
            for name, model in base_models.items():
                if hasattr(model, 'predict_proba'):
                    proba_models[name] = model
            
            if len(proba_models) >= 2:
                estimators = [(name, model) for name, model in proba_models.items()]
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®æƒé‡ (åŸºäºæ‚¨æ•°æ®çš„æ€§èƒ½è¡¨ç°)
                model_weights = {
                    'adaboost': 1.2,    # AdaBoostè¡¨ç°æœ€å¥½ (AUROC 0.7529)
                    'gb': 1.1,          # GradientBoostingç¬¬äºŒ (AUROC 0.7520)
                    'lgb': 1.1,         # LightGBMç¬¬ä¸‰ (AUROC 0.7494)
                    'catboost': 1.1,    # CatBoostç¬¬å›› (AUROC 0.7474)
                    'lr': 1.0,          # é€»è¾‘å›å½’ç¬¬äº” (AUROC 0.7473)
                    'xgb': 1.0,         # XGBoost
                    'rf': 1.0,          # RandomForest
                    'extra_trees': 0.9, # ExtraTrees
                    'svc': 0.9          # SVC
                }
                
                # æ„å»ºæƒé‡åˆ—è¡¨
                weights = []
                for name, _ in estimators:
                    weight = model_weights.get(name, 1.0)
                    weights.append(weight)
                
                weighted_voting = VotingClassifier(
                    estimators=estimators, 
                    voting='soft',
                    weights=weights
                )
                
                # å¦‚æœæä¾›äº†è®­ç»ƒæ•°æ®ï¼Œåˆ™è®­ç»ƒåˆ†ç±»å™¨
                if X is not None and y is not None:
                    weighted_voting.fit(X, y)
                    
                ensemble_models['weighted_voting'] = weighted_voting
                
                print(f"âœ… åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ (ä½¿ç”¨{len(proba_models)}ä¸ªæ¨¡å‹)")
                print("   æƒé‡åˆ†é…:")
                for (name, _), weight in zip(estimators, weights):
                    print(f"     â€¢ {name}: {weight}")
            else:
                print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œè·³è¿‡åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨")
                
        except Exception as e:
            print(f"âŒ åˆ›å»ºåŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨å¤±è´¥: {str(e)}")
        
        print(f"\nğŸ“ˆ ç²¾é€‰é›†æˆæ¨¡å‹æ„å»ºå®Œæˆï¼Œå…±åˆ›å»º {len(ensemble_models)} ä¸ªé›†æˆæ¨¡å‹")
        print("   é¢„æœŸè®­ç»ƒæ—¶é—´: çº¦20-25åˆ†é’Ÿ (vs åŸç‰ˆ149å°æ—¶)")
        print("   é¢„æœŸæ€§èƒ½æå‡: 1-3% AUROCæå‡")
        
        return ensemble_models
        
        # ä»¥ä¸‹æ˜¯è¢«è·³è¿‡çš„è€—æ—¶ä»£ç ï¼ˆç¡¬æŠ•ç¥¨å’Œå †å åˆ†ç±»å™¨ï¼‰
        try:
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åŸºç¡€æ¨¡å‹ç”¨äºé›†æˆ
            if len(base_models) < 3:
                print("âš ï¸ éœ€è¦è‡³å°‘3ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œé›†æˆ")
                return {}
            
            ensemble_models = {}
            
            # 1. ç¡¬æŠ•ç¥¨åˆ†ç±»å™¨
            try:
                # ğŸ”§ ä¿®å¤ï¼šåªä½¿ç”¨æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹
                proba_models = {}
                for name, model in base_models.items():
                    if hasattr(model, 'predict_proba'):
                        proba_models[name] = model
                
                if len(proba_models) >= 2:
                    estimators = [(name, model) for name, model in proba_models.items()]
                    voting_hard = VotingClassifier(estimators=estimators, voting='hard')
                    
                    # å¦‚æœæä¾›äº†è®­ç»ƒæ•°æ®ï¼Œåˆ™è®­ç»ƒæŠ•ç¥¨åˆ†ç±»å™¨
                    if X is not None and y is not None:
                        voting_hard.fit(X, y)
                        
                    ensemble_models['voting_hard'] = voting_hard
                    print("âœ… ç¡¬æŠ•ç¥¨åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
                else:
                    print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œè·³è¿‡ç¡¬æŠ•ç¥¨åˆ†ç±»å™¨")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºç¡¬æŠ•ç¥¨åˆ†ç±»å™¨å¤±è´¥: {str(e)}")
            
            # 2. è½¯æŠ•ç¥¨åˆ†ç±»å™¨
            try:
                # ğŸ”§ ä¿®å¤ï¼šåªä½¿ç”¨æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹
                proba_models = {}
                for name, model in base_models.items():
                    if hasattr(model, 'predict_proba'):
                        proba_models[name] = model
                
                if len(proba_models) >= 2:
                    estimators = [(name, model) for name, model in proba_models.items()]
                    voting_soft = VotingClassifier(estimators=estimators, voting='soft')
                    
                    # å¦‚æœæä¾›äº†è®­ç»ƒæ•°æ®ï¼Œåˆ™è®­ç»ƒæŠ•ç¥¨åˆ†ç±»å™¨
                    if X is not None and y is not None:
                        voting_soft.fit(X, y)
                        
                    ensemble_models['voting_soft'] = voting_soft
                    print("âœ… è½¯æŠ•ç¥¨åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
                else:
                    print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œè·³è¿‡è½¯æŠ•ç¥¨åˆ†ç±»å™¨")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºè½¯æŠ•ç¥¨åˆ†ç±»å™¨å¤±è´¥: {str(e)}")
            
            # 3. å †å åˆ†ç±»å™¨ (stacking)
            try:
                # ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºæœ€ç»ˆåˆ†ç±»å™¨
                from sklearn.linear_model import LogisticRegression
                
                # ğŸ”§ ä¿®å¤ï¼šåªä½¿ç”¨æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹
                proba_models = {}
                for name, model in base_models.items():
                    if hasattr(model, 'predict_proba'):
                        proba_models[name] = model
                
                if len(proba_models) >= 2:
                    estimators = [(name, model) for name, model in proba_models.items()]
                    stacking_clf = StackingClassifier(
                        estimators=estimators,
                        final_estimator=LogisticRegression(
                    random_state=self.random_state, 
                    max_iter=5000, 
                    solver='liblinear', 
                    tol=1e-3, 
                    n_jobs=N_JOBS
                ),
                        cv=CV_SETTINGS['inner_splits']
                    )
                    
                    # å¦‚æœæä¾›äº†è®­ç»ƒæ•°æ®ï¼Œåˆ™è®­ç»ƒå †å åˆ†ç±»å™¨
                    if X is not None and y is not None:
                        stacking_clf.fit(X, y)
                        
                    ensemble_models['stacking'] = stacking_clf
                    print("âœ… å †å åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
                else:
                    print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œè·³è¿‡å †å åˆ†ç±»å™¨")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºå †å åˆ†ç±»å™¨å¤±è´¥: {str(e)}")
            
            # 4. ğŸ†• åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨
            try:
                # é€‰æ‹©è¡¨ç°æœ€å¥½çš„å‡ ä¸ªæ¨¡å‹è¿›è¡ŒåŠ æƒæŠ•ç¥¨
                best_models = ['lgb', 'xgb', 'catboost', 'rf'] if 'catboost' in base_models else ['lgb', 'xgb', 'rf', 'gb']
                available_models = {k: v for k, v in base_models.items() if k in best_models and hasattr(v, 'predict_proba')}
                
                if len(available_models) >= 3:
                    estimators = [(name, model) for name, model in available_models.items()]
                    weighted_voting = VotingClassifier(
                        estimators=estimators, 
                        voting='soft',
                        weights=[1.2, 1.1, 1.0, 0.9][:len(estimators)]  # ç»™æ›´å¥½çš„æ¨¡å‹æ›´é«˜æƒé‡
                    )
                    
                    if X is not None and y is not None:
                        weighted_voting.fit(X, y)
                        
                    ensemble_models['weighted_voting'] = weighted_voting
                    print("âœ… åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
                else:
                    print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œè·³è¿‡åŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºåŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨å¤±è´¥: {str(e)}")
            
            print(f"ğŸ¯ æˆåŠŸåˆ›å»º {len(ensemble_models)} ä¸ªé›†æˆæ¨¡å‹")
            return ensemble_models
            
        except Exception as e:
            print(f"âŒ æ„å»ºé›†æˆæ¨¡å‹å¤±è´¥: {str(e)}")
            return {}
        
    def get_models(self):
        """è·å–å·²è®­ç»ƒçš„æ¨¡å‹"""
        return self.models
        
    def tune_hyperparameters(self, X_train, y_train, model_type='random_forest', search_method='random'):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print(f"\n{'-'*40}")
        print(f"è¶…å‚æ•°è°ƒä¼˜: {model_type}")
        print(f"{'-'*40}")
        
        # è·å–æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼
        model = self.build_base_models()[model_type]
        param_grid = MODEL_PARAMS.get(model_type, {})
        
        if not param_grid:
            print(f"âœ— æœªæ‰¾åˆ° {model_type} çš„å‚æ•°ç½‘æ ¼")
            return model
            
        # é€‰æ‹©æœç´¢æ–¹æ³•
        if search_method == 'grid':
            search = GridSearchCV(
                model,
                param_grid,
                cv=CV_SETTINGS['inner_splits'],
                scoring='roc_auc',
                n_jobs=N_JOBS,
                verbose=1
            )
        else:
            search = RandomizedSearchCV(
                model,
                param_grid,
                n_iter=20,
                cv=CV_SETTINGS['inner_splits'],
                scoring='roc_auc',
                random_state=self.random_state,
                n_jobs=N_JOBS,
                verbose=1
            )
            
        # æ‰§è¡Œæœç´¢
        try:
            search.fit(X_train, y_train)
            self.best_params[model_type] = search.best_params_
            
            print(f"\næœ€ä½³å‚æ•°:")
            for param, value in search.best_params_.items():
                print(f"  {param}: {value}")
            print(f"æœ€ä½³å¾—åˆ†: {search.best_score_:.4f}")
            
            return search.best_estimator_
            
        except Exception as e:
            print(f"âœ— è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {e}")
            return model
            
    def save_hyperparameter_results(self, search_results, model_type):
        """ä¿å­˜è¶…å‚æ•°è°ƒä¼˜ç»“æœ"""
        self.best_params[model_type] = {
            'best_params': search_results.best_params_,
            'best_score': search_results.best_score_,
            'cv_results': search_results.cv_results_
        } 
 