"""
Data preprocessing module for CESD Depression Prediction Model
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, RobustScaler
from imblearn.over_sampling import SMOTE
from ..config import CATEGORICAL_VARS, NUMERICAL_VARS, SMOTE_PARAMS, EXCLUDED_VARS

class DataProcessor:
    """æ•°æ®é¢„å¤„ç†ç±» - å®Œå…¨æŒ‰ç…§åŸå§‹ç‰ˆæœ¬é€»è¾‘"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.label_encoders = {}
        self.scaler = None
        self.imputation_values = {}
        
        # æ¨¡å‹ç‰¹å®šçš„ç¼–ç å™¨
        self.svm_label_encoders = {}
        self.svm_onehot_encoder = None
        self.svm_nominal_label_encoders = {}
        self.svm_scaler = None
        
        self.tree_label_encoders = {}
        
        self.ensemble_label_encoders = {}
        self.ensemble_scaler = None
        
        # ğŸ†• é¢„å®šä¹‰å…³é”®åˆ†ç±»å˜é‡çš„æ‰€æœ‰å¯èƒ½ç±»åˆ«
        self.adlfive_all_categories = [
            '0.Fully Independent',
            '1.Mild Dependence', 
            '2.Moderate Dependence',
            '3.Significant Dependence',
            '4.Severe Dependence',
            '5.Total Dependence'
        ]
        
        # å…¶ä»–åˆ†ç±»å˜é‡çš„é¢„å®šä¹‰ç±»åˆ«
        self.categorical_all_categories = {
            'child': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # CHARLSæœ‰10ï¼ŒKLOSAæ²¡æœ‰
            'ragender': ['1.man', '2.woman'],
            'work': ['0.Not working for pay', '1.Working for pay'],
            'diabe': ['0.No', '1.Yes'],
            'stroke': ['0.No', '1.Yes'],
            'livere': ['0.No', '1.Yes']
        }
        
    def load_data(self, file_path, dataset_name="Dataset"):
        """åŠ è½½æ•°æ®"""
        print(f"\n{'='*60}")
        print(f"åŠ è½½{dataset_name}æ•°æ®")
        print(f"{'='*60}")
        
        try:
            data = pd.read_csv(file_path)
            print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ: {data.shape}")
            
            # æ£€æŸ¥ç›®æ ‡å˜é‡
            if 'depressed' not in data.columns:
                print("âœ— æœªæ‰¾åˆ°ç›®æ ‡å˜é‡ 'depressed'")
                return None
                
            # å¤„ç†ç›®æ ‡å˜é‡
            if data['depressed'].dtype == 'object':
                data['depressed'] = data['depressed'].map({'0.No': 0, '1.Yes': 1})
                print("  âœ“ å·²å°†depressedä»å­—ç¬¦ä¸²æ ¼å¼(0.No/1.Yes)è½¬æ¢ä¸ºæ•°å€¼æ ¼å¼(0/1)")
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿depressedå˜é‡ä¸ºæ•´æ•°ç±»å‹ï¼Œå¤„ç†NaNå€¼
            if data['depressed'].dtype == 'float64':
                # åˆ é™¤depressedä¸ºNaNçš„æ ·æœ¬
                missing_count = data['depressed'].isnull().sum()
                if missing_count > 0:
                    print(f"  âš ï¸ å‘ç° {missing_count} ä¸ªdepressedç¼ºå¤±å€¼ï¼Œåˆ é™¤è¿™äº›æ ·æœ¬")
                    data = data.dropna(subset=['depressed'])
                    print(f"  åˆ é™¤åæ ·æœ¬æ•°: {len(data)}")
                
                # è½¬æ¢ä¸ºæ•´æ•°ç±»å‹
                data['depressed'] = data['depressed'].astype(int)
                print("  âœ“ å·²å°†depressedè½¬æ¢ä¸ºæ•´æ•°ç±»å‹")
            
            # ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼ˆä»…å¯¹è®­ç»ƒæ•°æ®ï¼‰
            if dataset_name == "CHARLS":
                self._check_data_completeness(data)
            
            # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            self._display_basic_stats(data)
            
            return data
            
        except Exception as e:
            print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _check_data_completeness(self, data):
        """æ£€æŸ¥è®­ç»ƒæ•°æ®çš„å®Œæ•´æ€§ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«"""
        print(f"\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        print("-" * 40)
        
        # å®šä¹‰å…³é”®åˆ†ç±»å˜é‡åŠå…¶é¢„æœŸç±»åˆ«
        expected_categories = {
            'adlfive': ['0.Fully Independent', '1.Mild Dependence', '2.Moderate Dependence', 
                       '3.Significant Dependence', '4.Severe Dependence', '5.Total Dependence'],  # ä¿®æ­£ï¼šä½¿ç”¨å®é™…å­—ç¬¦ä¸²æ ¼å¼
            'child': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # CHARLSæœ‰10ï¼ŒKLOSAæ²¡æœ‰
            'ragender': ['1.man', '2.woman'],
            'work': ['0.Not working for pay', '1.Working for pay'],
            'diabe': ['0.No', '1.Yes'],
            'stroke': ['0.No', '1.Yes'],
            'livere': ['0.No', '1.Yes']
        }
        
        missing_categories = {}
        
        for var, expected in expected_categories.items():
            if var in data.columns:
                actual_values = set(data[var].dropna().unique())
                expected_set = set(expected)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„ç±»åˆ«
                missing = expected_set - actual_values
                if missing:
                    missing_categories[var] = missing
                    print(f"  âš ï¸  {var}: ç¼ºå°‘ç±»åˆ« {sorted(missing)}")
                else:
                    print(f"  âœ…  {var}: ç±»åˆ«å®Œæ•´")
        
        if missing_categories:
            print(f"\nâš ï¸  è­¦å‘Šï¼šä»¥ä¸‹å˜é‡ç¼ºå°‘æŸäº›ç±»åˆ«ï¼Œå¯èƒ½å½±å“å¤–éƒ¨éªŒè¯:")
            for var, missing in missing_categories.items():
                print(f"    {var}: ç¼ºå°‘ {sorted(missing)}")
            print(f"\nğŸ’¡ å»ºè®®ï¼š")
            print(f"    1. æ£€æŸ¥æ•°æ®é‡‡æ ·æ˜¯å¦åˆç†")
            print(f"    2. è€ƒè™‘ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿ç±»åˆ«å®Œæ•´æ€§")
            print(f"    3. æˆ–è€…ä½¿ç”¨æ‰©å±•æ˜ å°„ç­–ç•¥å¤„ç†æ–°ç±»åˆ«")
        else:
            print(f"\nâœ… æ‰€æœ‰å…³é”®å˜é‡ç±»åˆ«å®Œæ•´ï¼Œé€‚åˆä¸¥æ ¼æ˜ å°„ç­–ç•¥")
            
    def preprocess_data_before_split(self, data):
        """æ•°æ®åˆ†å‰²å‰é¢„å¤„ç† - å®Œå…¨æŒ‰ç…§åŸå§‹ç‰ˆæœ¬"""
        print(f"\n{'='*60}")
        print("æ•°æ®é¢„å¤„ç† - æ–°çš„ç¼ºå¤±å€¼å¤„ç†é¡ºåº")
        print(f"{'='*60}")
        
        if data is None:
            raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
        processed_data = data.copy()
        
        # 1. åˆ é™¤IDå˜é‡
        vars_found = []
        for var in EXCLUDED_VARS:
            if var in processed_data.columns:
                processed_data = processed_data.drop(columns=[var])
                vars_found.append(var)
        
        if vars_found:
            print(f"âœ… å·²åˆ é™¤IDå˜é‡: {', '.join(vars_found)}")
        
        # 2. å¤„ç†ç›®æ ‡å˜é‡ç¼ºå¤±å€¼
        depressed_missing = processed_data['depressed'].isnull().sum()
        if depressed_missing > 0:
            print(f"âš ï¸ ç›®æ ‡å˜é‡æœ‰ {depressed_missing} ä¸ªç¼ºå¤±å€¼ï¼Œåˆ é™¤è¿™äº›æ ·æœ¬")
            valid_indices = ~processed_data['depressed'].isnull()
            processed_data = processed_data[valid_indices]
            print(f"åˆ é™¤åæ ·æœ¬æ•°: {len(processed_data)}")
        
        # âœ… ä¼˜åŒ–ï¼šåˆ é™¤å†—ä½™çš„depressionæ ‡ç­¾åˆ›å»º
        # processed_data['depression'] = processed_data['depressed'].astype(int)
        
        return processed_data
        
    def prepare_features_by_model_type(self, data, model_type='tree', is_training=True):
        """æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡ç‰¹å¾æ•°æ® - å®Œå…¨æŒ‰ç…§åŸå§‹ç‰ˆæœ¬"""
        print(f"\n{'-'*50}")
        print(f"æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡ç‰¹å¾æ•°æ®: {model_type.upper()}")
        print(f"{'-'*50}")
        
        try:
            # 1. åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡ (ä¼˜åŒ–ï¼šç®€åŒ–é€»è¾‘)
            target_col = 'depressed'
            
            if target_col not in data.columns:
                raise ValueError(f"ç›®æ ‡å˜é‡ '{target_col}' ä¸å­˜åœ¨")
            
            features = data.drop(columns=[target_col])
            target = data[target_col]
            
            # ç¡®ä¿ç›®æ ‡å˜é‡ä¸ºæ•°å€¼ç±»å‹ï¼ˆåœ¨éœ€è¦æ—¶è½¬æ¢ï¼‰
            if target.dtype == 'object':
                target = target.astype(int)
            
            print(f"åŸå§‹ç‰¹å¾æ•°é‡: {features.shape[1]}")
            print(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{target.value_counts()}")
            
            # 2. å¤„ç†ç¼ºå¤±å€¼
            features = self.impute_features(features, is_training)
            
            # 3. è¯†åˆ«ç‰¹å¾ç±»å‹
            print(f"ğŸ” å¼€å§‹ç‰¹å¾ç±»å‹è¯†åˆ«...")
            print(f"  é…ç½®ä¸­çš„åˆ†ç±»å˜é‡æ•°é‡: {len(CATEGORICAL_VARS)}")
            print(f"  é…ç½®ä¸­çš„æ•°å€¼å˜é‡æ•°é‡: {len(NUMERICAL_VARS)}")
            
            numeric_columns = []
            binary_columns = []
            ordinal_columns = []
            nominal_columns = []
            
            for column in features.columns:
                try:
                    # ğŸ” ç‰¹æ®Šè°ƒè¯•ï¼šæ£€æŸ¥hhreså˜é‡çš„å¤„ç†
                    if column == 'hhres':
                        if column in CATEGORICAL_VARS:
                            print(f"  âŒ é”™è¯¯: hhresè¢«è¯†åˆ«ä¸ºåˆ†ç±»å˜é‡!")
                        else:
                            print(f"  âœ… æ­£ç¡®: hhresè¢«è¯†åˆ«ä¸ºæ•°å€¼å˜é‡")
                    
                    if column in CATEGORICAL_VARS:
                        unique_vals = features[column].dropna().unique()
                        if len(unique_vals) == 2:
                            binary_columns.append(column)
                        elif self._is_ordinal_categorical(column, unique_vals):
                            ordinal_columns.append(column)
                        else:
                            nominal_columns.append(column)
                    else:
                        numeric_columns.append(column)
                except:
                    if column in CATEGORICAL_VARS:
                        nominal_columns.append(column)
                    else:
                        numeric_columns.append(column)
            
            print(f"ç‰¹å¾ç±»å‹ç»Ÿè®¡:")
            print(f"  æ•°å€¼å˜é‡: {len(numeric_columns)} ä¸ª")
            print(f"  äºŒåˆ†ç±»å˜é‡: {len(binary_columns)} ä¸ª") 
            print(f"  æœ‰åºåˆ†ç±»å˜é‡: {len(ordinal_columns)} ä¸ª")
            print(f"  æ— åºåˆ†ç±»å˜é‡: {len(nominal_columns)} ä¸ª")
            
            # 4. æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†ç‰¹å¾
            if model_type.lower() == 'svm':
                print(f"\nğŸ”§ SVMæ¨¡å‹ç‰¹å¾å¤„ç†:")
                print("  - è¿ç»­å‹å˜é‡: æ ‡å‡†åŒ–")
                print("  - äºŒåˆ†ç±»å˜é‡: æ ‡ç­¾ç¼–ç  (0/1)")
                print("  - æœ‰åºåˆ†ç±»å˜é‡: æ ‡ç­¾ç¼–ç  (ä¿æŒé¡ºåº)")
                print("  - æ— åºåˆ†ç±»å˜é‡: ç‹¬çƒ­ç¼–ç ")
                
                features_processed = self._process_features_for_svm(
                    features, numeric_columns, binary_columns, 
                    ordinal_columns, nominal_columns, is_training
                )
                
            elif model_type.lower() in ['tree', 'forest', 'xgboost', 'lightgbm', 'catboost']:
                print(f"\nğŸŒ³ æ ‘æ¨¡å‹ç‰¹å¾å¤„ç†:")
                print("  - è¿ç»­å‹å˜é‡: ä¿æŒåŸæ ·")
                print("  - æ‰€æœ‰åˆ†ç±»å˜é‡: æ ‡ç­¾ç¼–ç ")
                
                features_processed = self._process_features_for_tree(
                    features, numeric_columns, binary_columns, 
                    ordinal_columns, nominal_columns, is_training
                )
                
            elif model_type.lower() in ['linear', 'logistic', 'ridge', 'lasso']:
                print(f"\nğŸ“ˆ çº¿æ€§æ¨¡å‹ç‰¹å¾å¤„ç†:")
                print("  - è¿ç»­å‹å˜é‡: æ ‡å‡†åŒ–")
                print("  - äºŒåˆ†ç±»å˜é‡: æ ‡ç­¾ç¼–ç ")
                print("  - æœ‰åºåˆ†ç±»å˜é‡: æ ‡ç­¾ç¼–ç ")
                print("  - æ— åºåˆ†ç±»å˜é‡: ç‹¬çƒ­ç¼–ç ")
                
                features_processed = self._process_features_for_linear(
                    features, numeric_columns, binary_columns, 
                    ordinal_columns, nominal_columns, is_training
                )
                
            elif model_type.lower() == 'ensemble':
                print(f"\nğŸ¯ é›†æˆæ¨¡å‹ç‰¹å¾å¤„ç†:")
                print("  - ä½¿ç”¨æ··åˆç­–ç•¥ï¼Œå…¼é¡¾ä¸åŒåŸºç¡€æ¨¡å‹éœ€æ±‚")
                
                features_processed = self._process_features_for_ensemble(
                    features, numeric_columns, binary_columns, 
                    ordinal_columns, nominal_columns, is_training
                )
                
            else:
                print(f"\nâš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹ {model_type}ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†")
                features_processed = self._process_features_for_tree(
                    features, numeric_columns, binary_columns, 
                    ordinal_columns, nominal_columns, is_training
                )
            
            # 5. æœ€ç»ˆæ£€æŸ¥
            final_null_count = features_processed.isnull().sum().sum()
            if final_null_count > 0:
                print(f"ğŸš¨ è­¦å‘Š: {model_type}æ¨¡å‹ç‰¹å¾å¤„ç†å®Œæˆåä»æœ‰ {final_null_count} ä¸ªç¼ºå¤±å€¼!")
                # ç´§æ€¥ä¿®å¤
                for col in features_processed.columns:
                    if features_processed[col].isnull().any():
                        if pd.api.types.is_numeric_dtype(features_processed[col]):
                            features_processed[col].fillna(0, inplace=True)
                        else:
                            features_processed[col].fillna('Unknown', inplace=True)
            
            print(f"\nâœ… {model_type.upper()} æ¨¡å‹ç‰¹å¾å¤„ç†å®Œæˆ")
            print(f"   æœ€ç»ˆç‰¹å¾æ•°: {features_processed.shape[1]}")
            print(f"   æ ·æœ¬æ•°: {features_processed.shape[0]}")
            
            return features_processed, target
            
        except Exception as e:
            print(f"âŒ {model_type} ç‰¹å¾å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def _is_ordinal_categorical(self, column, unique_vals):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰åºåˆ†ç±»å˜é‡"""
        if all(isinstance(val, (int, float)) for val in unique_vals):
            sorted_vals = sorted(unique_vals)
            if len(sorted_vals) <= 10 and sorted_vals == list(range(int(min(sorted_vals)), int(max(sorted_vals)) + 1)):
                return True
        return False
    
    def _process_features_for_svm(self, features, numeric_cols, binary_cols, ordinal_cols, nominal_cols, is_training):
        """SVMæ¨¡å‹çš„ç‰¹å¾å¤„ç† - ä½¿ç”¨ç›´æ¥ç¼–ç ç­–ç•¥"""
        features_processed = features.copy()
        
        # SVMæ¨¡å‹ï¼šäºŒå…ƒå˜é‡ä¿æŒåŸæ ·ï¼Œåºæ•°å˜é‡ç›´æ¥ç¼–ç ï¼Œåä¹‰å˜é‡ç‹¬çƒ­ç¼–ç 
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€çš„ç¼–ç å™¨å¤„ç†äºŒå…ƒå’Œåºæ•°å˜é‡
        from ..utils.categorical_encoder import UnifiedCategoricalEncoder
        
        # æ‰¹é‡å¤„ç†äºŒå…ƒå˜é‡
        features_processed = UnifiedCategoricalEncoder.batch_encode_columns(
            features_processed, binary_cols, self.svm_label_encoders, is_training, 
            "äºŒå…ƒå˜é‡ç›´æ¥ç¼–ç "
        )
        
        # æ‰¹é‡å¤„ç†åºæ•°å˜é‡  
        features_processed = UnifiedCategoricalEncoder.batch_encode_columns(
            features_processed, ordinal_cols, self.svm_label_encoders, is_training,
            "åºæ•°å˜é‡ç›´æ¥ç¼–ç "
        )
        
        if nominal_cols:
            print(f"\nåä¹‰å˜é‡ç‹¬çƒ­ç¼–ç : {len(nominal_cols)} ä¸ª")
            # å…ˆå¯¹åä¹‰å˜é‡è¿›è¡Œç›´æ¥ç¼–ç 
            for col in nominal_cols:
                features_processed[col] = features_processed[col].fillna('missing').astype(str)
                
                if is_training:
                    if col not in self.svm_nominal_label_encoders:
                        self.svm_nominal_label_encoders[col] = LabelEncoder()
                    unique_values = sorted(features_processed[col].unique())
                    self.svm_nominal_label_encoders[col].fit(unique_values)
                    features_processed[col] = self.svm_nominal_label_encoders[col].transform(features_processed[col])
                else:
                    if col in self.svm_nominal_label_encoders:
                        encoder = self.svm_nominal_label_encoders[col]
                        known_categories = set(encoder.classes_)
                        current_categories = set(features_processed[col].unique())
                        new_categories = current_categories - known_categories
                        
                        if new_categories:
                            raise ValueError(f"âŒ ä¸¥æ ¼æ˜ å°„é”™è¯¯ï¼š{col} å‘ç°è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„æ–°ç±»åˆ«: {new_categories}")
                        else:
                            features_processed[col] = encoder.transform(features_processed[col])
                    else:
                        # ä¸´æ—¶ç¼–ç å™¨
                        temp_encoder = LabelEncoder()
                        unique_vals = sorted(features_processed[col].unique())
                        temp_encoder.fit(unique_vals)
                        features_processed[col] = temp_encoder.transform(features_processed[col])
            
            # ğŸ†• æ”¹è¿›çš„ç‹¬çƒ­ç¼–ç ï¼šä½¿ç”¨é¢„å®šä¹‰ç±»åˆ«
            # æ³¨æ„ï¼šOneHotEncoderéœ€è¦åŸå§‹å­—ç¬¦ä¸²æ•°æ®ï¼Œä¸æ˜¯LabelEncoderåçš„æ•°å­—
            nominal_encoded_original = features[nominal_cols].fillna('missing')
            
            if is_training:
                # è®­ç»ƒæ—¶ï¼šåˆ›å»ºä½¿ç”¨é¢„å®šä¹‰ç±»åˆ«çš„ç¼–ç å™¨
                categories_list = []
                for col in nominal_cols:
                    if col == 'adlfive':
                        # ç‰¹æ®Šå¤„ç†adlfiveï¼Œä½¿ç”¨é¢„å®šä¹‰ç±»åˆ«
                        categories_list.append(self.adlfive_all_categories)
                    elif col in self.categorical_all_categories:
                        # å…¶ä»–é¢„å®šä¹‰åˆ†ç±»å˜é‡
                        categories_list.append(self.categorical_all_categories[col])
                    else:
                        # å¯¹äºæœªé¢„å®šä¹‰çš„å˜é‡ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„å®é™…ç±»åˆ«
                        categories_list.append(sorted(nominal_encoded_original[col].unique()))
                
                self.svm_onehot_encoder = OneHotEncoder(
                    categories=categories_list,
                    sparse_output=False, 
                    handle_unknown='ignore'
                )
                nominal_onehot = self.svm_onehot_encoder.fit_transform(nominal_encoded_original)
                
                # ä¿å­˜è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
                self.svm_onehot_feature_names = []
                for i, col in enumerate(nominal_cols):
                    categories = self.svm_onehot_encoder.categories_[i]
                    for cat in categories:
                        self.svm_onehot_feature_names.append(f"{col}_{cat}")
                
                print(f"âœ… ç‹¬çƒ­ç¼–ç å®Œæˆï¼Œç”Ÿæˆ {len(self.svm_onehot_feature_names)} ä¸ªç‰¹å¾")
                print(f"âœ… adlfiveç‰¹å¾: {[f for f in self.svm_onehot_feature_names if 'adlfive_' in f]}")
                
            else:
                # é¢„æµ‹æ—¶ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„ç¼–ç å™¨
                if self.svm_onehot_encoder is not None:
                    nominal_onehot = self.svm_onehot_encoder.transform(nominal_encoded_original)
                else:
                    # å¦‚æœæ²¡æœ‰è®­ç»ƒå¥½çš„ç‹¬çƒ­ç¼–ç å™¨ï¼Œè·³è¿‡ç‹¬çƒ­ç¼–ç 
                    nominal_onehot = nominal_encoded_original.values
            
            # åˆ›å»ºç‹¬çƒ­ç¼–ç çš„åˆ—å
            if is_training and self.svm_onehot_encoder is not None:
                feature_names = self.svm_onehot_feature_names
            elif hasattr(self, 'svm_onehot_feature_names') and self.svm_onehot_feature_names:
                # ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„ç‰¹å¾åç§°
                feature_names = self.svm_onehot_feature_names
            else:
                feature_names = [f"onehot_{i}" for i in range(nominal_onehot.shape[1])]
            
            # åˆ é™¤åŸå§‹åä¹‰å˜é‡ï¼Œæ·»åŠ ç‹¬çƒ­ç¼–ç å˜é‡
            features_processed = features_processed.drop(columns=nominal_cols)
            nominal_df = pd.DataFrame(nominal_onehot, columns=feature_names, index=features_processed.index)
            features_processed = pd.concat([features_processed, nominal_df], axis=1)
        
        # æ•°å€¼å˜é‡æ ‡å‡†åŒ–
        if numeric_cols:
            print(f"\næ•°å€¼å˜é‡æ ‡å‡†åŒ–: {len(numeric_cols)} ä¸ª")
            if is_training:
                self.svm_scaler = StandardScaler()
                features_processed[numeric_cols] = self.svm_scaler.fit_transform(features_processed[numeric_cols])
            else:
                if self.svm_scaler is not None:
                    features_processed[numeric_cols] = self.svm_scaler.transform(features_processed[numeric_cols])
        
        return features_processed
    
    def _process_features_for_tree(self, features, numeric_cols, binary_cols, ordinal_cols, nominal_cols, is_training):
        """æ ‘æ¨¡å‹çš„ç‰¹å¾å¤„ç† - ä½¿ç”¨ç›´æ¥ç¼–ç ç­–ç•¥"""
        features_processed = features.copy()
        
        # æ ‘æ¨¡å‹ï¼šæ‰€æœ‰åˆ†ç±»å˜é‡éƒ½ç”¨ç›´æ¥ç¼–ç 
        categorical_cols = binary_cols + ordinal_cols + nominal_cols
        if categorical_cols:
            print(f"\nç›´æ¥ç¼–ç å¤„ç† {len(categorical_cols)} ä¸ªåˆ†ç±»å˜é‡:")
            for col in categorical_cols:
                # å…ˆå¡«å……ç¼ºå¤±å€¼
                features_processed[col] = features_processed[col].fillna('missing').astype(str)
                
                if is_training:
                    # è®­ç»ƒé˜¶æ®µï¼šåˆ›å»ºç¼–ç å™¨å¹¶ä¿å­˜
                    if col not in self.tree_label_encoders:
                        self.tree_label_encoders[col] = LabelEncoder()
                    
                    # è·å–æ‰€æœ‰å”¯ä¸€å€¼å¹¶æ’åºï¼Œç¡®ä¿ç¼–ç ä¸€è‡´æ€§
                    unique_values = sorted(features_processed[col].unique())
                    self.tree_label_encoders[col].fit(unique_values)
                    
                    # ç¼–ç æ•°æ®
                    features_processed[col] = self.tree_label_encoders[col].transform(features_processed[col])
                    
                    print(f"  ğŸ“Š è®­ç»ƒç¼–ç å™¨: {col}")
                    print(f"    ç±»åˆ«æ•°é‡: {len(unique_values)}")
                    print(f"    ç¼–ç æ˜ å°„: {dict(zip(unique_values, range(len(unique_values))))}")
                    
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ç¼–ç å™¨åŒæ—¶ä¿å­˜åˆ°ä¸»ç¼–ç å™¨å­—å…¸
                    if col not in self.label_encoders:
                        self.label_encoders[col] = self.tree_label_encoders[col]
                    
                else:
                    # æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨è®­ç»ƒå¥½çš„ç¼–ç å™¨
                    if col in self.tree_label_encoders:
                        encoder = self.tree_label_encoders[col]
                        known_categories = set(encoder.classes_)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç±»åˆ«
                        current_categories = set(features_processed[col].unique())
                        new_categories = current_categories - known_categories
                        
                        if new_categories:
                            raise ValueError(f"âŒ ä¸¥æ ¼æ˜ å°„é”™è¯¯ï¼š{col} å‘ç°è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„æ–°ç±»åˆ«: {new_categories}")
                        else:
                            # æ²¡æœ‰æ–°ç±»åˆ«ï¼Œä½¿ç”¨åŸå§‹ç¼–ç å™¨
                            features_processed[col] = encoder.transform(features_processed[col])
                            print(f"  âœ… ä½¿ç”¨åŸå§‹ç¼–ç å™¨: {col}")
                            
                    else:
                        # ğŸ”§ ä¿®å¤ï¼šå°è¯•ä»ä¸»ç¼–ç å™¨å­—å…¸è·å–ç¼–ç å™¨
                        if col in self.label_encoders:
                            encoder = self.label_encoders[col]
                            print(f"  âœ… ä»ä¸»ç¼–ç å™¨è·å–: {col}")
                            features_processed[col] = encoder.transform(features_processed[col])
                        else:
                            print(f"  ! ä¸¥é‡è­¦å‘Š: {col} ç¼ºå°‘è®­ç»ƒå¥½çš„æ ‡ç­¾ç¼–ç å™¨")
                            print(f"    è¿™ä¼šå¯¼è‡´ç¼–ç ä¸ä¸€è‡´ï¼å»ºè®®æ£€æŸ¥ç¼–ç å™¨ä¿å­˜/åŠ è½½æµç¨‹")
                            
                            # ğŸš¨ ç´§æ€¥å¤„ç†ï¼šå°è¯•ä½¿ç”¨æ•°å€¼ç¼–ç 
                            try:
                                # å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼ˆå¯èƒ½æ•°æ®å·²ç»æ˜¯æ•°å€¼ç¼–ç ï¼‰
                                features_processed[col] = pd.to_numeric(features_processed[col], errors='coerce')
                                # å¡«å……è½¬æ¢å¤±è´¥çš„å€¼
                                if features_processed[col].isna().any():
                                    features_processed[col] = features_processed[col].fillna(0)
                                print(f"    å·²è½¬æ¢ä¸ºæ•°å€¼ç¼–ç ")
                            except:
                                # æœ€åæ‰‹æ®µï¼šä¸´æ—¶æ ‡ç­¾ç¼–ç ï¼ˆä¼šæœ‰ç¼–ç ä¸ä¸€è‡´é—®é¢˜ï¼‰
                                temp_encoder = LabelEncoder()
                                unique_vals = features_processed[col].astype(str).unique()
                                temp_encoder.fit(unique_vals)
                                features_processed[col] = temp_encoder.transform(features_processed[col].astype(str))
                                print(f"    ä½¿ç”¨ä¸´æ—¶ç¼–ç å™¨ï¼ˆå¯èƒ½ä¸ä¸€è‡´ï¼‰")
        
        # æ•°å€¼å˜é‡ä¿æŒåŸæ ·ï¼ˆæ ‘æ¨¡å‹ä¸éœ€è¦æ ‡å‡†åŒ–ï¼‰
        if numeric_cols:
            print(f"\næ•°å€¼å˜é‡ä¿æŒåŸæ ·: {len(numeric_cols)} ä¸ª")
        
        return features_processed
    
    def _process_features_for_linear(self, features, numeric_cols, binary_cols, ordinal_cols, nominal_cols, is_training):
        """çº¿æ€§æ¨¡å‹çš„ç‰¹å¾å¤„ç†ï¼ˆç±»ä¼¼SVMï¼‰"""
        return self._process_features_for_svm(features, numeric_cols, binary_cols, ordinal_cols, nominal_cols, is_training)
    
    def _process_features_for_ensemble(self, features, numeric_cols, binary_cols, ordinal_cols, nominal_cols, is_training):
        """é›†æˆæ¨¡å‹çš„ç‰¹å¾å¤„ç† - ä½¿ç”¨ç›´æ¥ç¼–ç ç­–ç•¥"""
        features_processed = features.copy()
        
        # é›†æˆæ¨¡å‹ä½¿ç”¨ç›´æ¥ç¼–ç ï¼ˆé€‚åˆæ ‘æ¨¡å‹ï¼‰+ éƒ¨åˆ†æ ‡å‡†åŒ–ï¼ˆé€‚åˆçº¿æ€§æ¨¡å‹ï¼‰
        categorical_cols = binary_cols + ordinal_cols + nominal_cols
        if categorical_cols:
            print(f"\nç›´æ¥ç¼–ç å¤„ç† {len(categorical_cols)} ä¸ªåˆ†ç±»å˜é‡:")
            for col in categorical_cols:
                # å…ˆå¡«å……ç¼ºå¤±å€¼
                features_processed[col] = features_processed[col].fillna('missing').astype(str)
                
                if is_training:
                    # è®­ç»ƒé˜¶æ®µï¼šåˆ›å»ºç¼–ç å™¨å¹¶ä¿å­˜
                    if col not in self.ensemble_label_encoders:
                        self.ensemble_label_encoders[col] = LabelEncoder()
                    
                    # è·å–æ‰€æœ‰å”¯ä¸€å€¼å¹¶æ’åºï¼Œç¡®ä¿ç¼–ç ä¸€è‡´æ€§
                    unique_values = sorted(features_processed[col].unique())
                    self.ensemble_label_encoders[col].fit(unique_values)
                    
                    # ç¼–ç æ•°æ®
                    features_processed[col] = self.ensemble_label_encoders[col].transform(features_processed[col])
                    
                    print(f"  ğŸ“Š è®­ç»ƒç¼–ç å™¨: {col}")
                    print(f"    ç±»åˆ«æ•°é‡: {len(unique_values)}")
                    print(f"    ç¼–ç æ˜ å°„: {dict(zip(unique_values, range(len(unique_values))))}")
                    
                else:
                    # æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨è®­ç»ƒå¥½çš„ç¼–ç å™¨
                    if col in self.ensemble_label_encoders:
                        encoder = self.ensemble_label_encoders[col]
                        known_categories = set(encoder.classes_)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç±»åˆ«
                        current_categories = set(features_processed[col].unique())
                        new_categories = current_categories - known_categories
                        
                        if new_categories:
                            raise ValueError(f"âŒ ä¸¥æ ¼æ˜ å°„é”™è¯¯ï¼š{col} å‘ç°è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„æ–°ç±»åˆ«: {new_categories}")
                        else:
                            # æ²¡æœ‰æ–°ç±»åˆ«ï¼Œä½¿ç”¨åŸå§‹ç¼–ç å™¨
                            features_processed[col] = encoder.transform(features_processed[col])
                            print(f"  âœ… ä½¿ç”¨åŸå§‹ç¼–ç å™¨: {col}")
                            
                    else:
                        raise ValueError(f"âŒ é”™è¯¯ï¼š{col} ç¼ºå°‘è®­ç»ƒå¥½çš„ç¼–ç å™¨ï¼Œæ— æ³•è¿›è¡Œä¸¥æ ¼æ˜ å°„")
        
        # æ•°å€¼å˜é‡è½»åº¦æ ‡å‡†åŒ–ï¼ˆRobustScalerï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
        if numeric_cols:
            print(f"\nè½»åº¦æ ‡å‡†åŒ– {len(numeric_cols)} ä¸ªæ•°å€¼å˜é‡:")
            if is_training:
                self.ensemble_scaler = RobustScaler()
                features_processed[numeric_cols] = self.ensemble_scaler.fit_transform(features_processed[numeric_cols])
                print(f"  - å·²ä½¿ç”¨RobustScaleræ ‡å‡†åŒ–")
            else:
                if hasattr(self, 'ensemble_scaler') and self.ensemble_scaler is not None:
                    features_processed[numeric_cols] = self.ensemble_scaler.transform(features_processed[numeric_cols])
        
        return features_processed
    
    def impute_features(self, features, is_training):
        """ä¿®å¤ç‰ˆç¼ºå¤±å€¼å¡«å…… - é¿å…æ•°æ®æ³„éœ²å’ŒPandasè­¦å‘Š"""
        print("\nå¤„ç†ç¼ºå¤±å€¼...")
        
        # ğŸ”§ ä¿®å¤ï¼šåˆ›å»ºå‰¯æœ¬é¿å…SettingWithCopyWarning
        features = features.copy()
        
        null_counts = features.isnull().sum()
        columns_with_nulls = null_counts[null_counts > 0]
        
        if not columns_with_nulls.empty:
            print("å‘ç°ç¼ºå¤±å€¼:")
            for col, count in columns_with_nulls.items():
                print(f"  - {col}: {count} ä¸ªç¼ºå¤±å€¼ ({count/len(features)*100:.2f}%)")
                
                # ğŸ”§ ç‰¹æ®Šå¤„ç†ï¼šcomparable_hexpç›´æ¥ä½¿ç”¨ä¸­ä½æ•°
                if col == 'comparable_hexp':
                    if is_training:
                        fill_value = features[col].median()
                        if pd.isna(fill_value):
                            fill_value = 0.0
                        self.imputation_values[col] = fill_value
                    else:
                        fill_value = self.imputation_values.get(col, 0.0)
                    
                    features[col] = features[col].fillna(fill_value)
                    print(f"    å·²å¡«å……: {fill_value}")
                    continue
                
                if col in NUMERICAL_VARS:
                    if is_training:
                        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿è®¡ç®—å¡«å……å€¼æ—¶æ•°æ®æœ‰æ•ˆ
                        valid_values = features[col].dropna()
                        if len(valid_values) > 0:
                            fill_value = valid_values.median()
                        else:
                            fill_value = 0.0  # é»˜è®¤å€¼
                        self.imputation_values[col] = fill_value
                    else:
                        # ğŸ”§ ä¿®å¤ï¼šé¿å…æ•°æ®æ³„éœ² - ä»…ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å¡«å……å€¼
                        fill_value = self.imputation_values.get(col, 0.0)
                    
                    # ğŸ”§ ä¿®å¤ï¼šé¿å…inplaceè­¦å‘Š
                    features[col] = features[col].fillna(fill_value)
                else:
                    if is_training:
                        # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨çš„ä¼—æ•°è®¡ç®—
                        valid_values = features[col].dropna()
                        if len(valid_values) > 0:
                            mode_values = valid_values.mode()
                            fill_value = mode_values[0] if len(mode_values) > 0 else '0.Default'
                        else:
                            fill_value = '0.Default'
                        self.imputation_values[col] = fill_value
                    else:
                        # ğŸ”§ ä¿®å¤ï¼šé¿å…æ•°æ®æ³„éœ² - ä»…ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å¡«å……å€¼
                        fill_value = self.imputation_values.get(col, '0.Default')
                    
                    # ğŸ”§ ä¿®å¤ï¼šé¿å…inplaceè­¦å‘Š
                    features[col] = features[col].fillna(fill_value)
                    
                print(f"    å·²å¡«å……: {fill_value}")
        else:
            print("âœ“ æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
            
        return features
    
    def apply_smote(self, X, y):
        """åº”ç”¨SMOTEè¿›è¡Œæ•°æ®å¹³è¡¡"""
        print("\nåº”ç”¨SMOTEè¿›è¡Œæ•°æ®å¹³è¡¡...")
        
        try:
            smote = SMOTE(**SMOTE_PARAMS)
            X_resampled, y_resampled = smote.fit_resample(X, y)
            
            # æ˜¾ç¤ºå¹³è¡¡åçš„åˆ†å¸ƒ
            unique_new, counts_new = np.unique(y_resampled, return_counts=True)
            print("\nSMOTEå¤„ç†åæ•°æ®åˆ†å¸ƒ:")
            for label, count in zip(unique_new, counts_new):
                print(f"  ç±»åˆ« {label}: {count} æ ·æœ¬ ({count/len(y_resampled)*100:.1f}%)")
                
            return X_resampled, y_resampled
            
        except Exception as e:
            print(f"âœ— SMOTEå¤„ç†å¤±è´¥: {str(e)}")
            return X, y
    
    def _display_basic_stats(self, data):
        """æ˜¾ç¤ºæ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\næ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        print(f"æ ·æœ¬æ•°é‡: {len(data)}")
        print(f"ç‰¹å¾æ•°é‡: {data.shape[1]}")
        
        if 'depressed' in data.columns:
            print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
            print(data['depressed'].value_counts(normalize=True))
        
        # æ˜¾ç¤ºæ•°å€¼å˜é‡ç»Ÿè®¡
        numerical_columns = [col for col in NUMERICAL_VARS if col in data.columns]
        if numerical_columns:
            print(f"\næ•°å€¼å˜é‡ç»Ÿè®¡:")
            print(data[numerical_columns].describe())

    # ä¿æŒå‘åå…¼å®¹çš„æ—§æ–¹æ³•
    def preprocess_data(self, data, is_training=True):
        """é¢„å¤„ç†æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        if data is None:
            return None
            
        # æ£€æŸ¥å’Œåˆ›å»ºç¼ºå¤±å˜é‡
        if 'coresd' not in data.columns:
            data['coresd'] = '0.Default'
        if 'ftrhlp' not in data.columns:
            data['ftrhlp'] = '0.Default'
            
        # æ’é™¤IDå˜é‡
        for var in EXCLUDED_VARS:
            if var in data.columns:
                data = data.drop(columns=[var])
        
        # å¤„ç†ç¼ºå¤±å€¼
        data = self.impute_features(data, is_training)
        
        # ç¼–ç åˆ†ç±»å˜é‡
        for col in CATEGORICAL_VARS:
            if col in data.columns:
                if is_training:
                    self.label_encoders[col] = LabelEncoder()
                    data[col] = self.label_encoders[col].fit_transform(data[col].astype(str))
                else:
                    if col in self.label_encoders:
                        data[col] = self._direct_encode_with_consistency(data[col], col, is_training)
        
        # æ ‡å‡†åŒ–æ•°å€¼å˜é‡
        numerical_columns = [col for col in NUMERICAL_VARS if col in data.columns]
        if numerical_columns:
            if is_training:
                self.scaler = StandardScaler()
                data[numerical_columns] = self.scaler.fit_transform(data[numerical_columns])
            else:
                if self.scaler is not None:
                    data[numerical_columns] = self.scaler.transform(data[numerical_columns])
                    
        return data 

    def _direct_encode_with_consistency(self, data, column_name, is_training=True):
        """å¯¹åˆ†ç±»å˜é‡è¿›è¡Œä¸¥æ ¼ç¼–ç  - åªå…è®¸è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„ç±»åˆ«"""
        if is_training:
            # è®­ç»ƒé˜¶æ®µï¼šåˆ›å»ºç¼–ç å™¨
            print(f"    ğŸ“Š è®­ç»ƒç¼–ç å™¨: {column_name}")
            
            # å¤„ç†ç¼ºå¤±å€¼
            data_clean = data.fillna('missing')
            
            # ğŸ”§ ä¿®å¤ï¼šå¯¹äºadlfiveå˜é‡ï¼Œä½¿ç”¨ç‰¹æ®Šçš„æ•°æ®ç±»å‹ç»Ÿä¸€å¤„ç†
            if column_name == 'adlfive':
                # adlfiveå®é™…æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦æ•°å€¼è½¬æ¢
                data_str = data_clean.astype(str)
            else:
                data_str = data_clean.astype(str)
            
            # åˆ›å»ºç¼–ç å™¨
            unique_values = sorted(data_str.unique())
            encoder = LabelEncoder()
            encoder.fit(unique_values)
            
            # ç¼–ç æ•°æ®
            encoded_data = encoder.transform(data_str)
            
            # ä¿å­˜ç¼–ç å™¨
            self.label_encoders[column_name] = encoder
            
            # æ˜¾ç¤ºç¼–ç æ˜ å°„
            mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))
            print(f"      ç±»åˆ«æ•°é‡: {len(unique_values)}")
            print(f"      ç¼–ç æ˜ å°„: {mapping}")
            
            return pd.Series(encoded_data, index=data.index, name=column_name)
            
        else:
            # æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨è®­ç»ƒå¥½çš„ç¼–ç å™¨ï¼Œä¸¥æ ¼æ˜ å°„
            print(f"    âœ… ä½¿ç”¨ä¸¥æ ¼ç¼–ç å™¨: {column_name}")
            
            if column_name not in self.label_encoders:
                raise ValueError(f"âŒ é”™è¯¯ï¼š{column_name} ç¼ºå°‘è®­ç»ƒå¥½çš„ç¼–ç å™¨ï¼Œæ— æ³•è¿›è¡Œä¸¥æ ¼æ˜ å°„")
            
            encoder = self.label_encoders[column_name]
            
            # å¤„ç†ç¼ºå¤±å€¼
            data_clean = data.fillna('missing')
            
            # ğŸ”§ ä¿®å¤ï¼šå¯¹äºadlfiveå˜é‡ï¼Œä½¿ç”¨ç‰¹æ®Šçš„æ•°æ®ç±»å‹ç»Ÿä¸€å¤„ç†
            if column_name == 'adlfive':
                # adlfiveå®é™…æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦æ•°å€¼è½¬æ¢
                data_str = data_clean.astype(str)
            else:
                data_str = data_clean.astype(str)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç±»åˆ«
            known_categories = set(encoder.classes_)
            current_categories = set(data_str.unique())
            new_categories = current_categories - known_categories
            
            if new_categories:
                raise ValueError(f"âŒ ä¸¥æ ¼æ˜ å°„é”™è¯¯ï¼š{column_name} å‘ç°è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„æ–°ç±»åˆ«: {new_categories}")
            
            # ä¸¥æ ¼æ˜ å°„ï¼šåªå…è®¸å·²çŸ¥ç±»åˆ«
            try:
                encoded_data = encoder.transform(data_str)
                print(f"      âœ… ä¸¥æ ¼æ˜ å°„æˆåŠŸï¼Œæ— æ–°ç±»åˆ«")
                return pd.Series(encoded_data, index=data.index, name=column_name)
            except ValueError as e:
                raise ValueError(f"âŒ ä¸¥æ ¼æ˜ å°„å¤±è´¥ï¼š{column_name} ç¼–ç é”™è¯¯ - {e}")
    
    def _process_features_direct_encoding(self, features, is_training=True):
        """
        ä½¿ç”¨ç›´æ¥ç¼–ç å¤„ç†æ‰€æœ‰åˆ†ç±»å˜é‡ï¼Œç¡®ä¿ä¸€è‡´æ€§
        """
        from ..config import CATEGORICAL_VARS
        
        features_processed = features.copy()
        
        # è¯†åˆ«åˆ†ç±»å˜é‡
        categorical_cols = [col for col in features.columns if col in CATEGORICAL_VARS]
        
        if categorical_cols:
            print(f"\nğŸ”¤ ç›´æ¥ç¼–ç å¤„ç† {len(categorical_cols)} ä¸ªåˆ†ç±»å˜é‡:")
            
            for col in categorical_cols:
                features_processed[col] = self._direct_encode_with_consistency(
                    features_processed[col], col, is_training
                )
        
        return features_processed 
    
    def validate_preprocessing_consistency(self, train_data, test_data):
        """éªŒè¯è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é¢„å¤„ç†çš„ä¸€è‡´æ€§"""
        print("ğŸ” éªŒè¯æ•°æ®é¢„å¤„ç†ä¸€è‡´æ€§...")
        
        # æ£€æŸ¥åˆ†ç±»å˜é‡
        categorical_vars = ['adlfive', 'ragender', 'work', 'diabe', 'stroke', 'livere', 'child']
        
        for col in categorical_vars:
            if col in train_data.columns and col in test_data.columns:
                train_cats = set(train_data[col].dropna().unique())
                test_cats = set(test_data[col].dropna().unique())
                
                missing_in_test = train_cats - test_cats
                new_in_test = test_cats - train_cats
                
                if missing_in_test:
                    print(f"  âš ï¸ {col}: æµ‹è¯•æ•°æ®ç¼ºå°‘ç±»åˆ« {missing_in_test}")
                if new_in_test:
                    print(f"  âš ï¸ {col}: æµ‹è¯•æ•°æ®æœ‰æ–°ç±»åˆ« {new_in_test}")
                
                if not missing_in_test and not new_in_test:
                    print(f"  âœ… {col}: ç±»åˆ«å®Œå…¨ä¸€è‡´")
        
        print("âœ… æ•°æ®é¢„å¤„ç†ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")