"""
Feature selection module for CESD Depression Prediction Model
"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import (
    SelectKBest, f_classif, chi2, mutual_info_classif,
    RFE, RFECV, SelectFromModel, VarianceThreshold
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score
import warnings
warnings.filterwarnings('ignore')

class FeatureSelector:
    """ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.selected_features = {}
        self.feature_scores = {}
        self.selection_methods = {}
        
    def fit(self, X, y, methods=['variance', 'univariate', 'rfe', 'model_based'], 
            k_best=None, variance_threshold=0.01):
        """
        æ‹Ÿåˆç‰¹å¾é€‰æ‹©å™¨
        
        Parameters:
        -----------
        X : DataFrame
            ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        methods : list
            ç‰¹å¾é€‰æ‹©æ–¹æ³•åˆ—è¡¨
        k_best : int, å¯é€‰
            é€‰æ‹©çš„ç‰¹å¾æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œé»˜è®¤ä½¿ç”¨ç‰¹å¾æ€»æ•°çš„30%
        variance_threshold : float
            æ–¹å·®é˜ˆå€¼
        """
        print(f"\n{'='*60}")
        print("ğŸ” å¼€å§‹ç‰¹å¾é€‰æ‹©")
        print(f"{'='*60}")
        
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"é€‰æ‹©æ–¹æ³•: {', '.join(methods)}")
        
        # å¦‚æœk_bestæœªæŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨ç‰¹å¾æ€»æ•°çš„60%
        if k_best is None:
            k_best = max(5, int(X.shape[1] * 0.6))
            print(f"æœªæŒ‡å®šç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼: {k_best} (ç‰¹å¾æ€»æ•°çš„60%)")
        
        self.all_features = X.columns.tolist()
        
        # 1. æ–¹å·®è¿‡æ»¤
        if 'variance' in methods:
            self._variance_filter(X, y, variance_threshold)
        
        # 2. å•å˜é‡ç»Ÿè®¡æ£€éªŒ
        if 'univariate' in methods or 'statistical' in methods:
            self._univariate_selection(X, y, k_best)
        
        # 3. é€’å½’ç‰¹å¾æ¶ˆé™¤
        if 'rfe' in methods:
            self._recursive_feature_elimination(X, y, k_best)
        
        # 4. åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
        if 'model_based' in methods or 'model' in methods:
            self._model_based_selection(X, y, k_best)
        
        # 5. ç»¼åˆé€‰æ‹©
        self._ensemble_selection(k_best)
        
        print(f"\nâœ… ç‰¹å¾é€‰æ‹©å®Œæˆ")
        return self
        
    def transform(self, X):
        """åº”ç”¨ç‰¹å¾é€‰æ‹©"""
        if not hasattr(self, 'final_features'):
            raise ValueError("è¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
            
        transformed_data = X[self.final_features]
        return {
            'transformed_data': transformed_data,
            'selected_features': self.final_features,
            'feature_scores': getattr(self, 'feature_scores', {})
        }
    
    def fit_transform(self, X, y, **kwargs):
        """æ‹Ÿåˆå¹¶è½¬æ¢"""
        return self.fit(X, y, **kwargs).transform(X)
    
    def _variance_filter(self, X, y, threshold):
        """æ–¹å·®è¿‡æ»¤"""
        print(f"\nğŸ“Š æ–¹å·®è¿‡æ»¤ (é˜ˆå€¼: {threshold})")
        
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(X)
        
        selected_features = X.columns[selector.get_support()].tolist()
        removed_count = len(X.columns) - len(selected_features)
        
        self.selected_features['variance'] = selected_features
        self.feature_scores['variance'] = selector.variances_
        
        print(f"  ç§»é™¤ä½æ–¹å·®ç‰¹å¾: {removed_count} ä¸ª")
        print(f"  ä¿ç•™ç‰¹å¾: {len(selected_features)} ä¸ª")
        
    def _univariate_selection(self, X, y, k):
        """å•å˜é‡ç»Ÿè®¡æ£€éªŒ"""
        print(f"\nğŸ“ˆ å•å˜é‡ç»Ÿè®¡æ£€éªŒ (é€‰æ‹©å‰ {k} ä¸ª)")
        
        # åˆ†åˆ«å¤„ç†æ•°å€¼å‹å’Œåˆ†ç±»å‹ç‰¹å¾
        numeric_features = X.select_dtypes(include=[np.number]).columns
        categorical_features = X.select_dtypes(exclude=[np.number]).columns
        
        selected_features = []
        scores_dict = {}
        
        # æ•°å€¼å‹ç‰¹å¾ä½¿ç”¨Fæ£€éªŒ
        if len(numeric_features) > 0:
            selector_f = SelectKBest(score_func=f_classif, k='all')
            selector_f.fit(X[numeric_features], y)
            
            f_scores = selector_f.scores_
            f_features = numeric_features[np.argsort(f_scores)[-min(k//2, len(numeric_features)):]]
            selected_features.extend(f_features.tolist())
            
            for i, feature in enumerate(numeric_features):
                scores_dict[feature] = f_scores[i]
            
            print(f"  Fæ£€éªŒé€‰æ‹©æ•°å€¼å‹ç‰¹å¾: {len(f_features)} ä¸ª")
        
        # åˆ†ç±»å‹ç‰¹å¾ä½¿ç”¨äº’ä¿¡æ¯
        if len(categorical_features) > 0:
            selector_mi = SelectKBest(score_func=mutual_info_classif, k='all')
            selector_mi.fit(X[categorical_features], y)
            
            mi_scores = selector_mi.scores_
            mi_features = categorical_features[np.argsort(mi_scores)[-min(k//2, len(categorical_features)):]]
            selected_features.extend(mi_features.tolist())
            
            for i, feature in enumerate(categorical_features):
                scores_dict[feature] = mi_scores[i]
            
            print(f"  äº’ä¿¡æ¯é€‰æ‹©åˆ†ç±»å‹ç‰¹å¾: {len(mi_features)} ä¸ª")
        
        # å¦‚æœæ€»æ•°è¶…è¿‡kï¼ŒæŒ‰åˆ†æ•°æ’åºé€‰æ‹©å‰kä¸ª
        if len(selected_features) > k:
            feature_scores = [(f, scores_dict[f]) for f in selected_features]
            feature_scores.sort(key=lambda x: x[1], reverse=True)
            selected_features = [f[0] for f in feature_scores[:k]]
        
        self.selected_features['univariate'] = selected_features
        self.feature_scores['univariate'] = scores_dict
        
        print(f"  æœ€ç»ˆé€‰æ‹©: {len(selected_features)} ä¸ªç‰¹å¾")
        
    def _recursive_feature_elimination(self, X, y, k):
        """é€’å½’ç‰¹å¾æ¶ˆé™¤"""
        print(f"\nğŸ”„ é€’å½’ç‰¹å¾æ¶ˆé™¤ (ç›®æ ‡: {k} ä¸ªç‰¹å¾)")
        
        # ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºåŸºç¡€ä¼°è®¡å™¨
        estimator = LogisticRegression(random_state=self.random_state, max_iter=2000, solver='liblinear')
        
        # ä½¿ç”¨äº¤å‰éªŒè¯çš„RFE
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
        selector = RFECV(
            estimator, 
            step=1, 
            cv=cv, 
            scoring='roc_auc',
            min_features_to_select=min(k, X.shape[1]//2)
        )
        
        try:
            selector.fit(X, y)
            selected_features = X.columns[selector.support_].tolist()
            
            # å¦‚æœé€‰æ‹©çš„ç‰¹å¾å¤ªå¤šï¼Œé€‰æ‹©æ’åæœ€é«˜çš„kä¸ª
            if len(selected_features) > k:
                rankings = selector.ranking_
                top_indices = np.where(selector.support_)[0]
                top_rankings = rankings[top_indices]
                sorted_indices = np.argsort(top_rankings)[:k]
                selected_features = [selected_features[i] for i in sorted_indices]
            
            self.selected_features['rfe'] = selected_features
            self.feature_scores['rfe'] = selector.ranking_
            
            print(f"  é€‰æ‹©ç‰¹å¾: {len(selected_features)} ä¸ª")
            print(f"  æœ€ä½³ç‰¹å¾æ•°é‡: {selector.n_features_}")
            
        except Exception as e:
            print(f"  âš ï¸ RFEå¤±è´¥: {e}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•RFE
            simple_rfe = RFE(estimator, n_features_to_select=k)
            simple_rfe.fit(X, y)
            selected_features = X.columns[simple_rfe.support_].tolist()
            
            self.selected_features['rfe'] = selected_features
            self.feature_scores['rfe'] = simple_rfe.ranking_
            print(f"  ä½¿ç”¨ç®€å•RFEï¼Œé€‰æ‹©ç‰¹å¾: {len(selected_features)} ä¸ª")
    
    def _model_based_selection(self, X, y, k):
        """åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©"""
        print(f"\nğŸŒ³ åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹© (ç›®æ ‡: {k} ä¸ªç‰¹å¾)")
        
        # ä½¿ç”¨éšæœºæ£®æ—
        rf = RandomForestClassifier(
            n_estimators=100, 
            random_state=self.random_state,
            n_jobs=-1
        )
        rf.fit(X, y)
        
        # åŸºäºç‰¹å¾é‡è¦æ€§é€‰æ‹©
        selector = SelectFromModel(rf, max_features=k, prefit=True)
        selected_features = X.columns[selector.get_support()].tolist()
        
        # å¦‚æœé€‰æ‹©çš„ç‰¹å¾ä¸è¶³kä¸ªï¼Œè¡¥å……é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾
        if len(selected_features) < k:
            importances = rf.feature_importances_
            remaining_features = [f for f in X.columns if f not in selected_features]
            remaining_importances = [importances[X.columns.get_loc(f)] for f in remaining_features]
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_remaining = sorted(zip(remaining_features, remaining_importances), 
                                    key=lambda x: x[1], reverse=True)
            
            # è¡¥å……ç‰¹å¾ç›´åˆ°è¾¾åˆ°kä¸ª
            need_count = k - len(selected_features)
            additional_features = [f[0] for f in sorted_remaining[:need_count]]
            selected_features.extend(additional_features)
        
        self.selected_features['model_based'] = selected_features
        self.feature_scores['model_based'] = rf.feature_importances_
        
        print(f"  é€‰æ‹©ç‰¹å¾: {len(selected_features)} ä¸ª")
        print(f"  å¹³å‡ç‰¹å¾é‡è¦æ€§: {np.mean(rf.feature_importances_):.4f}")
    
    def _ensemble_selection(self, k):
        """é›†æˆç‰¹å¾é€‰æ‹©"""
        print(f"\nğŸ¯ é›†æˆç‰¹å¾é€‰æ‹©")
        
        # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾è¢«é€‰ä¸­çš„æ¬¡æ•°
        feature_votes = {}
        for feature in self.all_features:
            votes = 0
            for method, features in self.selected_features.items():
                if feature in features:
                    votes += 1
            feature_votes[feature] = votes
        
        # æŒ‰æŠ•ç¥¨æ•°æ’åº
        sorted_features = sorted(feature_votes.items(), key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©è·å¾—æœ€å¤šæŠ•ç¥¨çš„å‰kä¸ªç‰¹å¾
        self.final_features = [f[0] for f in sorted_features[:k]]
        self.feature_votes = feature_votes
        
        print(f"  æœ€ç»ˆé€‰æ‹©ç‰¹å¾: {len(self.final_features)} ä¸ª")
        print(f"  å¹³å‡æŠ•ç¥¨æ•°: {np.mean([v for v in feature_votes.values()]):.2f}")
        
        # æ˜¾ç¤ºæŠ•ç¥¨ç»“æœ
        print(f"\nğŸ“Š ç‰¹å¾æŠ•ç¥¨ç»“æœ (å‰10ä¸ª):")
        for i, (feature, votes) in enumerate(sorted_features[:10]):
            status = "âœ“" if feature in self.final_features else "âœ—"
            print(f"  {i+1:2d}. {status} {feature}: {votes} ç¥¨")
    
    def get_feature_importance_summary(self):
        """è·å–ç‰¹å¾é‡è¦æ€§æ€»ç»“"""
        if not hasattr(self, 'final_features'):
            return None
            
        summary = []
        for feature in self.final_features:
            votes = self.feature_votes.get(feature, 0)
            
            # æ”¶é›†å„æ–¹æ³•çš„åˆ†æ•°
            scores = {}
            for method, score_dict in self.feature_scores.items():
                if isinstance(score_dict, dict):
                    scores[method] = score_dict.get(feature, 0)
                elif hasattr(score_dict, '__getitem__'):
                    try:
                        idx = self.all_features.index(feature)
                        scores[method] = score_dict[idx]
                    except:
                        scores[method] = 0
            
            summary.append({
                'feature': feature,
                'votes': votes,
                **scores
            })
        
        return pd.DataFrame(summary)
    
    def save_results(self, filepath='feature_selection_results.csv'):
        """ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ"""
        summary = self.get_feature_importance_summary()
        if summary is not None:
            summary.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"âœ… ç‰¹å¾é€‰æ‹©ç»“æœå·²ä¿å­˜: {filepath}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'final_features': self.final_features,
            'feature_votes': self.feature_votes,
            'selected_by_method': self.selected_features
        }
        
        import json
        with open(filepath.replace('.csv', '_detailed.json'), 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        return summary 

    def optimize_feature_count(self, X, y, max_features=None, step_size=1, cv_folds=10):
        """
        é€šè¿‡äº¤å‰éªŒè¯è‡ªåŠ¨ä¼˜åŒ–ç‰¹å¾æ•°é‡ï¼Œæµ‹è¯•ä»1ä¸ªç‰¹å¾åˆ°å…¨éƒ¨ç‰¹å¾çš„å„ç§ç»„åˆã€‚
        
        å‚æ•°:
            X (DataFrame): ç‰¹å¾çŸ©é˜µ
            y (Series): ç›®æ ‡å˜é‡
            max_features (int, optional): æœ€å¤§ç‰¹å¾æ•°é‡ã€‚é»˜è®¤ä¸ºç‰¹å¾æ€»æ•°
            step_size (int, optional): ç‰¹å¾æ•°é‡çš„é€’å¢æ­¥é•¿ã€‚é»˜è®¤ä¸º1
            cv_folds (int, optional): äº¤å‰éªŒè¯æŠ˜æ•°ã€‚é»˜è®¤ä¸º5
            
        è¿”å›:
            dict: åŒ…å«æœ€ä¼˜ç‰¹å¾æ•°é‡å’Œè¯¦ç»†ä¼˜åŒ–ç»“æœçš„å­—å…¸
        """
        if max_features is None:
            max_features = X.shape[1]  # ä½¿ç”¨å…¨éƒ¨ç‰¹å¾
        
        # ä½¿ç”¨RandomForestClassifierä½œä¸ºè¯„ä¼°åŸºå‡†æ¨¡å‹
        base_model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        # è·å–ç‰¹å¾é‡è¦æ€§æ’åº
        print("\nğŸ” è·å–ç‰¹å¾é‡è¦æ€§æ’åº...")
        temp_model = RandomForestClassifier(n_estimators=100, random_state=42)
        temp_model.fit(X, y)
        
        # æŒ‰é‡è¦æ€§æ’åºç‰¹å¾
        feature_importances = temp_model.feature_importances_
        feature_names = X.columns.tolist()
        sorted_indices = feature_importances.argsort()[::-1]  # ä»é«˜åˆ°ä½æ’åº
        ranked_features = [feature_names[i] for i in sorted_indices]
        
        # å‡†å¤‡å­˜å‚¨ä¸åŒç‰¹å¾æ•°é‡çš„æ€§èƒ½
        performance = {}
        feature_counts = list(range(1, max_features + 1, step_size))
        
        print(f"\nğŸ“Š æµ‹è¯•ä»1åˆ°{max_features}ä¸ªç‰¹å¾(æ­¥é•¿={step_size})çš„æ€§èƒ½...")
        
        # æµ‹è¯•ä¸åŒç‰¹å¾æ•°é‡çš„æ€§èƒ½
        for k in feature_counts:
            selected_features = ranked_features[:k]
            X_selected = X[selected_features]
            
            # ä½¿ç”¨åˆ†å±‚äº¤å‰éªŒè¯è¯„ä¼°å½“å‰ç‰¹å¾æ•°é‡çš„æ€§èƒ½
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            cv_scores = cross_val_score(base_model, X_selected, y, cv=cv, scoring='roc_auc')
            
            performance[k] = {
                'mean_auc': cv_scores.mean(),
                'std_auc': cv_scores.std(),
                'features': selected_features
            }
            
            print(f"  ç‰¹å¾æ•°é‡: {k:2d} â†’ AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # æ‰¾å‡ºAUCæœ€é«˜çš„ç‰¹å¾æ•°é‡
        optimal_count = max(performance.keys(), key=lambda k: performance[k]['mean_auc'])
        optimal_auc = performance[optimal_count]['mean_auc']
        optimal_std = performance[optimal_count]['std_auc']
        
        print(f"\nâœ¨ æœ€ä½³ç‰¹å¾æ•°é‡: {optimal_count} (AUC: {optimal_auc:.4f} Â± {optimal_std:.4f})")
        print(f"âœ… æœ€ä½³ç‰¹å¾é›†: {performance[optimal_count]['features']}")
        
        # ç»˜åˆ¶æ€§èƒ½æ›²çº¿å¹¶åœ¨æœ€ä½³ç‚¹æ ‡æ˜Ÿ
        self._plot_feature_count_performance(performance, optimal_count)
        
        # ä¿å­˜è¯¦ç»†ä¼˜åŒ–ç»“æœ
        optimization_results = {
            'optimal_count': optimal_count,
            'optimal_auc': float(optimal_auc),
            'optimal_std': float(optimal_std),
            'optimal_features': performance[optimal_count]['features'],
            'all_results': {k: {'mean_auc': float(v['mean_auc']), 'std_auc': float(v['std_auc'])} 
                           for k, v in performance.items()}
        }
        
        # å°†æœ€ä½³ç‰¹å¾ä¿å­˜åˆ°selected_features_
        self.selected_features_ = {'model': performance[optimal_count]['features']}
        
        return optimization_results

    def _plot_feature_count_performance(self, performance, optimal_count):
        """
        ç»˜åˆ¶ç‰¹å¾æ•°é‡ä¸æ€§èƒ½å…³ç³»å›¾ï¼Œå¹¶åœ¨æœ€ä½³ç‰¹å¾ç‚¹æ ‡è®°æ˜Ÿå·ã€‚
        
        å‚æ•°:
            performance (dict): åŒ…å«ä¸åŒç‰¹å¾æ•°é‡æ€§èƒ½çš„å­—å…¸
            optimal_count (int): æœ€ä½³ç‰¹å¾æ•°é‡
        """
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            from datetime import datetime
            
            # æå–æ•°æ®ç”¨äºç»˜å›¾
            feature_counts = sorted(performance.keys())
            mean_aucs = [performance[k]['mean_auc'] for k in feature_counts]
            std_aucs = [performance[k]['std_auc'] for k in feature_counts]
            
            # åˆ›å»ºå›¾è¡¨
            plt.figure(figsize=(10, 6))
            
            # ç»˜åˆ¶AUCæ›²çº¿
            plt.plot(feature_counts, mean_aucs, 'b-', marker='o', markersize=4, label='å¹³å‡AUC')
            
            # æ·»åŠ è¯¯å·®å¸¦
            plt.fill_between(
                feature_counts, 
                [m-s for m,s in zip(mean_aucs, std_aucs)],
                [m+s for m,s in zip(mean_aucs, std_aucs)],
                color='b', alpha=0.1, label='æ ‡å‡†å·®'
            )
            
            # æ ‡è®°æœ€ä½³ç‚¹(æ˜Ÿæ˜Ÿ)
            plt.plot(optimal_count, performance[optimal_count]['mean_auc'], 'r*', markersize=15, 
                    label=f'æœ€ä½³ç‰¹å¾æ•°: {optimal_count}')
            
            # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
            plt.title('ç‰¹å¾æ•°é‡ä¼˜åŒ–: AUCæ€§èƒ½æ›²çº¿', fontsize=14)
            plt.xlabel('ç‰¹å¾æ•°é‡', fontsize=12)
            plt.ylabel('å¹³å‡AUC (5æŠ˜äº¤å‰éªŒè¯)', fontsize=12)
            plt.grid(True, linestyle='--', alpha=0.7)
            plt.legend(loc='lower right')
            
            # è®¾ç½®yè½´ä»0.5å¼€å§‹
            y_min = min(0.5, min(m-s for m,s in zip(mean_aucs, std_aucs))-0.02)
            y_max = max(m+s for m,s in zip(mean_aucs, std_aucs))+0.02
            plt.ylim([y_min, y_max])
            
            # ä¿å­˜å›¾è¡¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"feature_optimization_{timestamp}.png"
            plt.tight_layout()
            plt.savefig(save_path)
            print(f"\nğŸ“ˆ æ€§èƒ½æ›²çº¿å·²ä¿å­˜ä¸º: {save_path}")
            
            # å…³é—­å›¾è¡¨é¿å…å†…å­˜æ³„æ¼
            plt.close()
            
        except Exception as e:
            print(f"\nâš ï¸ æ— æ³•ç”Ÿæˆæ€§èƒ½æ›²çº¿å›¾: {str(e)}") 