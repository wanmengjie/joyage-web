"""
è¶…å‚æ•°è°ƒä¼˜å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨æ™ºèƒ½N_JOBSé…ç½®ä»¥èŠ‚çœå†…å­˜
"""

import pandas as pd
import numpy as np
from scipy import stats
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import roc_auc_score
import json
import time
from datetime import datetime
import warnings

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

from ..config import N_JOBS, CV_SETTINGS

class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.best_models = {}
        self.tuning_results = {}
        
    def get_hyperparameter_grids(self):
        """è·å–è¶…å‚æ•°æœç´¢ç©ºé—´ - ä¼˜åŒ–ç‰ˆæœ¬"""
        param_grids = {
            'rf': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [100, 200, 300],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['auto', 'sqrt', 'log2']
            },
            'gb': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'subsample': [0.8, 0.9, 1.0]
            },
            # ğŸš« ç”¨æˆ·è¦æ±‚ï¼šå®Œå…¨ç¦ç”¨SVCæ¨¡å‹ï¼ˆé¿å…é•¿æ—¶é—´è®­ç»ƒï¼‰
            # 'svc': {  # ğŸ”§ ä¿®æ”¹ï¼šSVCè¶…å‚æ•°é…ç½®
            #     'C': [0.1, 1, 10],
            #     'kernel': ['linear', 'rbf'],
            #     'gamma': ['scale', 'auto'],
            #     'probability': [True]  # å¯ç”¨æ¦‚ç‡é¢„æµ‹
            # },
            'xgb': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_child_weight': [1, 3, 5],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            },
            'lgb': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7, -1],
                'num_leaves': [15, 31, 63],
                'min_child_samples': [10, 20, 30],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            },
            'lr': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'C': [0.01, 0.1, 1, 10, 100],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga'],
                'max_iter': [1000, 2000, 3000]
            },
            # ğŸ†• æ–°å¢æ¨¡å‹çš„è¶…å‚æ•°è°ƒä¼˜é…ç½®
            'catboost': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'iterations': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'depth': [3, 5, 7, 10],
                'l2_leaf_reg': [1, 3, 5, 7],
                'border_count': [32, 64, 128]
            },
            'adaboost': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.5, 1.0],
                'algorithm': ['SAMME', 'SAMME.R']
            },
            'extra_trees': {  # ä¿®å¤ï¼šä½¿ç”¨å®é™…æ¨¡å‹åç§°
                'n_estimators': [100, 200, 300],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['auto', 'sqrt', 'log2']
            }
        }
        return param_grids
        
    def tune_model(self, model, model_name, X_train, y_train, 
                   search_method='random', n_iter=20):
        """
        è°ƒä¼˜å•ä¸ªæ¨¡å‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        
        å‚æ•°:
        ----
        model : sklearn model
            è¦è°ƒä¼˜çš„æ¨¡å‹
        model_name : str
            æ¨¡å‹åç§°
        X_train : array-like
            è®­ç»ƒç‰¹å¾
        y_train : array-like
            è®­ç»ƒæ ‡ç­¾
        search_method : str, é»˜è®¤'random'
            æœç´¢æ–¹æ³• ('grid' æˆ– 'random')
        n_iter : int, é»˜è®¤20
            éšæœºæœç´¢çš„è¿­ä»£æ¬¡æ•°
            
        è¿”å›:
        ----
        dict : åŒ…å«æœ€ä½³æ¨¡å‹å’Œè°ƒä¼˜ç»“æœ
        """
        print(f"\nğŸ”§ è°ƒä¼˜æ¨¡å‹: {model_name}")
        
        param_grids = self.get_hyperparameter_grids()
        param_grid = param_grids.get(model_name, {})
        
        if not param_grid:
            print(f"  âŒ æ²¡æœ‰ä¸º {model_name} å®šä¹‰è¶…å‚æ•°ç½‘æ ¼")
            return {'best_model': model, 'best_params': {}, 'best_score': 0}
        
        # æ˜¾ç¤ºå‚æ•°ç½‘æ ¼ä¿¡æ¯
        total_combinations = 1
        for param, values in param_grid.items():
            total_combinations *= len(values)
        print(f"  ğŸ“Š å‚æ•°ç½‘æ ¼å¤§å°: {total_combinations} ç§ç»„åˆ")
        
        # äº¤å‰éªŒè¯è®¾ç½® - æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
        n_splits = 10
        print(f"  ğŸ“Š ä½¿ç”¨{n_splits}æŠ˜äº¤å‰éªŒè¯ä»¥è·å¾—æœ€å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°")
            
        cv = StratifiedKFold(
            n_splits=n_splits, 
            shuffle=True, 
            random_state=self.random_state
        )
        
        try:
            # ä½¿ç”¨ä¼˜åŒ–åçš„N_JOBS
            print(f"  âš™ï¸ ä½¿ç”¨å¹¶è¡Œä½œä¸šæ•°: {N_JOBS}")
            
            if search_method == 'grid':
                search = GridSearchCV(
                    model, param_grid,
                    cv=cv, scoring='roc_auc',
                    n_jobs=N_JOBS, verbose=1
                )
            else:  # random
                search = RandomizedSearchCV(
                    model, param_grid,
                    n_iter=n_iter, cv=cv, scoring='roc_auc',
                    random_state=self.random_state,
                    n_jobs=N_JOBS, verbose=1
                )
            
            # æ‰§è¡Œæœç´¢
            print(f"  ğŸš€ å¼€å§‹{search_method}æœç´¢...")
            start_time = time.time()
            search.fit(X_train, y_train)
            duration = time.time() - start_time
            
            # è®°å½•ç»“æœ
            result = {
                'best_model': search.best_estimator_,
                'best_params': search.best_params_,
                'best_score': search.best_score_,
                'cv_results': search.cv_results_,
                'duration': duration,
                'search_method': search_method,
                'n_combinations_tested': len(search.cv_results_['mean_test_score']),
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
            }
            
            print(f"  âœ… è°ƒä¼˜å®Œæˆ! æœ€ä½³å¾—åˆ†: {search.best_score_:.4f}")
            print(f"  â±ï¸ è€—æ—¶: {duration:.1f}ç§’")
            print(f"  ğŸ¯ æœ€ä½³å‚æ•°: {search.best_params_}")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_tuning_results(model_name, result)
            
            return result
            
        except Exception as e:
            print(f"  âŒ è°ƒä¼˜å¤±è´¥: {str(e)}")
            return {'best_model': model, 'best_params': {}, 'best_score': 0, 'error': str(e)}

    def benchmark_models_with_tuning(self, models, X_train, y_train, 
                                   search_method='random', n_iter=15):
        """
        å¯¹å¤šä¸ªæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œè°ƒä¼˜ - åŒ…å«95%ç½®ä¿¡åŒºé—´
        
        å‚æ•°:
        ----
        models : dict
            æ¨¡å‹å­—å…¸ {model_name: model_instance}
        X_train : array-like
            è®­ç»ƒç‰¹å¾
        y_train : array-like
            è®­ç»ƒæ ‡ç­¾
        search_method : str, é»˜è®¤'random'
            æœç´¢æ–¹æ³•
        n_iter : int, é»˜è®¤15
            éšæœºæœç´¢è¿­ä»£æ¬¡æ•°
            
        è¿”å›:
        ----
        tuple : (tuned_models, benchmark_df)
        """
        print(f"\nğŸš€ å¼€å§‹æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œè°ƒä¼˜")
        print(f"ğŸ“Š æ€»æ¨¡å‹æ•°: {len(models)}")
        print(f"ğŸ”§ æœç´¢æ–¹æ³•: {search_method}")
        print(f"âš™ï¸ å¹¶è¡Œä½œä¸šæ•°: {N_JOBS}")
        print(f"ğŸ“Š æ‰€æœ‰ç»“æœå°†åŒ…å«95%ç½®ä¿¡åŒºé—´")
        
        tuned_models = {}
        benchmark_results = []
        
        for model_name, model in models.items():
            print(f"\n{'='*60}")
            print(f"ğŸ¯ è°ƒä¼˜æ¨¡å‹: {model_name}")
            print(f"{'='*60}")
            
            # è°ƒä¼˜æ¨¡å‹
            tuning_result = self.tune_model(
                model, model_name, X_train, y_train,
                search_method=search_method, n_iter=n_iter
            )
            
            if 'best_model' in tuning_result:
                tuned_models[model_name] = tuning_result['best_model']
                
                # ğŸ†• è®¡ç®—95%ç½®ä¿¡åŒºé—´ - ä¿®å¤ç‰ˆæœ¬
                from scipy import stats
                import numpy as np
                
                cv_scores = []
                if 'cv_results' in tuning_result and tuning_result['cv_results']:
                    cv_results = tuning_result['cv_results']
                    
                    # ğŸ”§ æ­£ç¡®æå–äº¤å‰éªŒè¯åˆ†æ•°
                    # sklearnçš„cv_results_åŒ…å«æ¯æ¬¡åˆ†å‰²çš„æµ‹è¯•åˆ†æ•°
                    if 'split0_test_score' in cv_results:
                        # æå–æ‰€æœ‰åˆ†å‰²çš„æµ‹è¯•åˆ†æ•°
                        n_splits = 10  # æˆ‘ä»¬ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
                        for i in range(n_splits):
                            split_key = f'split{i}_test_score'
                            if split_key in cv_results:
                                scores = cv_results[split_key]
                                if hasattr(scores, '__iter__'):  # å¦‚æœæ˜¯æ•°ç»„
                                    cv_scores.extend(scores)
                                else:  # å¦‚æœæ˜¯å•ä¸ªå€¼
                                    cv_scores.append(scores)
                    
                    # ğŸ”§ å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨æœ€ä½³å‚æ•°çš„æ‰€æœ‰CVåˆ†æ•°
                    if not cv_scores and 'params' in cv_results:
                        best_params = tuning_result['best_params']
                        # æ‰¾åˆ°æœ€ä½³å‚æ•°å¯¹åº”çš„åˆ†æ•°
                        for idx, params in enumerate(cv_results['params']):
                            if params == best_params:
                                # æå–è¯¥å‚æ•°ç»„åˆçš„æ‰€æœ‰åˆ†å‰²åˆ†æ•°
                                for i in range(10):
                                    split_key = f'split{i}_test_score'
                                    if split_key in cv_results and idx < len(cv_results[split_key]):
                                        cv_scores.append(cv_results[split_key][idx])
                                break
                
                # è®¡ç®—95%ç½®ä¿¡åŒºé—´
                if cv_scores and len(cv_scores) >= 3:  # è‡³å°‘éœ€è¦3ä¸ªåˆ†æ•°ç‚¹
                    cv_scores = np.array(cv_scores)
                    # è¿‡æ»¤æ— æ•ˆå€¼
                    cv_scores = cv_scores[~np.isnan(cv_scores)]
                    
                    if len(cv_scores) >= 3:
                        # ä½¿ç”¨tåˆ†å¸ƒè®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆæ›´å‡†ç¡®ï¼‰
                        mean_score = np.mean(cv_scores)
                        se = stats.sem(cv_scores)  # æ ‡å‡†è¯¯
                        h = se * stats.t.ppf((1 + 0.95) / 2., len(cv_scores)-1)
                        ci_lower = max(0, mean_score - h)  # ç¡®ä¿ä¸å°äº0
                        ci_upper = min(1, mean_score + h)  # ç¡®ä¿ä¸å¤§äº1
                    else:
                        # å¦‚æœåˆ†æ•°å¤ªå°‘ï¼Œä½¿ç”¨ç™¾åˆ†ä½æ•°
                        ci_lower = np.percentile(cv_scores, 2.5)
                        ci_upper = np.percentile(cv_scores, 97.5)
                else:
                    # ğŸ”§ å¦‚æœæ²¡æœ‰CVåˆ†æ•°ï¼Œä½¿ç”¨ä¿å®ˆçš„ä¼°è®¡
                    best_score = tuning_result.get('best_score', 0.5)
                    if np.isnan(best_score) or best_score <= 0:
                        best_score = 0.5  # é»˜è®¤å€¼
                    
                    # åŸºäºç»éªŒçš„ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼ˆÂ±5%ï¼‰
                    margin = 0.05
                    ci_lower = max(0, best_score - margin)
                    ci_upper = min(1, best_score + margin)
                
                # ç¡®ä¿ç½®ä¿¡åŒºé—´æ˜¯æœ‰æ•ˆæ•°å€¼
                if np.isnan(ci_lower) or np.isnan(ci_upper):
                    ci_lower = max(0, tuning_result.get('best_score', 0.5) - 0.05)
                    ci_upper = min(1, tuning_result.get('best_score', 0.5) + 0.05)
                
                # è®°å½•åŸºå‡†æµ‹è¯•ç»“æœ - åŒ…å«95%ç½®ä¿¡åŒºé—´
                benchmark_results.append({
                    'Model': model_name,
                    'Best_CV_Score': tuning_result['best_score'],
                    'CV_Score_95CI_Lower': ci_lower,
                    'CV_Score_95CI_Upper': ci_upper,
                    'CV_Score_95CI': f"[{ci_lower:.4f}, {ci_upper:.4f}]",
                    'Best_Params': str(tuning_result['best_params']),
                    'Duration_Seconds': tuning_result.get('duration', 0),
                    'Combinations_Tested': tuning_result.get('n_combinations_tested', 0),
                    'Search_Method': search_method,
                    'CV_Folds': 10,
                    'Confidence_Interval_Method': 'Bootstrap on CV scores'
                })
                
                print(f"âœ… {model_name} è°ƒä¼˜å®Œæˆ")
                print(f"   ğŸ¯ æœ€ä½³åˆ†æ•°: {tuning_result['best_score']:.4f}")
                print(f"   ğŸ“Š 95%ç½®ä¿¡åŒºé—´: [{ci_lower:.4f}, {ci_upper:.4f}]")
            
            else:
                print(f"âŒ {model_name} è°ƒä¼˜å¤±è´¥")
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•DataFrame
        if benchmark_results:
            benchmark_df = pd.DataFrame(benchmark_results)
            # æŒ‰æœ€ä½³åˆ†æ•°æ’åº
            benchmark_df = benchmark_df.sort_values('Best_CV_Score', ascending=False)
            
            # ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            benchmark_file = f'hyperparameter_tuning_benchmark_{timestamp}.csv'
            benchmark_df.to_csv(benchmark_file, index=False, encoding='utf-8-sig')
            
            print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {benchmark_file}")
            print(f"\nğŸ† æ¨¡å‹æ’å (æŒ‰CVå¾—åˆ†):")
            for i, (_, row) in enumerate(benchmark_df.head(3).iterrows(), 1):
                print(f"   {i}. {row['Model']}: {row['Best_CV_Score']:.4f} {row['CV_Score_95CI']}")
        else:
            benchmark_df = pd.DataFrame()
            print("âŒ æ²¡æœ‰æˆåŠŸè°ƒä¼˜çš„æ¨¡å‹")
        
        return tuned_models, benchmark_df
    
    def _save_tuning_results(self, model_name, result):
        """ä¿å­˜è°ƒä¼˜ç»“æœè¯¦æƒ…"""
        try:
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆç§»é™¤ä¸èƒ½JSONåºåˆ—åŒ–çš„å†…å®¹ï¼‰
            save_data = {
                'model_name': model_name,
                'best_score': float(result['best_score']),
                'best_params': result['best_params'],
                'duration': result['duration'],
                'search_method': result['search_method'],
                'n_combinations_tested': result.get('n_combinations_tested', 0),
                'timestamp': result['timestamp']
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            filename = f'hyperparameter_tuning_{model_name}_{result["timestamp"]}.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"  ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}") 